{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window functions¶\n",
    "-  `pyspark.sql.functions`는 substr()과 같이 문자열을 잘라내고, 그 결과를 생성하여 행으로 적용\n",
    "- 윈도우 함수는 그룹으로 구분하고 그 범위 내에서 계산을 할 때 사용, 이 때 `over()` 함수로 적용되는 그룹의 윈도우를 구분하게 된다.\n",
    "    - 순위 함수 ranking functions: rank, dense_rank, percent_rank, ntile, row_number\n",
    "    - 분석 함수 analytic functions: cume_dist, first_value, last_value, lag, lead\n",
    "    - 집합 함수 aggregate functions: sum, avg, min, max, count와 같이 앞서 배웠던 집합 함수를 윈도우에 대해 적용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list 안에 문자열로 되어있다.\n",
    "marks=[\n",
    "    \"김하나, English, 100\",\n",
    "    \"김하나, Math, 80\",\n",
    "    \"임하나, English, 70\",\n",
    "    \"임하나, Math, 100\",\n",
    "    \"김갑돌, English, 82.3\",\n",
    "    \"김갑돌, Math, 98.5\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df로\n",
    "- 이름, 과목, 성적을 하나의 컬럼으로 읽음 -> 이를 컴마로 분리하여 하나의 컬럼 생성\n",
    "\n",
    "\n",
    "- RDD의 map()을 사용하여 컴마로 분할 -> 3개의 컬럼으로 dataframe 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD로 만들어서 ,로 분리하자\n",
    "# x: \"김하나, English, 100\"\n",
    "# space로 분리되어 있을 경우 아무것도 안적어도 된다.\n",
    "_marksRdd=spark.sparkContext.parallelize(marks).map(lambda x:x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#읽으면서 컬럼명 수정\n",
    "_marksDf=spark.createDataFrame(_marksRdd, schema=[\"name\", \"subject\", \"mark\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- mark: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_marksDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+\n",
      "|  name| subject| mark|\n",
      "+------+--------+-----+\n",
      "|김하나| English|  100|\n",
      "|김하나|    Math|   80|\n",
      "|임하나| English|   70|\n",
      "|임하나|    Math|  100|\n",
      "|김갑돌| English| 82.3|\n",
      "|김갑돌|    Math| 98.5|\n",
      "+------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_marksDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "| subject|count|\n",
      "+--------+-----+\n",
      "|    Math|    3|\n",
      "| English|    3|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_marksDf.groupBy('subject').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  group window (그룹 window)\n",
    "- `partitionBy`: 컬럼별로 구분, 즉 컬럼 값에 따라 partition에 포함되는 행을 할당.\n",
    "    - partition을 정하지 않으면 모든 행을 동일한 노드에 할당한다.\n",
    "- `orderBy`: partition 내에서 컬럼 값에 대해 행의 순서를 정렬한다. 순서대로 정렬\n",
    "- `frame`: 현재 행을 기준으로 포함할 행을 분할.\n",
    "    - 행 프레임: 현재 행을 기준으로 몇 개 앞, 몇 개 뒤의 물리적 범위를 `window.rowsBetween(start, end)` 정한다.\n",
    "    - 범위 프레임: 논리적인 범위를 `windowSpec.rangeBetween(start, end)` 정한다. 현재 성적이 70점이면 RANGE BETWEEN 20 PRECEDING AND 10 FOLLOWING은 50 ~ 80을 의미(앞에 20 뒤에 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "win = Window.partitionBy(\"subject\").orderBy(\"mark\") #과목별로"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 순위 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### row_number\n",
    "`row_number()` 윈도우 함수는 각 그룹별로 일련번호를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+----------+\n",
      "|  name| subject| mark|row_number|\n",
      "+------+--------+-----+----------+\n",
      "|임하나|    Math|  100|         1|\n",
      "|김하나|    Math|   80|         2|\n",
      "|김갑돌|    Math| 98.5|         3|\n",
      "|김하나| English|  100|         1|\n",
      "|임하나| English|   70|         2|\n",
      "|김갑돌| English| 82.3|         3|\n",
      "+------+--------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import row_number\n",
    "_marksDf.withColumn(\"row_number\", row_number().over(win)).show()\n",
    "#over안에 위에서 만든 win 적는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 정렬이 되지 않고 있다. ==> `mark`가 문자열로 정의되어있기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- mark: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_marksDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FloatType()로 형변환\n",
    "from pyspark.sql.types import FloatType\n",
    "_marksDf = _marksDf.withColumn('markF', _marksDf['mark'].cast(FloatType()))\n",
    "# markF라는 컬럼을 만들고 cast해서 FloatType로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "winF = Window.partitionBy(\"subject\").orderBy(F.col(\"markF\").desc())\n",
    "# Float로 winF다시만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+-----+----------+\n",
      "|  name| subject| mark|markF|row_number|\n",
      "+------+--------+-----+-----+----------+\n",
      "|임하나|    Math|  100|100.0|         1|\n",
      "|김갑돌|    Math| 98.5| 98.5|         2|\n",
      "|김하나|    Math|   80| 80.0|         3|\n",
      "|김하나| English|  100|100.0|         1|\n",
      "|김갑돌| English| 82.3| 82.3|         2|\n",
      "|임하나| English|   70| 70.0|         3|\n",
      "+------+--------+-----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import row_number\n",
    "_marksDf.withColumn(\"row_number\", row_number().over(winF)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rank\n",
    "`rank()` 윈도우 함수는 각 그룹별로 등위를 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+-----+----+\n",
      "|  name| subject| mark|markF|rank|\n",
      "+------+--------+-----+-----+----+\n",
      "|임하나|    Math|  100|100.0|   1|\n",
      "|김갑돌|    Math| 98.5| 98.5|   2|\n",
      "|김하나|    Math|   80| 80.0|   3|\n",
      "|김하나| English|  100|100.0|   1|\n",
      "|김갑돌| English| 82.3| 82.3|   2|\n",
      "|임하나| English|   70| 70.0|   3|\n",
      "+------+--------+-----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "_marksDf.withColumn(\"rank\", rank().over(winF)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분석적 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cume_dist()\n",
    "`cume_dist()` 윈도우 함수는 누적 분포 값을 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+-----+------------------+\n",
      "|  name| subject| mark|markF|         cume_dist|\n",
      "+------+--------+-----+-----+------------------+\n",
      "|임하나|    Math|  100|100.0|0.3333333333333333|\n",
      "|김갑돌|    Math| 98.5| 98.5|0.6666666666666666|\n",
      "|김하나|    Math|   80| 80.0|               1.0|\n",
      "|김하나| English|  100|100.0|0.3333333333333333|\n",
      "|김갑돌| English| 82.3| 82.3|0.6666666666666666|\n",
      "|임하나| English|   70| 70.0|               1.0|\n",
      "+------+--------+-----+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import cume_dist\n",
    "\n",
    "_marksDf.withColumn(\"cume_dist\", cume_dist().over(winF)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+-----+-----+\n",
      "|  name| subject| mark|markF|  lag|\n",
      "+------+--------+-----+-----+-----+\n",
      "|임하나|    Math|  100|100.0| null|\n",
      "|김갑돌|    Math| 98.5| 98.5|  100|\n",
      "|김하나|    Math|   80| 80.0| 98.5|\n",
      "|김하나| English|  100|100.0| null|\n",
      "|김갑돌| English| 82.3| 82.3|  100|\n",
      "|임하나| English|   70| 70.0| 82.3|\n",
      "+------+--------+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lag\n",
    "# lag는 하나씩 밀려서 나온다\n",
    "_marksDf.withColumn(\"lag\", lag('mark', 1).over(winF)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+-----+-----+\n",
      "|  name| subject| mark|markF|  lag|\n",
      "+------+--------+-----+-----+-----+\n",
      "|임하나|    Math|  100|100.0| 98.5|\n",
      "|김갑돌|    Math| 98.5| 98.5|   80|\n",
      "|김하나|    Math|   80| 80.0| null|\n",
      "|김하나| English|  100|100.0| 82.3|\n",
      "|김갑돌| English| 82.3| 82.3|   70|\n",
      "|임하나| English|   70| 70.0| null|\n",
      "+------+--------+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead\n",
    "#1개씩 앞당김\n",
    "_marksDf.withColumn(\"lag\", lead('mark', 1).over(winF)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Functions\n",
    "그룹을 구분할 때, 정렬할 필요가 없다.  \n",
    "mark가 string으로 설정되어 있다고 평균, 합계, 최소, 최대 함수가 어떻게 출력될까? 문제 없이 실행되지만, 결과는 올바르지 않다. **데이터타입은 항상 주의**해야 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "winAgg  = Window.partitionBy(\"subject\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+-----+-----------------+-----------------+----+-----+\n",
      "|  name| subject| mark|markF|              avg|              sum| min|  max|\n",
      "+------+--------+-----+-----+-----------------+-----------------+----+-----+\n",
      "|김하나|    Math|   80| 80.0|92.83333333333333|            278.5|80.0|100.0|\n",
      "|임하나|    Math|  100|100.0|92.83333333333333|            278.5|80.0|100.0|\n",
      "|김갑돌|    Math| 98.5| 98.5|92.83333333333333|            278.5|80.0|100.0|\n",
      "|김하나| English|  100|100.0|84.10000101725261|252.3000030517578|70.0|100.0|\n",
      "|임하나| English|   70| 70.0|84.10000101725261|252.3000030517578|70.0|100.0|\n",
      "|김갑돌| English| 82.3| 82.3|84.10000101725261|252.3000030517578|70.0|100.0|\n",
      "+------+--------+-----+-----+-----------------+-----------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "_marksDf.withColumn(\"avg\", F.avg(F.col(\"markF\")).over(winAgg))\\\n",
    "    .withColumn(\"sum\", F.sum(F.col(\"markF\")).over(winAgg))\\\n",
    "    .withColumn(\"min\", F.min(F.col(\"markF\")).over(winAgg))\\\n",
    "    .withColumn(\"max\", F.max(F.col(\"markF\")).over(winAgg))\\\n",
    "    .show() #수학,과목끼리, 영어과목끼리 같은 avg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
