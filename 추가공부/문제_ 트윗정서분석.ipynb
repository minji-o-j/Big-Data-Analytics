{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제: 트윗 정서 분석\n",
    "\n",
    "트윗의 정서를 분석해보자.\n",
    "트윗은 API를 사용하여 수집할 수 있으나, 프로그램도 작성해야 하고 수집에 상당한 시간이 필요하다.\n",
    "또한 트윗 메시지에 대해 긍정인지, 부정인지 정서를 하나 하나 판단해서 태깅해야 한다.\n",
    "스탠포드 대학원생들이 수집해 놓은 트윗이 있다. 오픈소스가 아니므로 주의해야 한다 http://help.sentiment140.com/for-students/\n",
    "이 데이터의 항목은:\n",
    "* 트윗 정서 (0=부정, 2=중립, 4=긍정)\n",
    "* 트윗 ID\n",
    "* 트윗 일자\n",
    "* 조회 (없으면 NO_QUERY)\n",
    "* 사용자\n",
    "* 트윗 텍스트\n",
    "\n",
    "1) 전체 데이터 갯수, 각 '부정' '긍정' '중립' 별 데이터 갯수  \n",
    "2) 트윗 정서 컬럼을 'label'로 변경  \n",
    "5) stopwords 제거하고, 출력  \n",
    "6) pipeline으로 tf-idf 계산하고 'features' 컬럼으로 변경  \n",
    "\n",
    "https://github.com/tthustla/twitter_sentiment_analysis_part2/blob/master/Capstone_part3-Copy1.ipynb\n",
    "https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-2-333514854913"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame 생성\n",
    "\n",
    "현재 작업디렉토리 아래 data/stanford 폴더 아래 csv 파일을 읽어 DataFrame을 생성하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetDf = spark.read\\\n",
    "    .format('com.databricks.spark.csv')\\\n",
    "    .options(header='false', inferschema='true')\\\n",
    "    .load(os.path.join(\"data\", \"트윗.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스키마가 자동인식되었는지 확인하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: integer (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweetDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "트윗의 건수를 세어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "498"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetDf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컬럼명 수정\n",
    "\n",
    "컬럼명을 label, ID, tweetTime, query, user, text로 변경한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetDf = tweetDf.withColumnRenamed(\"_c0\", \"label\") # 0=neg, 2=0, 4=pos\n",
    "tweetDf = tweetDf.withColumnRenamed(\"_c1\", \"ID\")\n",
    "tweetDf = tweetDf.withColumnRenamed(\"_c2\", \"tweetTime\")\n",
    "tweetDf = tweetDf.withColumnRenamed(\"_c3\", \"query\")\n",
    "tweetDf = tweetDf.withColumnRenamed(\"_c4\", \"user\")\n",
    "tweetDf = tweetDf.withColumnRenamed(\"_c5\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------------+-------+--------+--------------------+\n",
      "|label| ID|           tweetTime|  query|    user|                text|\n",
      "+-----+---+--------------------+-------+--------+--------------------+\n",
      "|    4|  3|Mon May 11 03:17:...|kindle2|  tpryan|@stellargirl I lo...|\n",
      "|    4|  4|Mon May 11 03:18:...|kindle2|  vcu451|Reading my kindle...|\n",
      "|    4|  5|Mon May 11 03:18:...|kindle2|  chadfu|Ok, first assesme...|\n",
      "|    4|  6|Mon May 11 03:19:...|kindle2|   SIX15|@kenburbary You'l...|\n",
      "|    4|  7|Mon May 11 03:21:...|kindle2|yamarama|@mikefish  Fair e...|\n",
      "+-----+---+--------------------+-------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweetDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 출력\n",
    "\n",
    "트윗은 사람들이 편하게 작성한 텍스트라서, 줄임말, 구어, 유행어, 이모티콘 등이 뒤섞여 있을 수 밖에 없다.\n",
    "몇 건만 출력을 해서 보더라도 그렇다는 것을 확인할 수 있다.\n",
    "* \"I've\"는 \"I have\"의 줄임말이다. 불용어로 제거될 수 있다.\n",
    "* \"looooovvvveee\"는 \"love\"의 강조하기 위해 쓰인 것으로 보인다.\n",
    "* 단어 뒤에 \"!!!\"와 같은 강조가 발견된다.\n",
    "* 이모티콘 \":)\"도 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                                        |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|@stellargirl I loooooooovvvvvveee my Kindle2. Not that the DX is cool, but the 2 is fantastic in its own right.                             |\n",
      "|Reading my kindle2...  Love it... Lee childs is good read.                                                                                  |\n",
      "|Ok, first assesment of the #kindle2 ...it fucking rocks!!!                                                                                  |\n",
      "|@kenburbary You'll love your Kindle2. I've had mine for a few months and never looked back. The new big one is huge! No need for remorse! :)|\n",
      "|@mikefish  Fair enough. But i have the Kindle2 and I think it's perfect  :)                                                                 |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweetDf.select('text').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결측값\n",
    "\n",
    "분석에 앞서 결측이 있는지 보는 것이 중요하다. 데이터에 자체에 또는 수집 과정에서 결측이 발생할 수 있고, 분석과정에서 영향을 미칠 수 있기 때문이다.\n",
    "* tweetDf.columns은 모든 컬럼을 조회하고,\n",
    "* 이들 컬럼에 대해 반복문으로 수행하면서,\n",
    "* isnan(컬럼) 또는 col(c).isNull()인지 개수를 세어서 count(),\n",
    "* 컬럼명으로 출력하기 위해 alias() 하고,\n",
    "* select() 명령어에 리스트로 넘겨서 출력한다.\n",
    "\n",
    "수행하는 명령문이 복잡하면서도 짧게 작성이 되어 있다. Python의 특징을 잘 보여주고 있다.\n",
    "그 결과 트윗에 결측은 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+-----+----+----+\n",
      "|label| ID|tweetTime|query|user|text|\n",
      "+-----+---+---------+-----+----+----+\n",
      "|    0|  0|        0|    0|   0|   0|\n",
      "+-----+---+---------+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "tweetDf.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in tweetDf.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또는 Dictionary 타입으로 만들어서 (키, null 개수)를 저장할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0, 'ID': 0, 'tweetTime': 0, 'query': 0, 'user': 0, 'text': 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{c: tweetDf.where(tweetDf[c].isNull()).count() for c in tweetDf.columns}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 일자별 트윗건수\n",
    "\n",
    "#### 트윗일자 형변환\n",
    "\n",
    "트윗일자는 ```Mon May 11 03:17:40 UTC 2009``` 형식으로 표현된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|tweetTime                   |\n",
      "+----------------------------+\n",
      "|Mon May 11 03:17:40 UTC 2009|\n",
      "|Mon May 11 03:18:03 UTC 2009|\n",
      "|Mon May 11 03:18:54 UTC 2009|\n",
      "|Mon May 11 03:19:04 UTC 2009|\n",
      "|Mon May 11 03:21:41 UTC 2009|\n",
      "+----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweetDf.select('tweetTime').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python 요일, 일시 형식으로 변환\n",
    "\n",
    "Python에서 요일, 일시는 다양한 표현형식으로 출력할 수 있다.\n",
    "아래 표를 참조하자.\n",
    "\n",
    "표시 | 설명               |예\n",
    "---|--------------------|--------------------\n",
    "%a | 요일 줄임말           | Sun, Mon, ..., Sat\n",
    "%b | 월 줄임말            | Jan, Feb, ..., Dec\n",
    "%d | 숫자로 표현한 2글자 일자 | 01, 02, ..., 31\n",
    "%H | 숫자로 표현한 2자리 시 | 00, 01, ..., 23\n",
    "%M | 숫자로 표현한 2자리 분 | 00, 01, ..., 59\n",
    "%S | 숫자로 표현한 2자리 초 | 00, 01, ..., 59\n",
    "%d | 숫자로 표현한 2자리 일자 | 01, 02, ..., 31\n",
    "%m | 숫자로 표현한 2자리 월  | 01, 02, ..., 12\n",
    "%y | 숫자로 표현한 2자리 년수 (세기 불포함) | 00, 01, ..., 99\n",
    "%Y | 숫자로 표현한 4자리 년수 (세기 포함)  | 0001, 0001, ..., 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "연습으로 트윗일자를 일정한 표현형식으로 인식하여, 다른 형식 \"년-월-일-시-분-초\"로 변경하여 출력해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2009-05-11 03:17:40'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "_tweetTime = 'Mon May 11 03:17:40 UTC 2009'\n",
    "datetime.strftime(datetime.strptime(_tweetTime,'%a %b %d %H:%M:%S UTC %Y'), '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark에서 요일, 일시 형식으로 변환\n",
    "\n",
    "트윗일자는 -> timestamp로 변환하고 -> 그리고 DateType()으로 변환한다.\n",
    "\n",
    "먼저 트윗일자를 timestamp로 변환해보자.\n",
    "트윗일자를 패턴으로 인식하자.\n",
    "* EEE: 3글자 요일 예: Mon\n",
    "* MMM: 3글자 월 예: Jan\n",
    "* dd: 2글자 일자 예: 01, 02, ..., 31\n",
    "* z: timezone 예: GMT+02:00; EET\n",
    "\n",
    "```SparkUpgradeException... due to the upgrading of Spark 3.0```라는 오류가 발생하면\n",
    "**```spark.sql.legacy.timeParserPolicy=LEGACY```**라고 설정을 변경해주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "_tweetDf = tweetDf.withColumn('timestamp', F.unix_timestamp(tweetDf.tweetTime, \"EEE MMM dd HH:mm:ss Z yyyy\").alias())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------------+-------+------+--------------------+----------+\n",
      "|label| ID|           tweetTime|  query|  user|                text| timestamp|\n",
      "+-----+---+--------------------+-------+------+--------------------+----------+\n",
      "|    4|  3|Mon May 11 03:17:...|kindle2|tpryan|@stellargirl I lo...|1242011860|\n",
      "|    4|  4|Mon May 11 03:18:...|kindle2|vcu451|Reading my kindle...|1242011883|\n",
      "|    4|  5|Mon May 11 03:18:...|kindle2|chadfu|Ok, first assesme...|1242011934|\n",
      "+-----+---+--------------------+-------+------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tweetDf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "timestamp를 Spark의 DateType()으로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "_tweetDf = _tweetDf.withColumn('date', F.from_unixtime('timestamp').cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------------+-------+------+--------------------+----------+----------+\n",
      "|label| ID|           tweetTime|  query|  user|                text| timestamp|      date|\n",
      "+-----+---+--------------------+-------+------+--------------------+----------+----------+\n",
      "|    4|  3|Mon May 11 03:17:...|kindle2|tpryan|@stellargirl I lo...|1242011860|2009-05-11|\n",
      "|    4|  4|Mon May 11 03:18:...|kindle2|vcu451|Reading my kindle...|1242011883|2009-05-11|\n",
      "|    4|  5|Mon May 11 03:18:...|kindle2|chadfu|Ok, first assesme...|1242011934|2009-05-11|\n",
      "+-----+---+--------------------+-------+------+--------------------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tweetDf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 년, 월, 일 컬럼\n",
    "\n",
    "방금 생성한 date 컬럼에서 년, 월, 일을 분리하여 컬럼으로 만들어 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tweetDf = _tweetDf\\\n",
    "    .withColumn('year', F.year(\"date\").alias())\\\n",
    "    .withColumn('month', F.month(\"date\").alias())\\\n",
    "    .withColumn('day', F.dayofmonth(\"date\").alias())\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------------+-------+--------+--------------------+----------+----------+----+-----+---+\n",
      "|label| ID|           tweetTime|  query|    user|                text| timestamp|      date|year|month|day|\n",
      "+-----+---+--------------------+-------+--------+--------------------+----------+----------+----+-----+---+\n",
      "|    4|  3|Mon May 11 03:17:...|kindle2|  tpryan|@stellargirl I lo...|1242011860|2009-05-11|2009|    5| 11|\n",
      "|    4|  4|Mon May 11 03:18:...|kindle2|  vcu451|Reading my kindle...|1242011883|2009-05-11|2009|    5| 11|\n",
      "|    4|  5|Mon May 11 03:18:...|kindle2|  chadfu|Ok, first assesme...|1242011934|2009-05-11|2009|    5| 11|\n",
      "|    4|  6|Mon May 11 03:19:...|kindle2|   SIX15|@kenburbary You'l...|1242011944|2009-05-11|2009|    5| 11|\n",
      "|    4|  7|Mon May 11 03:21:...|kindle2|yamarama|@mikefish  Fair e...|1242012101|2009-05-11|2009|    5| 11|\n",
      "+-----+---+--------------------+-------+--------+--------------------+----------+----------+----+-----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tweetDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 년별, 월별 트윗건수\n",
    "\n",
    "트윗에 작성된 글은 시대상을 반영하기 때문에 작성일시를 확인해보자.\n",
    "pivot을 해서, 트윗의 작성년월을 조회해 볼 수 있다. 트윗은 2009년 5, 6월에 작성되었고 건수도 분배되어 수집한 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+\n",
      "|year|  5|  6|\n",
      "+----+---+---+\n",
      "|2009|248|250|\n",
      "+----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tweetDf.groupBy('year').pivot('month').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 긍정, 부정\n",
    "\n",
    "#### 긍정, 부정별 건수\n",
    "\n",
    "트윗은 정서가 판단되어, label이 붙어있다. 그 개수를 조회해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    4|  182|\n",
      "|    2|  139|\n",
      "|    0|  177|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tweetDf.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 중립 filter\n",
    "\n",
    "부정 0과 긍정 4의 경우만 선별하여 분석을 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tweetDf = _tweetDf.filter(_tweetDf.label != 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "중립 2는 제거되어, 출력에 나타나지 않고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    4|  182|\n",
      "|    0|  177|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tweetDf.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어분리\n",
    "\n",
    "\"text\"를 단어로 분리해서 \"words\" 컬럼으로 만들자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "tokDf = tokenizer.transform(_tweetDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이음동의 처리\n",
    "\n",
    "앞 장에서 작성했던 함수를 호출하여, 아래 이음동의와 불필요한 단어를 처리해보자.\n",
    "* loooooooovvvvvveee -> love, \n",
    "* :) -> smile\n",
    "\n",
    "단, Pipeline에 udf를 단계로 추가하려면 ```SQLTransformer```를 사용해야 된다.\n",
    "명령어가 복잡해지므로 앞서 tokenizer는 pipeline에 넣지 않고 transform()을 실행하고, tokDf에 대해 udf를 실행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def trim(wordList):\n",
    "    regex = re.compile('\\d+')\n",
    "    cleaned=list()\n",
    "    for w in wordList:\n",
    "        if not regex.match(w):\n",
    "            cleaned.append(w.lstrip('‘').rstrip(\"’\").rstrip(',').rstrip('.')\\\n",
    "                           .replace(\"loooooooovvvvvveee\",\"love\")\\\n",
    "                           .replace(\":)\",\"smile\"))\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "trimUdf=f.udf(trim, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tweetDf = tokDf.withColumn('words', trimUdf(f.col('tokens')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 불용어 제거\n",
    "\n",
    "트윗에는 줄임말, 구어, 이모티콘 등을 포함하고 있어, 공식적으로 작성된 문서와는 다르다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "stop = StopWordsRemover(inputCol=\"words\", outputCol=\"nostops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=list()\n",
    "_stopwords=stop.getStopWords()\n",
    "for e in _stopwords:\n",
    "    stopwords.append(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영어 \"I've\"를 제거한다. 영어 불용어 사전에 포함되이 있지만, 사용방법을 배우기 위해 넣었다.\n",
    "실제 트윗에서 불용어에 해당되는 단어를 스스로 찾아서 추가할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "_mystopwords=[\"I've\"]\n",
    "for e in _mystopwords:\n",
    "    stopwords.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopWordsRemover_a3aba6a2c852"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop.setStopWords(stopwords)\n",
    "#stopDf=stop.transform(tokDf)\n",
    "#stopDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"nostops\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline\n",
    "\n",
    "위 tokenizer, stop, hashtf, idf를 단계적으로 처리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#label_stringIdx = StringIndexer(inputCol = \"_label\", outputCol = \"label\")\n",
    "#pipeline = Pipeline(stages=[tokenizer, stop, hashtf, idf])\n",
    "pipeline = Pipeline(stages=[stop, hashtf, idf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit = pipeline.fit(_tweetDf)\n",
    "\n",
    "_tweetDfDone = pipelineFit.transform(_tweetDf)\n",
    "#valDf = pipelineFit.transform(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------------+-------+--------+--------------------+----------+----------+----+-----+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|label| ID|           tweetTime|  query|    user|                text| timestamp|      date|year|month|day|              tokens|               words|             nostops|                  tf|            features|\n",
      "+-----+---+--------------------+-------+--------+--------------------+----------+----------+----+-----+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    4|  3|Mon May 11 03:17:...|kindle2|  tpryan|@stellargirl I lo...|1242011860|2009-05-11|2009|    5| 11|[@stellargirl, i,...|[@stellargirl, i,...|[@stellargirl, lo...|(65536,[2568,2701...|(65536,[2568,2701...|\n",
      "|    4|  4|Mon May 11 03:18:...|kindle2|  vcu451|Reading my kindle...|1242011883|2009-05-11|2009|    5| 11|[reading, my, kin...|[reading, my, kin...|[reading, kindle2...|(65536,[7472,4789...|(65536,[7472,4789...|\n",
      "|    4|  5|Mon May 11 03:18:...|kindle2|  chadfu|Ok, first assesme...|1242011934|2009-05-11|2009|    5| 11|[ok,, first, asse...|[ok, first, asses...|[ok, first, asses...|(65536,[1589,1187...|(65536,[1589,1187...|\n",
      "|    4|  6|Mon May 11 03:19:...|kindle2|   SIX15|@kenburbary You'l...|1242011944|2009-05-11|2009|    5| 11|[@kenburbary, you...|[@kenburbary, you...|[@kenburbary, lov...|(65536,[612,1198,...|(65536,[612,1198,...|\n",
      "|    4|  7|Mon May 11 03:21:...|kindle2|yamarama|@mikefish  Fair e...|1242012101|2009-05-11|2009|    5| 11|[@mikefish, , fai...|[@mikefish, , fai...|[@mikefish, , fai...|(65536,[22351,267...|(65536,[22351,267...|\n",
      "+-----+---+--------------------+-------+--------+--------------------+----------+----------+----+-----+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tweetDfDone.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래에서 보듯이 love, smile이 출력되고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------+\n",
      "|nostops                                                                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------+\n",
      "|[@stellargirl, love, kindle2, dx, cool, fantastic, right]                                                   |\n",
      "|[reading, kindle2, , love, lee, childs, good, read]                                                         |\n",
      "|[ok, first, assesment, #kindle2, ...it, fucking, rocks!!!]                                                  |\n",
      "|[@kenburbary, love, kindle2, mine, months, never, looked, back, new, big, one, huge!, need, remorse!, smile]|\n",
      "|[@mikefish, , fair, enough, kindle2, think, perfect, , smile]                                               |\n",
      "+------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tweetDfDone.select(\"nostops\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련과 테스트로 구분한다. 그 비율은 훈련 70%, 테스트 30%이다.\n",
    "\n",
    "```python\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainDf, valDf, testDf) = _tweetDfDone.randomSplit([0.6, 0.2, 0.2], seed = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB 분류\n",
    "\n",
    "Spark에서는 pyspark.ml.classification의 NaiveBayes 모델을 사용한다.\n",
    "modelType은 \"multinomial\"로 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb=NaiveBayes(featuresCol='features', labelCol='label', modelType='multinomial', predictionCol='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nb.fit(trainDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.transform(trainDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+\n",
      "|label|                text|prediction|\n",
      "+-----+--------------------+----------+\n",
      "|    0|Fuck this economy...|       0.0|\n",
      "|    0|@Karoli I firmly ...|       0.0|\n",
      "|    0|dear nike, stop w...|       0.0|\n",
      "|    0|Played with an an...|       0.0|\n",
      "|    0|US planning to re...|       0.0|\n",
      "|    0|omg so bored &amp...|       1.0|\n",
      "|    0|I'm itchy and mis...|       1.0|\n",
      "|    0|@sekseemess no. I...|       1.0|\n",
      "|    0|Blah, blah, blah ...|       0.0|\n",
      "|    0|started to think ...|       1.0|\n",
      "|    0|annoying new tren...|       0.0|\n",
      "|    0|omg. The commerci...|       0.0|\n",
      "|    0|@morind45 Because...|       0.0|\n",
      "|    0|yahoo answers can...|       1.0|\n",
      "|    0|RT @SmartChickPDX...|       1.0|\n",
      "|    0|needs someone to ...|       0.0|\n",
      "|    0|@legalgeekery Yea...|       1.0|\n",
      "|    0|Damn you North Ko...|       0.0|\n",
      "|    0|Why the hell is P...|       1.0|\n",
      "|    0|\"Are YOU burning ...|       1.0|\n",
      "+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select('label','text','prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "#evaluator=BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "evaluator=BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8280470553197826"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel = lr.fit(trainDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(valDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|    0|       0.0|\n",
      "|    0|       4.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       4.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       0.0|\n",
      "|    0|       4.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       0.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       0.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       0.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       0.0|\n",
      "|    4|       0.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select('label','prediction').show(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3701298701298702"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(valDf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.819672131147541"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
