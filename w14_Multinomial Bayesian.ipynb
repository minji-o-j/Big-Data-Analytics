{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 다항 베이지안 multinomial Bayesian\n",
    "\n",
    "**다항모델 Multinomial Bayesian**은 앞서 이항분포가 사건의 발생여부에 따른 이진적이었다면, 다항분포는 단어의 발생빈도에 따라 문서를 분류하는 문제를 예로 들 수 있다. 단어가 1번 발생할 확률, 2번 발생할 확률을 계산하게 된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  계산\n",
    "\n",
    "속성의 집합 x = (x1,...,xn), 그리고 Ck (k개의 클래스), 추론은 최대의 사후확률 $p(C_k | x_1,\\ldots,x_n)$로 결정된다.\n",
    "\n",
    "$posterior = \\frac{prior \\times likelihood}{evidence}$\n",
    "\n",
    "$P(C_k \\vert x) = \\frac{P(C_k) \\ P(x \\vert C_k)}{P(x)}$\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "P(C_k \\vert x_1, \\dots, x_n)\n",
    "    & \\varpropto P(C_k, x_1, \\dots, x_n) \\\\\n",
    "    & \\varpropto P(C_k) P(x_1 \\vert C_k) \\\n",
    "         P(x_2\\vert C_k) P(x_3\\vert C_k) \\cdots \\\\\n",
    "    & \\varpropto P(C_k) \\prod_{i=1}^n P(x_i \\vert C_k)\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "파이처럼 보이는것? product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 예제\n",
    "\n",
    "문서가 다음가 같이 주어졌다고 하자.\n",
    "문서는:\n",
    "* **문장**으로 분리한다. 이 경우 문장을 나누는 기준이 단순히 '.'으로 분리하기 쉽지 않을 수 있다. Mr. 같은 약어에 점을 포함되거나, 의문부호로 문장을 끝내거나, 마침표로 끝내지 않는 경우도 있기 때문이다.\n",
    "* **단어**로 분리한다. 단어로 분리되면, Bag of Words 모델에 따라, 단어의 앞 뒤에 무엇이 왔는지 무시되고, 오로지 단어의 집합으로 구성된다.\n",
    "* **품사**도 식별할 필요가 있다. 한국어는 조사가 붙어 격변화를 하고, 형용사도 다양하게 변화하므로 어근을 추출해 사용하기도 한다.\n",
    "* **불용어**는 제거해준다.\n",
    "\n",
    "이렇게 추출된 단어로 word vector를 생성한다. word vector는 (단어 key, 값 value)로 구성되면, 이 경우 값은 단어존재, 단어빈도, TF-IDF 등이 사용된다.\n",
    "\n",
    "베이지안과 같은 지도기계학습을 적용하기 위해서는, 각 문서에 대해 분류가 선행되어야 한다. 이를 위해서 별도로 판정 작업이 필요하다.\n",
    "문서 1, 3, 4, 6, 8, 9는 부정적, 반면에 문서2, 5, 7, 10 4건은 긍정적이다.\n",
    "부정의 label은 0, 긍정의 label은 1로 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서번호 | 문서 | 단어 | 클래스\n",
    "-----|------|------|------\n",
    "1 | I am sorry he has fleas poor dog | 'sorry','fleas','poor','dog' | 1\n",
    "2 | take the dog to the park my dog would love it | 'dog','park','dog','love' | 0\n",
    "3 | quit posting stupid worthless garbage | 'quit','posting','stupid','worthless','garbage' | 1\n",
    "4 | my dog has fleas quit buying worthless dog food stupid' | 'dog','fleas','quit','buying','worthless','dog','food','stupid' | 1\n",
    "5 | dog is so cute I love him | 'dog','cute','love' | 0\n",
    "6 | 강아지 벼룩 미안해 불쌍해 | '강아지','벼룩','미안해','불쌍해' | 1\n",
    "7 | 강아지 공원 강아지 좋아해 | '강아지','공원','강아지','좋아해' | 0\n",
    "8 | 멍청하게 쓸데없는 쓰레기 포스팅 | '멍청하게','쓸데없는','쓰레기','포스팅' | 1\n",
    "9 | 강아지 벼룩 쓸데없는 강아지 음식 사지마 멍청하게 | '강아지','벼룩','쓸데없는','강아지','음식','사지마','멍청하게' | 1\n",
    "10 | 강아지 귀여워 좋아해 | '강아지','귀여워','좋아해' | 0\n",
    "11 | my love my dog has fleas poor dog | 'dog','dog','fleas','love','poor' | ?\n",
    "12 | 강아지 벼룩 멍청하게 | '강아지','벼룩', '멍청하게' | ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사전확률\n",
    "\n",
    "* 총문서 10건\n",
    "    * 문서(0)은 4\n",
    "    * 문서(1)은 6\n",
    "\n",
    "* P(1) prior = $\\frac{N_1}{N} = \\frac{6}{10} = 0.6$ 총 10개 문서 중 6개\n",
    "* P(0) prior = $\\frac{N_0}{N} = \\frac{4}{10} = 0.4$ 총 10개 문서 중 4개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 조건확률\n",
    "\n",
    "* 전체 단어빈도 46 (영어 24 + 한글 22)\n",
    "    * 긍정(0)일때 전체 단어빈도 14 (중복제거 8: 'dog','park','love','cute','강아지','공원','좋아해','귀여워')\n",
    "    * 부정(1)일때 전체 단어빈도 32 (중복제거 21: 'sorry','fleas','poor','dog','quit','posting','stupid','worthless','garbage' ,'buying','food','강아지','벼룩','미안해','불쌍해','멍청하게','쓸데없는','쓰레기','포스팅','음식','사지마')\n",
    "* 전체 단어빈도 (중복제거) 27 (영어: 4 + 2 + 5 + 2 + 1, 한글: 4 + 2 + 4 + 2 + 1 = 14 + 13)\n",
    "\n",
    "'sorry'의 조건확률\n",
    "* P(sorry|1) = $\\frac{N_{sorry|1} + 1}{N_1 + N_{voca}} = \\frac{1+1}{32+27} = 0.03389831$\n",
    "\n",
    "'dog'의 조건확률이다. 이 단어는 양 쪽에 모두 3회 나타난다.\n",
    "* P(dog|0) = $\\frac{N_{dog|0} + 1}{N_1 + N_{voca}} = \\frac{3+1}{14+27} = 0.09756098$\n",
    "* P(dog|1) = $\\frac{N_{dog|1} + 1}{N_1 + N_{voca}} = \\frac{3+1}{32+27} = 0.06779661$\n",
    "\n",
    "'쓸데없는' 단어의 조건확률이다.\n",
    "* P(쓸데없는|0) = $\\frac{N_{쓸데없는|0} + 1}{N_1 + N_{voca}} = \\frac{0+1}{14+27} = 0.02439024$\n",
    "* P(쓸데없는|1) = $\\frac{N_{쓸데없는|1} + 1}{N_1 + N_{voca}} = \\frac{2+1}{32+27} = 0.05084746$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예측\n",
    "\n",
    "dog 2회, fleas, love, poor를 예측하면\n",
    "* post1 = prior1 x likelihood1 = prior1 x p(dog|1) x p(dog|1) x p(fleas|1) x p(poor|0) x p(love|0)\n",
    "* post0 = prior1 x likelihood1 = prior0 x p(dog|0) x p(dog|0) x p(fleas|0) x p(poor|0) x p(love|0)\n",
    "\n",
    "이 계산에 필요한 조건부 확률은:\n",
    "* p(dog|0) = 0.09756098\n",
    "* p(fleas|0) = 0.02439024\n",
    "* p(poor|0) = 0.02439024\n",
    "* p(love|0) = 0.07317073\n",
    "\n",
    "* p(dog|1) = 0.06779661\n",
    "* p(fleas|1) = 0.05084746\n",
    "* p(poor|1) = 0.03389831\n",
    "* p(love|1) = 0.01694915"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post0: 0.0000001657226369\n",
      "post1: 0.0000000805679737\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# dog^2 * fleas * poor * love\n",
    "print(\"post0: {:.16f}\".format(0.4*math.pow(0.09756098,2)*0.02439024*0.02439024*0.07317073))\n",
    "# dog^2 * fleas * poor * love\n",
    "print(\"post1: {:.16f}\".format(0.6*math.pow(0.06779661,2)*0.05084746*0.03389831*0.01694915))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "0일 경우의 사후확률 post0: 0.0000001657226369은 1일 경우의 사후확률 post1: 0.0000000805679737보다 크다. 그러므로 0으로 결정하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터\n",
    "\n",
    "앞서 구성한 텍스트 데이터를 가지고, d1, ..., d10을 구성한다.\n",
    "d1, d3, d4, d6, d8, d9는 부정적이고 d2, d5, d7, d10 4건은 긍정적이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "postingList=[['sorry','fleas','poor','dog'],\n",
    "             ['dog','park','dog','love'],\n",
    "             ['quit','posting','stupid','worthless','garbage'],\n",
    "             ['dog','fleas','quit','buying','worthless','dog','food','stupid'],\n",
    "             ['dog','cute','love'],\n",
    "             ['강아지','벼룩','미안해','불쌍해'],\n",
    "             ['강아지','공원','강아지','좋아해'],\n",
    "             ['멍청하게','쓸데없는','쓰레기','포스팅'],\n",
    "             ['강아지','벼룩','쓸데없는','강아지','음식','사지마','멍청하게'],\n",
    "             ['강아지','귀여워','좋아해']]\n",
    "classVec = [1,0,1,1,0,1,0,1,1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 단어목록\n",
    "\n",
    "#### 단어의 목록은 집합으로\n",
    "\n",
    "단어목록은 set로 선언하여, 중복이 없는 단어들을 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabSet=set([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add() 함수로 set에 단어들을 저장할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"vacabSet: {'Seoul'}\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabSet.add(\"Seoul\")\n",
    "f\"vacabSet: {vocabSet}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 배열과 다른 특징이 있다. set은 중복을 허용하지 않는다. 동일한 문자열을 또 입력해도, 하나만 보관된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"vocabSet: {'Seoul'}\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabSet.add(\"Seoul\")\n",
    "f\"vocabSet: {vocabSet}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다시 깨끗하게 비우고 시작하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vocabSet: set()'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabSet.clear()\n",
    "f\"vocabSet: {vocabSet}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 합집합\n",
    "문서로부터 단어를 하나씩 추가하는데, 이미 있는 단어가 등장하는 경우가 흔하다.\n",
    "따라서 중복을 막기 위해, union 연산 \"|\"으로 합집합을 구하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tokyo', 'Sydney', 'Seoul', 'New York'}\n"
     ]
    }
   ],
   "source": [
    "citiesA = set([\"Seoul\",\"Sydney\",\"Tokyo\"])\n",
    "citiesB = set([\"Seoul\",\"New York\"])\n",
    "citiesAll = citiesA|citiesB #A,B합집합 -> 중복제거됨\n",
    "print(citiesAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 줄씩 단어를 가져와 set()로 중복이 없도록 골라내고, 현재의 vacabSet에 더해가면서 중복이 없는 단어의 목록으로 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in postingList:\n",
    "    #vocaSet = vocaSet.union(set(doc))\n",
    "    vocabSet=vocabSet | set(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "중복이 없는, 단어의 갯수와 목록을 출력할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n of vocab: 27\n",
      "voca: {'강아지', '음식', '쓰레기', 'park', '쓸데없는', 'dog', '미안해', 'sorry', '벼룩', 'food', 'garbage', '좋아해', 'stupid', '귀여워', 'posting', 'poor', 'worthless', 'cute', '공원', '불쌍해', '포스팅', '사지마', 'quit', 'fleas', 'love', '멍청하게', 'buying'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"n of vocab: {0}\\nvoca: {1}\\n\".format(len(vocabSet),vocabSet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 단어 벡터\n",
    "\n",
    "단어벡터 Word Vector는 단어빈도로 구성된 배열을 말한다.\n",
    "이를 구성하기 위해서는:\n",
    "* **중복되지 않는 단어목록**을 구하고, 몇 개의 단어인지 알아낸다.\n",
    "* 단어의 수만큼 단어벡터를 만들고, **초기화**한다.\n",
    "* 문서의 단어벡터에 **단어빈도**를 입력한다.\n",
    "* 모든 문장에 단어별 빈도를 저장한 **단어벡터**를 생성한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어벡터 크기 결정해서 초기화\n",
    "\n",
    "vocabSet는 중복되지 않는 단어목록을 가지고 있다.\n",
    "set은 리스트와 달리 순서가 없는 특징이 있으므로 **indexing**기능을 사용할 수 없다.\n",
    "우리는 단어의 indexing이 필요하므로 set을 리스트로 변환해서 사용하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabList=list(vocabSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 수만큼 단어벡터의 크기를 정하자.\n",
    "0으로 초기화한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordVec = [0]*len(vocabList)\n",
    "wordVec = [0]*len(vocabSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력해보면, 앞서 찾아낸 중복없는 단어의 갯수만큼 word vector가 설정된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordVec: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    " print(\"wordVec: {}\".format(wordVec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한 문장을 단어벡터로 만들어 보기\n",
    "\n",
    "문장은 이미 단어로 분리되어 있다. 단어별로 초기화된 단어벡터에 빈도를 입력해보자.\n",
    "빈도는 단순히 1로 하거나, 발생빈도를 그대로 적을 수 있다.\n",
    "**단어목록에 없는 경우**에는 문제가 된다. 앞서 단어목록을 만들면서 빼먹은 경우인데, 새로 문장이 등장하면 그런 경우가 발생할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...inserting 'dog'\n",
      "...inserting 'park'\n",
      "...inserting 'dog'\n",
      "...inserting 'love'\n"
     ]
    }
   ],
   "source": [
    "for word in postingList[1]:\n",
    "    if word in vocabList:\n",
    "        print (\"...inserting '{}'\".format(word))\n",
    "        #returnVec[vocabList.index(word)] = 1 # 0 if none, 1 if exists\n",
    "        wordVec[vocabList.index(word)] += 1 # num of frequencies\n",
    "    else:\n",
    "        print (\"the word: %s is not in my Vocabulary!\".format(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 단어벡터의 단어 순서대로 빈도가 적혀졌다. 발생한 빈도가 적혀지니까, 2회 발생한 경우는 2로 1회는 1로 적힌다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 27 word vector: [0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print (\"len: {} word vector: {}\".format(len(wordVec), wordVec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 함수로 만들기\n",
    "\n",
    "단어벡터를 만드는 기능은 모든 문서에 대해 실행되어야 한다.\n",
    "앞서 단어벡터 만드는 작업을 함수로 만들어 실행하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function setOfWords2Vec\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    wordVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            print (\"...inserting '{}'\".format(word))\n",
    "            #wordVec[vocabList.index(word)] = 1 # 0 if none, 1 if exists\n",
    "            wordVec[vocabList.index(word)] += 1 # num of frequencies\n",
    "        else: print (\"the word: '{}' is not in my Vocabulary!\".format(word))\n",
    "    return wordVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어벡터 생성\n",
    "\n",
    "모든 문장에 대해 단어별 빈도를 저장한 훈련단어벡터를 생성해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...inserting 'sorry'\n",
      "...inserting 'fleas'\n",
      "...inserting 'poor'\n",
      "...inserting 'dog'\n",
      "...inserting 'dog'\n",
      "...inserting 'park'\n",
      "...inserting 'dog'\n",
      "...inserting 'love'\n",
      "...inserting 'quit'\n",
      "...inserting 'posting'\n",
      "...inserting 'stupid'\n",
      "...inserting 'worthless'\n",
      "...inserting 'garbage'\n",
      "...inserting 'dog'\n",
      "...inserting 'fleas'\n",
      "...inserting 'quit'\n",
      "...inserting 'buying'\n",
      "...inserting 'worthless'\n",
      "...inserting 'dog'\n",
      "...inserting 'food'\n",
      "...inserting 'stupid'\n",
      "...inserting 'dog'\n",
      "...inserting 'cute'\n",
      "...inserting 'love'\n",
      "...inserting '강아지'\n",
      "...inserting '벼룩'\n",
      "...inserting '미안해'\n",
      "...inserting '불쌍해'\n",
      "...inserting '강아지'\n",
      "...inserting '공원'\n",
      "...inserting '강아지'\n",
      "...inserting '좋아해'\n",
      "...inserting '멍청하게'\n",
      "...inserting '쓸데없는'\n",
      "...inserting '쓰레기'\n",
      "...inserting '포스팅'\n",
      "...inserting '강아지'\n",
      "...inserting '벼룩'\n",
      "...inserting '쓸데없는'\n",
      "...inserting '강아지'\n",
      "...inserting '음식'\n",
      "...inserting '사지마'\n",
      "...inserting '멍청하게'\n",
      "...inserting '강아지'\n",
      "...inserting '귀여워'\n",
      "...inserting '좋아해'\n"
     ]
    }
   ],
   "source": [
    "trainMat=[]\n",
    "for postinDoc in postingList:\n",
    "    trainMat.append(setOfWords2Vec(vocabList, postinDoc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련단어벡터를 출력하면, 문장의 수 10개별로 단어의 수 32개에 대해 빈도를 0, 1로 표시하여 출력하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word vector: [[0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0], [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0], [2, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"word vector: {trainMat}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### (3) 사전확률\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainNB0\n",
    "# nword_doc-np.zeros([nword,ndoc])\n",
    "numTrainDocs=len(trainMat) #6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'num of Train Docs: 10'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"num of Train Docs: {numTrainDocs}\" #전체문서의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classVec.count(1) #class [1,0,1,1,0,1,0,1,1,0] (10문장중 부정 문장의 개수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior1 = classVec.count(1)/numTrainDocs\n",
    "prior0 = classVec.count(0)/numTrainDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prior for 1: 0.6 prior for 0: 0.4'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"prior for 1: {prior1} prior for 0: {prior0}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 우도\n",
    "\n",
    "$\\frac{\\displaystyle 해당단어빈도 numWords + 1}{\\displaystyle 총단어빈도 totalNumWords + 고유단어수 vocab}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 고유단어수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "V=len(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Num of unique words: 27'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Num of unique words: {V}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 클래스별 총단어빈도, 해당 단어빈도\n",
    "\n",
    "분모, 분자로 나누어 계산한다.\n",
    "분모는 클래스별 총단어빈도를,\n",
    "분자에는 해당 단어빈도를 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "p0Num = np.zeros(V)\n",
    "p1Num = np.zeros(V)\n",
    "p0Denom = 0.0\n",
    "p1Denom = 0.0  # numWords with repetition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우도의 분모, 분자를 구한다.\n",
    "각 클래스의 전체 단어수와 해당 훈련데이터의 단어빈도를 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. class:1 빈도합:[0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0.]\n",
      "1. class:0 빈도합:[0. 0. 0. 1. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0.]\n",
      "2. class:1 빈도합:[0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 0. 0.]\n",
      "3. class:1 빈도합:[0. 0. 0. 0. 0. 3. 0. 1. 0. 1. 1. 0. 2. 0. 1. 1. 2. 0. 0. 0. 0. 0. 2. 2.\n",
      " 0. 0. 1.]\n",
      "4. class:0 빈도합:[0. 0. 0. 1. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 2. 0. 0.]\n",
      "5. class:1 빈도합:[1. 0. 0. 0. 0. 3. 1. 1. 1. 1. 1. 0. 2. 0. 1. 1. 2. 0. 0. 1. 0. 0. 2. 2.\n",
      " 0. 0. 1.]\n",
      "6. class:0 빈도합:[2. 0. 0. 1. 0. 3. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      " 2. 0. 0.]\n",
      "7. class:1 빈도합:[1. 0. 1. 0. 1. 3. 1. 1. 1. 1. 1. 0. 2. 0. 1. 1. 2. 0. 0. 1. 1. 0. 2. 2.\n",
      " 0. 1. 1.]\n",
      "8. class:1 빈도합:[3. 1. 1. 0. 2. 3. 1. 1. 2. 1. 1. 0. 2. 0. 1. 1. 2. 0. 0. 1. 1. 1. 2. 2.\n",
      " 0. 2. 1.]\n",
      "9. class:0 빈도합:[3. 0. 0. 1. 0. 3. 0. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      " 2. 0. 0.]\n",
      "\n",
      "\n",
      " 32.0 14.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(numTrainDocs):\n",
    "    if classVec[i] == 1:\n",
    "        p1Num += trainMat[i]\n",
    "        p1Denom += sum(trainMat[i]) #p1num안에 있는것 다 더함\n",
    "        print (\"{}. class:{} 빈도합:{}\".format(i,classVec[i],p1Num))\n",
    "    else:\n",
    "        p0Num += trainMat[i]\n",
    "        p0Denom += sum(trainMat[i])\n",
    "        print (\"{}. class:{} 빈도합:{}\".format(i,classVec[i],p0Num))\n",
    "        \n",
    "#마지막 2개가 빈도합\n",
    "print(\"\\n\\n\",p1Denom,p0Denom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 코드에서 각 클래스의 총단어빈도를 계산하였다.\n",
    "**총단어빈도**는 클래스 0에 14개, 클래스2에 32개이다.\n",
    "단 이들은 중복이 허용된 숫자이므로, 합한다고 해서 vacabSet에 있는 27과 같지 않다.\n",
    "분자인 단어빈도를 모두 더하면 당연히 분모인 총단어빈도가 된다.\n",
    "\n",
    "자, 이제 계산을 해보자.\n",
    "* 0에 대해, p0Denom은 총단어빈도에 해당하고 14이다.\n",
    "3번째 값은 단어빈도 1이다. 이를 총단어빈도 14로 나누어 계산한다.\n",
    "```python\n",
    "0.07142857 = 1/14 (단어빈도/총단어빈도)\n",
    "```\n",
    "\n",
    "* 1에 대해서도 마찬가지이다. 단어빈도 1인경우 총단어빈도 32로 나누어 계산한다.\n",
    "```python\n",
    "0.03125 = 1/32 (단어빈도/총단어빈도)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1Vect = p1Num/p1Denom\n",
    "p0Vect = p0Num/p0Denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---p0:\n",
      "p0Num=\n",
      "[3. 0. 0. 1. 0. 3. 0. 0. 0. 0. 0. 2. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      " 2. 0. 0.]\n",
      "p0Denom=14.0\n",
      "p0Vect\n",
      "[0.21428571 0.         0.         0.07142857 0.         0.21428571\n",
      " 0.         0.         0.         0.         0.         0.14285714\n",
      " 0.         0.07142857 0.         0.         0.         0.07142857\n",
      " 0.07142857 0.         0.         0.         0.         0.\n",
      " 0.14285714 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print (\"---p0:\\np0Num=\\n{0}\\np0Denom={1}\\np0Vect\\n{2}\".format(p0Num, p0Denom, p0Vect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---p1:\n",
      " p1Num=\n",
      "[3. 1. 1. 0. 2. 3. 1. 1. 2. 1. 1. 0. 2. 0. 1. 1. 2. 0. 0. 1. 1. 1. 2. 2.\n",
      " 0. 2. 1.]\n",
      "p1Denom=32.0\n",
      "p1Vect\n",
      "[0.09375 0.03125 0.03125 0.      0.0625  0.09375 0.03125 0.03125 0.0625\n",
      " 0.03125 0.03125 0.      0.0625  0.      0.03125 0.03125 0.0625  0.\n",
      " 0.      0.03125 0.03125 0.03125 0.0625  0.0625  0.      0.0625  0.03125]\n"
     ]
    }
   ],
   "source": [
    "print (\"---p1:\\n p1Num=\\n{0}\\np1Denom={1}\\np1Vect\\n{2}\".format(p1Num, p1Denom, p1Vect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 스무딩을 해보자.\n",
    "스무딩을 하지 않으면 0으로 연산하면서 문제가 될 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1Vect=(p1Num+1)/(p1Denom+V) #V:중복이 되지 않는 단어의 수\n",
    "p0Vect=(p0Num+1)/(p0Denom+V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---p0:\n",
      "p0Num=\n",
      "[4. 1. 1. 2. 1. 4. 1. 1. 1. 1. 1. 3. 1. 2. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1.\n",
      " 3. 1. 1.]\n",
      "p0Denom=41.0\n",
      "p0Vect\n",
      "[0.09756098 0.02439024 0.02439024 0.04878049 0.02439024 0.09756098\n",
      " 0.02439024 0.02439024 0.02439024 0.02439024 0.02439024 0.07317073\n",
      " 0.02439024 0.04878049 0.02439024 0.02439024 0.02439024 0.04878049\n",
      " 0.04878049 0.02439024 0.02439024 0.02439024 0.02439024 0.02439024\n",
      " 0.07317073 0.02439024 0.02439024]\n"
     ]
    }
   ],
   "source": [
    "print (\"---p0:\\np0Num=\\n{0}\\np0Denom={1}\\np0Vect\\n{2}\".format(p0Num+1, p0Denom+V, p0Vect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---p1:\n",
      " p1Num=\n",
      "[4. 2. 2. 1. 3. 4. 2. 2. 3. 2. 2. 1. 3. 1. 2. 2. 3. 1. 1. 2. 2. 2. 3. 3.\n",
      " 1. 3. 2.]\n",
      "p1Denom=59.0\n",
      "p1Vect\n",
      "[0.06779661 0.03389831 0.03389831 0.01694915 0.05084746 0.06779661\n",
      " 0.03389831 0.03389831 0.05084746 0.03389831 0.03389831 0.01694915\n",
      " 0.05084746 0.01694915 0.03389831 0.03389831 0.05084746 0.01694915\n",
      " 0.01694915 0.03389831 0.03389831 0.03389831 0.05084746 0.05084746\n",
      " 0.01694915 0.05084746 0.03389831]\n"
     ]
    }
   ],
   "source": [
    "print (\"---p1:\\n p1Num=\\n{0}\\np1Denom={1}\\np1Vect\\n{2}\".format(p1Num+1, p1Denom+V, p1Vect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'dog'은 양쪽에 각 3회씩 등장한다. 우도를 구해보면 각 p(dog|0)=0.09756098, p(dog|1)=0.06779661 이다.\n",
    "* P(dog|0) = $\\frac{N_{dog|0} + 1}{N_1 + N_{voca}} = \\frac{3+1}{14+27} = 0.09756098$\n",
    "* P(dog|1) = $\\frac{N_{dog|1} + 1}{N_1 + N_{voca}} = \\frac{3+1}{32+27} = 0.06779661$\n",
    "\n",
    "단어벡터의 2번째 위치를 차지하는 '쓸데없는'은 부정(1)인 경우에만 등장한다.\n",
    "* P(쓸데없는|0) = $\\frac{N_{쓸데없는|0} + 1}{N_1 + N_{voca}} = \\frac{0+1}{14+27} = 0.02439024$\n",
    "* P(쓸데없는|1) = $\\frac{N_{쓸데없는|1} + 1}{N_1 + N_{voca}} = \\frac{2+1}{32+27} = 0.05084746$\n",
    "\n",
    "이때 주의해야 할 점은, 인덱스 값을 순서를 찾아서 인덱스로 사용하면 안된다.\n",
    "인덱스는 Set에서 리스트를 생성하면서 순서가 변경되기 때문이다.\n",
    "**index() 함수로 인덱스를 찾아서** 사용하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0975609756097561, 0.06779661016949153)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0Vect[vocabList.index('dog')], p1Vect[vocabList.index('dog')] #긍/부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.024390243902439025, 0.05084745762711865)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0Vect[vocabList.index('쓸데없는')], p1Vect[vocabList.index('쓸데없는')] #긍/부"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) 예측\n",
    "\n",
    "\"my dog has fleas stupid\" 또는 \"강아지 벼룩 멍청하게\" 이런 한, 영 문장으로 분류를 해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확률계산을 할 때, 없었던 단어를 제거하면, dog, fleas, love, poor, dog이란 단어가 남는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the word: 'my' is not in my Vocabulary!\n",
      "...inserting 'dog'\n",
      "the word: 'has' is not in my Vocabulary!\n",
      "...inserting 'fleas'\n",
      "the word: 'problems' is not in my Vocabulary!\n",
      "...inserting 'love'\n",
      "the word: 'him' is not in my Vocabulary!\n",
      "...inserting 'poor'\n",
      "...inserting 'dog'\n"
     ]
    }
   ],
   "source": [
    "# classifyNB\n",
    "import math\n",
    "# my dog has flea problems. Love him, poor dog.\n",
    "testEntry1 = ['my', 'dog', 'has', 'fleas', 'problems', 'love', 'him', 'poor', 'dog']\n",
    "#testEntry1 = ['love', 'cute', 'dog']\n",
    "testData1 = np.array(setOfWords2Vec(vocabList, testEntry1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...inserting '강아지'\n",
      "...inserting '불쌍해'\n",
      "...inserting '미안해'\n",
      "the word: '어쩌나' is not in my Vocabulary!\n"
     ]
    }
   ],
   "source": [
    "testEntry2 = ['강아지', '불쌍해', '미안해', '어쩌나']\n",
    "testData2 = np.array(setOfWords2Vec(vocabList, testEntry2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 5개 dog, fleas, love, poor, dog에 대해, 2회 발생한 dog과 나머지 단어에 대해 빈도가 기록된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData1[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한글 단어 3개에 대해 빈도가 기록된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 리스트 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "post1 = prior1 * \\\n",
    "        math.pow(p1Vect[vocabList.index('dog')],2) * \\\n",
    "        p1Vect[vocabList.index('fleas')] * \\\n",
    "        p1Vect[vocabList.index('poor')] * \\\n",
    "        p1Vect[vocabList.index('love')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post1: 0.0000000806\n"
     ]
    }
   ],
   "source": [
    "print(\"post1: {:.10f}\".format(post1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "post0 = prior0 * \\\n",
    "        math.pow(p0Vect[vocabList.index('dog')],2) * \\\n",
    "        p0Vect[vocabList.index('fleas')] * \\\n",
    "        p0Vect[vocabList.index('poor')] * \\\n",
    "        p0Vect[vocabList.index('love')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post0: 0.0000001657\n"
     ]
    }
   ],
   "source": [
    "print(\"post0: {:.10f}\".format(post0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사후확률 1이 사후확률 0보다 크다. 따라서 0, 즉 **긍정**으로 예측된다.\n",
    "앞서 계산한 **가중치를 보면 dog은 긍정단어**로 계산되었다.\n",
    "부정단어 fleas, poor가 있다고 하더라도, dog과 더불어 love 단어가 사용되어서 0으로 예측된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post1>post0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([post1,post0]).argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 벡터연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### power\n",
    "\n",
    "동전의 앞면이 나올 확률은 0.5, 2회 연속 앞면이 나올 확률은 제곱을 해야 하고 0.25가 된다.\n",
    "```**```는 요소별 연산을 해서, $0.5^2$와 $0.5^1$의 결과 0.25, 0.5가 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.5 ])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([0.5,0.5])**np.array([2,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```**``` 대신 np.power() 함수를 사용해도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.5 ])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power(np.array([0.5,0.5]),np.array([2,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### product\n",
    "\n",
    "앞면이 2회 나오는 확률은 제곱을 하고, 이처럼 **거듭되는 확률은 곱셈연산**을 해서 계산한다.\n",
    "np.prod() 함수는 모든 요소의 곱셈 계산을 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Product은 곱셈을 의미한다. $\\prod_{n=2}^4\\ n = 2 \\times 3 \\times 4 = 24$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.125"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(np.power(np.array([0.5,0.5]), np.array([2,1]))) #array([0.25, 0.5 ]) 이것까지 곱해주는것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사후확률은:\n",
    "* 사전확률에\n",
    "* 해당단어의 발생확률에 빈도를 제곱한 후 (```np.power(p1Vect, testData1)```), 모두 곱셈 ```np.prod()```하여 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post1: 0.0000000805679707\n"
     ]
    }
   ],
   "source": [
    "#요소들의 곱을 일일히 해줄 필요 없다! 알아서 해줌\n",
    "post1=prior1*np.prod(np.power(p1Vect, testData1))\n",
    "print(\"post1: {:.16f}\".format(post1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post0: 0.0000001657226789\n"
     ]
    }
   ],
   "source": [
    "post0=prior0*np.prod(np.power(p0Vect, testData1))\n",
    "print(\"post0: {:.16f}\".format(post0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post1>post0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) sklearn\n",
    "\n",
    "* X는 배열이나 희소행렬 shape (n_samples, n_features)로 구성\n",
    "* y는 배열 shape (n_samples,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 27)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(trainMat).shape #문장 10개에 대하여 중복없이 27단어의 빈도수가 들어가있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(classVec).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(trainMat, classVec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예측\n",
    "\n",
    "testData1은 0, testData2는 1로 예측되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict([testData1, testData2])) #영어/한글 test문장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터\n",
    "\n",
    "데이터는 label, features로 구성하도록 하자.\n",
    "다음과 같이 단어로 입력해도 좋지만, 여기서는 단어분리와 불용어 제거를 직접 해보자.\n",
    "한글은 unicode로 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        [1,'I am sorry he has fleas poor dog'],\n",
    "        [0,'take the dog to the park my dog would love it'],\n",
    "        [1,'quit posting stupid worthless garbage'],\n",
    "        [1,'my dog has fleas quit buying worthless dog food stupid'],\n",
    "        [0,'dog is so cute I love him'],\n",
    "        [1,u'우리 강아지 벼룩 미안해 불쌍해'],\n",
    "        [0,u'강아지 공원 강아지 좋아해'],\n",
    "        [1,u'너 멍청하게 쓸데없는 쓰레기 포스팅'],\n",
    "        [1,u'강아지 벼룩 쓸데없는 강아지 음식 사지마 멍청하게'],\n",
    "        [0,u'나 강아지 귀여워 좋아해']\n",
    "    ],\n",
    "    ['cls','sent']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스키마를 살펴보면, cls는 정수, sent는 문자열로 인식되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cls: long (nullable = true)\n",
      " |-- sent: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()# 스파크 기계학습시 반드시 double로.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### StringIndexer\n",
    "문자열 컬럼을 label index로 변환\n",
    "우리의 경우는 정수이다. 이 경우는 정수를 문자열로 변환하고 난 후 label index로 변환한다.\n",
    "빈도수가 높은 문자열이 0이 배정된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"cls\", outputCol=\"label\") # double로 만들기 위함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RegexTokenizer\n",
    "텍스트를 분리해서 단어로 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "regexTok = RegexTokenizer(inputCol=\"sent\", outputCol=\"wordsRegex\", pattern=\"\\\\s+\")\n",
    "#tokenizer = Tokenizer(inputCol=\"sent\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 불용어\n",
    "\n",
    "단어에서 Stopwords를 제거하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "stop = StopWordsRemover(inputCol=\"wordsRegex\", outputCol=\"nostops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=list()\n",
    "_stopwords=stop.getStopWords()\n",
    "for e in _stopwords:\n",
    "    stopwords.append(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한글 u\"나\",u\"너\", u\"우리\"와 영어 \"take\"를 제거한다. 영어단어는 불용어 사전을 이용하고, \"take\"는 앞의 예제와 일치시키기 위해 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "_mystopwords=[u\"나\",u\"너\", u\"우리\", \"take\"]\n",
    "for e in _mystopwords:\n",
    "    stopwords.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopWordsRemover_49256797a507"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop.setStopWords(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HashingTF\n",
    "단어 -> word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"nostops\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer,regexTok,stop,hashingTF])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=pipeline.fit(df)\n",
    "trainDf = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------------------+------------------------------------------------------------------------------------+\n",
      "|label|nostops                                                 |features                                                                            |\n",
      "+-----+--------------------------------------------------------+------------------------------------------------------------------------------------+\n",
      "|0.0  |[sorry, fleas, poor, dog]                               |(262144,[6155,54556,85735,144961],[1.0,1.0,1.0,1.0])                                |\n",
      "|1.0  |[dog, park, dog, love]                                  |(262144,[54556,71826,186480],[2.0,1.0,1.0])                                         |\n",
      "|0.0  |[quit, posting, stupid, worthless, garbage]             |(262144,[1696,67357,111492,186022,247840],[1.0,1.0,1.0,1.0,1.0])                    |\n",
      "|0.0  |[dog, fleas, quit, buying, worthless, dog, food, stupid]|(262144,[1696,6155,54556,111492,121133,186022,258147],[1.0,1.0,2.0,1.0,1.0,1.0,1.0])|\n",
      "|1.0  |[dog, cute, love]                                       |(262144,[23837,54556,186480],[1.0,1.0,1.0])                                         |\n",
      "|0.0  |[강아지, 벼룩, 미안해, 불쌍해]                          |(262144,[5579,58749,60868,226029],[1.0,1.0,1.0,1.0])                                |\n",
      "|1.0  |[강아지, 공원, 강아지, 좋아해]                          |(262144,[1510,58749,242045],[1.0,2.0,1.0])                                          |\n",
      "|0.0  |[멍청하게, 쓸데없는, 쓰레기, 포스팅]                    |(262144,[81961,173322,185814,200546],[1.0,1.0,1.0,1.0])                             |\n",
      "|0.0  |[강아지, 벼룩, 쓸데없는, 강아지, 음식, 사지마, 멍청하게]|(262144,[3425,58749,60868,185814,200546,250881],[1.0,2.0,1.0,1.0,1.0,1.0])          |\n",
      "|1.0  |[강아지, 귀여워, 좋아해]                                |(262144,[58749,64077,242045],[1.0,1.0,1.0])                                         |\n",
      "+-----+--------------------------------------------------------+------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf.select(\"label\",\"nostops\",\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련데이터\n",
    "\n",
    "label은 double, features는 vector 타입으로 맞추어 주었는지 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cls: long (nullable = true)\n",
      " |-- sent: string (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- wordsRegex: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- nostops: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+\n",
      "|cls|label|            features|\n",
      "+---+-----+--------------------+\n",
      "|  1|  0.0|(262144,[6155,545...|\n",
      "|  0|  1.0|(262144,[54556,71...|\n",
      "|  1|  0.0|(262144,[1696,673...|\n",
      "|  1|  0.0|(262144,[1696,615...|\n",
      "|  0|  1.0|(262144,[23837,54...|\n",
      "|  1|  0.0|(262144,[5579,587...|\n",
      "|  0|  1.0|(262144,[1510,587...|\n",
      "|  1|  0.0|(262144,[81961,17...|\n",
      "|  1|  0.0|(262144,[3425,587...|\n",
      "|  0|  1.0|(262144,[58749,64...|\n",
      "+---+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf.select('cls','label','features').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델링\n",
    "Spark에서는 pyspark.ml.classification의 NaiveBayes 모델을 사용한다.\n",
    "\n",
    "```python\n",
    "(featuresCol='features', labelCol='label', predictionCol='prediction', probabilityCol='probability', rawPredictionCol='rawPrediction', smoothing=1.0, modelType='multinomial', thresholds=None, weightCol=None)\n",
    "```\n",
    "* smoothing 0보다 큰 값을 입력해야 하고, 기본값은 1.0\n",
    "* modelType은 \"multinomial\" (기본 값), \"bernoulli\" 2가지를 지원한다.\n",
    "TF-IDF와 같은 값은 쓰이면 multinomial을 사용한다.\n",
    "반면에 존재한다 아니다 0/1로 표시한 경우, bernoulli로 해주자.  \n",
    "정규분포면 스파크를 못쓰는것!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb=NaiveBayes(featuresCol='features', labelCol='label', modelType='multinomial', predictionCol='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nb.fit(trainDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "클래별 사전확률을 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.539, -0.8755])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "클래스의 조건확률을 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseMatrix(2, 262144, [-12.4768, -12.4768, -12.4768, -12.4768, -12.4768, -12.4768, -12.4768, -12.4768, ..., -12.4767, -12.4767, -12.4767, -12.4767, -12.4767, -12.4767, -12.4767, -12.4767], 1)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예측\n",
    "\n",
    "데이터가 적어서, train, test로 분리되어 있지 않다.\n",
    "테스트 데이터로 예측해보려면, 데이터는 DataFrame으로 넘겨주어야 한다. **컬럼명을 알아야** 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.transform(trainDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[cls: bigint, sent: string, label: double, wordsRegex: array<string>, nostops: array<string>, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------+----------+\n",
      "|label|                              sent|prediction|\n",
      "+-----+----------------------------------+----------+\n",
      "|  0.0|              I am sorry he has...|       0.0|\n",
      "|  1.0|              take the dog to t...|       1.0|\n",
      "|  0.0|              quit posting stup...|       0.0|\n",
      "|  0.0|              my dog has fleas ...|       0.0|\n",
      "|  1.0|              dog is so cute I ...|       1.0|\n",
      "|  0.0|    우리 강아지 벼룩 미안해 불쌍해|       0.0|\n",
      "|  1.0|         강아지 공원 강아지 좋아해|       1.0|\n",
      "|  0.0|너 멍청하게 쓸데없는 쓰레기 포스팅|       0.0|\n",
      "|  0.0| 강아지 벼룩 쓸데없는 강아지 음...|       0.0|\n",
      "|  1.0|           나 강아지 귀여워 좋아해|       1.0|\n",
      "+-----+----------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select('label','sent','prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가\n",
    "\n",
    "BinaryClassificationEvaluator는 두 개의 컬럼 rawPrediction과 label을 서로 비교해서 정확성을 평가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "#evaluator=BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "evaluator=BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
