{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.6 Gradient 알고리즘\n",
    "\n",
    "\n",
    "### gradient란?\n",
    "기울기 하강법 Gradient decent algorithm은 **오류를 계산하고, 반복을 통해 이를 점진적으로 줄여가면서 최적해**를 찾는다.\n",
    "gradient는 경사도를 말한다.\n",
    "처음에는 무작위 값에서 출발하여, 오류를 줄여가는 방향으로 경사도를 줄여가며 최적해를 찾아가게 된다.\n",
    "경사방법 Gradient algorithm은 **greedy 탐욕알고리즘**으로, 경사도를 선택하여 답을 찾아가지만, 그 답이 최적이라는 보장은 없다.\n",
    "오류함수가 2차함수인 경우 **local optimum**이 곧 global optimum이 된다.\n",
    "탐욕적으로 계속 최적해를 구해나가야 한다.\n",
    "\n",
    "OLS방법을 보편적으로 사용한다. 편미분해서 풀 수 있지만, 변인의 갯수만큼 방정식을 풀어야 한다.\n",
    "극대점을 찾기 위해서는 gradient 방향으로 오르는 것을 gradient ascent, 반대는 gradient descent(ascent)로 반복을 하면서 계수를 구한다.\n",
    "(matrix 연산은 $n^{2.373}$이 소요된다는 연구)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 회귀식\n",
    "\n",
    "우리가 찾고자 하는 회귀식 $h(x)$은 x를 선형적으로 $\\theta$가중치를 곱해서 더한 함수로 나타낸다.\n",
    "앞서의 회귀식과 동일하고, 기호만 다르게 사용되었다.\n",
    "$h_{\\theta}(x) = \\theta_0x_0 + \\theta_1x_1 + \\ldots + \\theta_nx_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 줄여서 표현하면:\n",
    "$h_{\\theta}(x)= \\theta_{0} + \\sum_{i=1}^n(\\theta^Tx)$  ($x_0=1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오류\n",
    "이 회귀식의 오류, 즉 $h(x) - y$는 **MSE Mean Squared Error**로 다음과 같이 나타낼수 있다.\n",
    "예측과 실제의 차이를 서로 상쇄하지 않도록 제곱을 하고, 갯수로 나누어 평균으로 계산된다.\n",
    "\n",
    "$J(\\theta) = \\frac{1}{n} \\sum_{i=1}^n (h_{\\theta}(x^{(i)}) - y^{(i)})^2$\n",
    "\n",
    "이 식이 최소화할 수 있는 가중치 $\\theta$를 추정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사도 계산\n",
    "\n",
    "gradient는 그 점에서의 **기울기**로서 방향, 크기를 나타낸다.\n",
    "벡터의 gradient는 '각 변수에 대한 편미분 벡터'이다.\n",
    "\n",
    "$\\nabla f(x,y)= \\frac{\\partial{f}}{\\partial{x}},\n",
    "                  \\frac{\\partial{f}}{\\partial{y}}$\n",
    "\n",
    "$x_{n+1}=x_n-\\alpha_n \\nabla f(x_n),\\ n \\ge 0$이면\n",
    "$f(x_0)\\ge f(x_1)\\ge f(x_2)\\ge \\cdots$이므로 `오류를 줄여나가는 방향` 결국 **최소값 local minimum**에 도달하게 된다.\n",
    "\n",
    "$\\alpha$는 학습율을 말한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 정의한 오류 $J(\\theta)$에 대해 gradient를 구하려면, 편미분을 해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial{\\theta_j}}J(\\theta)\n",
    "    &=\\frac{\\partial}{\\partial{\\theta_j}} \\frac{1}{n} (h_{\\theta}(x) - y)^2\\\\\n",
    "    &=2 \\cdot \\frac{1}{n} (h_{\\theta}(x) - y) \\cdot \\frac{\\partial}{\\partial{\\theta_j}} (h_{\\theta}(x) - y)\\\\\n",
    "    &=\\frac{2}{n}\\cdot (h_{\\theta}(x) - y) \\cdot \\frac{\\partial}{\\partial{\\theta_j}} (\\sum_{i=0}^n \\theta_i x_i - y)\\\\\n",
    "    &=\\frac{2}{n}\\cdot (h_{\\theta}(x) - y) x_j\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "n은평균 2는 미분해서 앞으로온것 (크게 신경 안써도됨)  \n",
    "위 식을 보면, **gradient는 오류에 x를 dot연산**해서 얻어진다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 갱신\n",
    "\n",
    "$\\alpha$는 학습비율 Learning Rate이며, 아래 값이 처음에는 큰 값으로 조정하다가, 반복이 계속될수록 적어지면서 0에 가까워질 때까지 현재 $\\theta$를 갱신해 나간다. 앞 2/n는 학습비율이 정해지면 서로 곱해져서 의미가 적어지게 된다.\n",
    "\n",
    "$\\theta_j := \\theta_j - \\alpha\\frac{2}{n}(h_\\theta(x)-y)x_{j})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 상수항을 넣어서 계산\n",
    "\n",
    "$h_\\theta(x)$를 아래와 같이 전개하고 계산해도 마찬가지이다. **$x_0=1$이면 전개할 필요가 없다.**\n",
    "\n",
    "* $J(\\theta)$ $MSE =\\frac{1}{n} \\sum_{i=1}^n((ax_i+b) - y_i)^2$\n",
    "\n",
    "오류를 편미분해서 기울기 gradient를 a,b에 대해 구하면 다음과 같다.\n",
    "* $\\frac{\\partial}{\\partial{a}}\n",
    "    =\\frac{2}{n} \\sum_{i=1}^n((ax_i+b) - y_i) (x_i)$　　　# x의 속성에 대한\n",
    "* $\\frac{\\partial}{\\partial{b}}\n",
    "    =\\frac{2}{n} \\sum_{i=1}^n((ax_i+b) - y_i) (1)$　　　#상수에 대한"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 알고리즘\n",
    "\n",
    "* 모든 계수 $\\theta = 1$ (또는 무작위), $\\alpha$ 설정\n",
    "* 반복\n",
    "    * $\\theta$ 갱신\n",
    "        * $\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial{\\theta_j}}J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제와 예측의 차이인 **오류 $(y-yhat)^2$를 최소화**하는 베타 값을 구해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 단계 1: 설정\n",
    "\n",
    "#### 데이터\n",
    "\n",
    "x와 y를 설정하자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x=np.array([1,2,3,4])\n",
    "y=np.array([6,5,7,10])\n",
    "#x=np.array([0.5,2.3,2.9])\n",
    "#y=np.array([1.4,1.9,3.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x에 상수항을 넣어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array([x, np.ones(len(x))])  # w0 * x0 + w1 * x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x의 shape을 컬럼 속성으로 맞추어 주기 위해 x.T로 해서, 현재 2,4 -> 4,2로 맞추어 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 1.],\n",
       "        [2., 1.],\n",
       "        [3., 1.],\n",
       "        [4., 1.]]),\n",
       " (4, 2))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습비율\n",
    "alpha는 학습율에 따라, 즉 반복마다 경사도의 일정비율만큼 감소시켜주게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### weights 초기화\n",
    "\n",
    "우리가 구하려는 theta는 처음에 1로 설정한다.\n",
    "속성 갯수 2개로 맞추어 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta=np.array(np.ones([x.shape[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 단계 2: gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### h(x)\n",
    "\n",
    "h(x)는 $x_1 \\times \\theta_{1} + x_0 \\times \\theta_{0}$으로 계산된다.\n",
    "따라서:\n",
    "```python\n",
    "h(0) = (1 * 1 + 1 * 1) = 2\n",
    "h(1) = (2 * 1 + 1 * 1) = 3\n",
    "h(2) = (3 * 1 + 1 * 1) = 4\n",
    "h(3) = (4 * 1 + 1 * 1) = 5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벡터 곱셈은 합계까지 구하지 않고, 요소별 곱셈만 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [2., 1.],\n",
       "       [3., 1.],\n",
       "       [4., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x*theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벡터는 dot연산으로 h(x)의 결과를 계산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 3., 4., 5.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(x, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수로 만들어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 3., 4., 5.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def h(x, theta):\n",
    "    return np.dot(x, theta)\n",
    "\n",
    "h(x,theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### 단순 오류\n",
    "\n",
    "h - y, 즉 예측 - 실제의 차이를 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "error=h(x,theta)-y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'simple error y-h: [-4. -2. -3. -5.]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"simple error y-h: {error}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### gradient\n",
    "\n",
    "```python\n",
    "np.sum(y-h)*(-2)/len(x)의 결과는 -7\n",
    "np.sum((y-h)*x)*(-2)/len(x) 결과는 -18.5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 상수항\n",
    "\n",
    "아래 결과는 x 길이로 나누어주어서 그렇다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(h(x,theta)-y)*(2)/len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### x에 대한 w\n",
    "\n",
    "dot연산을 해줄 경우 shape에 유의한다.\n",
    "* ```x.T```는 ```(2,4)```,\n",
    "* ```h(x,theta)```는 ```(4,)```,\n",
    "* ```(2,4)```와 ```(4,)```를 dot연산을 하면 -> ```(2, )``` 결과가 계산된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-18.5,  -7. ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(x.T*(h(x,theta)-y), axis=1)*(2)/len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### dot연산으로 한꺼번에 계산\n",
    "\n",
    "$x_0$은 1이므로 위는 ```np.dot(x.T,error)```로 바꿔쓸 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient=np.dot(x.T,error)*(2)/len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gradient: [-18.5  -7. ]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"gradient: {gradient}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단계 3: update\n",
    "\n",
    "기울기에 학습률을 곱하여 theta를 갱신하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta -= alpha*gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x의 모양은 (4,2)이고 컬럼2에 상수값을 가지고 있으므로, 아래 값은 w1, w0의 값이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'theta: [1.185 1.07 ]'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"theta: {theta}\" #업데이트됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단계 4: 반복 및 종료\n",
    "\n",
    "1회 반복이 종료되었고 다음 반복을 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [2., 1.],\n",
       "       [3., 1.],\n",
       "       [4., 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  5,  7, 10])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta=np.array(np.ones([x.shape[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(x,y,theta,alpha,numIterations):\n",
    "    #alpha=0.01\n",
    "    #theta=np.ones([x.shape[1]]) # 2 of(4,2)\n",
    "    #i=0\n",
    "    #numIterations=10000\n",
    "    #oldCost=-np.inf\n",
    "    #cost=np.inf\n",
    "    for i in range(numIterations):\n",
    "        #i+=1\n",
    "        h=np.dot(x,theta)\n",
    "        error=h-y\n",
    "        #oldCost=cost\n",
    "        cost=np.sum((h-y)**2)/len(x) #full MSE\n",
    "        #cost=np.sum((h-y)**2)/(2*len(X)) #Half MSE\n",
    "        #gradient=[np.sum(h-y)*2,np.sum((h-y)*x)*2]\n",
    "        gradient=np.dot(x.T,error)*2/len(x) #w0,w1\n",
    "        theta-=alpha*gradient\n",
    "        if i%100==0:\n",
    "            print(\"Iteration {0} | theta{1} Cost {2:.5f}\".format(i,theta,cost))\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 | theta[1.185 1.07 ] Cost 13.50000\n",
      "Iteration 100 | theta[1.93236759 1.93477432] Cost 1.46163\n",
      "Iteration 200 | theta[1.79445099 2.34026598] Cost 1.27598\n",
      "Iteration 300 | theta[1.69226343 2.64070986] Cost 1.17406\n",
      "Iteration 400 | theta[1.61654887 2.86331993] Cost 1.11811\n",
      "Iteration 500 | theta[1.56044912 3.02826002] Cost 1.08739\n",
      "Iteration 600 | theta[1.51888273 3.15047027] Cost 1.07053\n",
      "Iteration 700 | theta[1.48808464 3.2410204 ] Cost 1.06127\n",
      "Iteration 800 | theta[1.46526519 3.30811237] Cost 1.05619\n",
      "Iteration 900 | theta[1.44835741 3.35782331] Cost 1.05340\n",
      "Iteration 1000 | theta[1.43582981 3.39465599] Cost 1.05186\n",
      "Iteration 1100 | theta[1.42654764 3.4219467 ] Cost 1.05102\n",
      "Iteration 1200 | theta[1.41967014 3.4421674 ] Cost 1.05056\n",
      "Iteration 1300 | theta[1.41457434 3.45714967] Cost 1.05031\n",
      "Iteration 1400 | theta[1.41079867 3.4682506 ] Cost 1.05017\n",
      "Iteration 1500 | theta[1.40800114 3.47647568] Cost 1.05009\n",
      "Iteration 1600 | theta[1.40592834 3.48256996] Cost 1.05005\n",
      "Iteration 1700 | theta[1.40439253 3.48708543] Cost 1.05003\n",
      "Iteration 1800 | theta[1.40325459 3.49043112] Cost 1.05002\n",
      "Iteration 1900 | theta[1.40241145 3.49291006] Cost 1.05001\n",
      "Iteration 2000 | theta[1.40178673 3.4947468 ] Cost 1.05000\n",
      "Iteration 2100 | theta[1.40132385 3.49610771] Cost 1.05000\n",
      "Iteration 2200 | theta[1.40098089 3.49711605] Cost 1.05000\n",
      "Iteration 2300 | theta[1.40072678 3.49786318] Cost 1.05000\n",
      "Iteration 2400 | theta[1.4005385  3.49841675] Cost 1.05000\n",
      "Iteration 2500 | theta[1.40039899 3.49882691] Cost 1.05000\n",
      "Iteration 2600 | theta[1.40029563 3.49913081] Cost 1.05000\n",
      "Iteration 2700 | theta[1.40021904 3.49935599] Cost 1.05000\n",
      "Iteration 2800 | theta[1.4001623  3.49952283] Cost 1.05000\n",
      "Iteration 2900 | theta[1.40012025 3.49964644] Cost 1.05000\n",
      "Iteration 3000 | theta[1.4000891  3.49973804] Cost 1.05000\n",
      "Iteration 3100 | theta[1.40006602 3.4998059 ] Cost 1.05000\n",
      "Iteration 3200 | theta[1.40004891 3.49985619] Cost 1.05000\n",
      "Iteration 3300 | theta[1.40003624 3.49989344] Cost 1.05000\n",
      "Iteration 3400 | theta[1.40002685 3.49992105] Cost 1.05000\n",
      "Iteration 3500 | theta[1.4000199 3.4999415] Cost 1.05000\n",
      "Iteration 3600 | theta[1.40001474 3.49995666] Cost 1.05000\n",
      "Iteration 3700 | theta[1.40001092 3.49996788] Cost 1.05000\n",
      "Iteration 3800 | theta[1.40000809 3.4999762 ] Cost 1.05000\n",
      "Iteration 3900 | theta[1.400006   3.49998237] Cost 1.05000\n",
      "Iteration 4000 | theta[1.40000444 3.49998694] Cost 1.05000\n",
      "Iteration 4100 | theta[1.40000329 3.49999032] Cost 1.05000\n",
      "Iteration 4200 | theta[1.40000244 3.49999283] Cost 1.05000\n",
      "Iteration 4300 | theta[1.40000181 3.49999469] Cost 1.05000\n",
      "Iteration 4400 | theta[1.40000134 3.49999606] Cost 1.05000\n",
      "Iteration 4500 | theta[1.40000099 3.49999708] Cost 1.05000\n",
      "Iteration 4600 | theta[1.40000074 3.49999784] Cost 1.05000\n",
      "Iteration 4700 | theta[1.40000054 3.4999984 ] Cost 1.05000\n",
      "Iteration 4800 | theta[1.4000004  3.49999881] Cost 1.05000\n",
      "Iteration 4900 | theta[1.4000003  3.49999912] Cost 1.05000\n",
      "Iteration 5000 | theta[1.40000022 3.49999935] Cost 1.05000\n",
      "Iteration 5100 | theta[1.40000016 3.49999952] Cost 1.05000\n",
      "Iteration 5200 | theta[1.40000012 3.49999964] Cost 1.05000\n",
      "Iteration 5300 | theta[1.40000009 3.49999974] Cost 1.05000\n",
      "Iteration 5400 | theta[1.40000007 3.4999998 ] Cost 1.05000\n",
      "Iteration 5500 | theta[1.40000005 3.49999985] Cost 1.05000\n",
      "Iteration 5600 | theta[1.40000004 3.49999989] Cost 1.05000\n",
      "Iteration 5700 | theta[1.40000003 3.49999992] Cost 1.05000\n",
      "Iteration 5800 | theta[1.40000002 3.49999994] Cost 1.05000\n",
      "Iteration 5900 | theta[1.40000001 3.49999996] Cost 1.05000\n",
      "Iteration 6000 | theta[1.40000001 3.49999997] Cost 1.05000\n",
      "Iteration 6100 | theta[1.40000001 3.49999998] Cost 1.05000\n",
      "Iteration 6200 | theta[1.40000001 3.49999998] Cost 1.05000\n",
      "Iteration 6300 | theta[1.4        3.49999999] Cost 1.05000\n",
      "Iteration 6400 | theta[1.4        3.49999999] Cost 1.05000\n",
      "Iteration 6500 | theta[1.4        3.49999999] Cost 1.05000\n",
      "Iteration 6600 | theta[1.4        3.49999999] Cost 1.05000\n",
      "Iteration 6700 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 6800 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 6900 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 7000 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 7100 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 7200 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 7300 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 7400 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 7500 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 7600 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 7700 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 7800 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 7900 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 8000 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 8100 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 8200 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 8300 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 8400 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 8500 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 8600 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 8700 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 8800 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 8900 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 9000 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 9100 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 9200 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 9300 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 9400 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 9500 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 9600 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 9700 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 9800 | theta[1.4 3.5] Cost 1.05000\n",
      "Iteration 9900 | theta[1.4 3.5] Cost 1.05000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.4, 3.5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradientDescent(x, y, theta, alpha, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Python List 데이터의 Gradient Descent\n",
    "\n",
    "#### 함수\n",
    "\n",
    "리스트를 사용하면 벡터와 달리 반복문을 사용하는 것이 필요하다.\n",
    "앞서 행렬을 사용한 것과 달리, 상수항에 대한 w0을 별도로 계산하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def computeAvgError(a,b,x,y):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(x)):\n",
    "        totalError += (y[i] - (a + b* x[i])) ** 2\n",
    "    return totalError / float(len(x))\n",
    "\n",
    "#x: attribute, 1d float array or list\n",
    "#y: class, 1d int array\n",
    "#alpha: learning rate\n",
    "def gradientDescentL(x,y,alpha,iter): #list 방식으로 gradient Descent 푸는 함수\n",
    "    a=random.random()\n",
    "    b=random.random()\n",
    "    #alpha=0.01\n",
    "    n=len(x)\n",
    "    for j in range(iter):\n",
    "        aGradient = 0\n",
    "        bGradient = 0\n",
    "        for i in range(n):\n",
    "            aGradient += (2./n) * (((a + b * x[i])) - y[i])*(1)\n",
    "            bGradient += (2./n) * (((a + b * x[i])) - y[i])*(x[i])\n",
    "            #aGradient += (2./n) * (y[i] - ((a + b * x[i])))*(-1)\n",
    "            #bGradient += (2./n) * (y[i] - ((a + b * x[i])))*(-x[i])\n",
    "        a = a - (alpha * aGradient)\n",
    "        b = b - (alpha * bGradient)\n",
    "        if (j%100==0):\n",
    "            print (\"iter:{0} a={1:.3f} b={2:.3f} AvgError={3:.3f}\".format(j,a,b,computeAvgError(a,b,x,y)))\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터\n",
    "\n",
    "앞서 데이터는 상수항을 추가하였지만, 여기서는 w계산을 별도로 하기 때문에 상수항이 없게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x=np.array([1,2,3,4])\n",
    "#y=np.array([6,5,7,10])\n",
    "x=[1,2,3,4]\n",
    "y=[6,5,7,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실행\n",
    "\n",
    "a, b가 무작위로 설정되었으니 실행을 여러 번 해보면 초반의 출력결과가 다르게 되지만,\n",
    "결국 동일한 값에 수렴하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:0 a=0.420 b=0.954 AvgError=18.895\n",
      "iter:100 a=1.555 b=2.062 AvgError=1.682\n",
      "iter:200 a=2.059 b=1.890 AvgError=1.397\n",
      "iter:300 a=2.432 b=1.763 AvgError=1.240\n",
      "iter:400 a=2.709 b=1.669 AvgError=1.155\n",
      "iter:500 a=2.914 b=1.599 AvgError=1.107\n",
      "iter:600 a=3.066 b=1.548 AvgError=1.082\n",
      "iter:700 a=3.178 b=1.509 AvgError=1.067\n",
      "iter:800 a=3.262 b=1.481 AvgError=1.059\n",
      "iter:900 a=3.323 b=1.460 AvgError=1.055\n",
      "iter:1000 a=3.369 b=1.445 AvgError=1.053\n",
      "iter:1100 a=3.403 b=1.433 AvgError=1.052\n",
      "iter:1200 a=3.428 b=1.424 AvgError=1.051\n",
      "iter:1300 a=3.447 b=1.418 AvgError=1.050\n",
      "iter:1400 a=3.461 b=1.413 AvgError=1.050\n",
      "iter:1500 a=3.471 b=1.410 AvgError=1.050\n",
      "iter:1600 a=3.478 b=1.407 AvgError=1.050\n",
      "iter:1700 a=3.484 b=1.405 AvgError=1.050\n",
      "iter:1800 a=3.488 b=1.404 AvgError=1.050\n",
      "iter:1900 a=3.491 b=1.403 AvgError=1.050\n",
      "iter:2000 a=3.493 b=1.402 AvgError=1.050\n",
      "iter:2100 a=3.495 b=1.402 AvgError=1.050\n",
      "iter:2200 a=3.496 b=1.401 AvgError=1.050\n",
      "iter:2300 a=3.497 b=1.401 AvgError=1.050\n",
      "iter:2400 a=3.498 b=1.401 AvgError=1.050\n",
      "iter:2500 a=3.499 b=1.400 AvgError=1.050\n",
      "iter:2600 a=3.499 b=1.400 AvgError=1.050\n",
      "iter:2700 a=3.499 b=1.400 AvgError=1.050\n",
      "iter:2800 a=3.499 b=1.400 AvgError=1.050\n",
      "iter:2900 a=3.500 b=1.400 AvgError=1.050\n",
      "iter:3000 a=3.500 b=1.400 AvgError=1.050\n",
      "iter:3100 a=3.500 b=1.400 AvgError=1.050\n",
      "iter:3200 a=3.500 b=1.400 AvgError=1.050\n",
      "iter:3300 a=3.500 b=1.400 AvgError=1.050\n",
      "iter:3400 a=3.500 b=1.400 AvgError=1.050\n",
      "iter:3500 a=3.500 b=1.400 AvgError=1.050\n",
      "iter:3600 a=3.500 b=1.400 AvgError=1.050\n",
      "iter:3700 a=3.500 b=1.400 AvgError=1.050\n",
      "iter:3800 a=3.500 b=1.400 AvgError=1.050\n",
      "iter:3900 a=3.500 b=1.400 AvgError=1.050\n",
      "iter:4000 a=3.500 b=1.400 AvgError=1.050\n",
      "iter:4100 a=3.500 b=1.400 AvgError=1.050\n",
      "iter:4200 a=3.500 b=1.400 AvgError=1.050\n",
      "iter:4300 a=3.500 b=1.400 AvgError=1.050\n",
      "iter:4400 a=3.500 b=1.400 AvgError=1.050\n",
      "iter:4500 a=3.500 b=1.400 AvgError=1.050\n",
      "iter:4600 a=3.500 b=1.400 AvgError=1.050\n",
      "iter:4700 a=3.500 b=1.400 AvgError=1.050\n",
      "iter:4800 a=3.500 b=1.400 AvgError=1.050\n",
      "iter:4900 a=3.500 b=1.400 AvgError=1.050\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.01\n",
    "a,b=gradientDescentL(x,y,alpha,5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### w 출력\n",
    "\n",
    "함수에서는 소수점 3자리로 출력하지만, 있는 그대로 출력하면 소수점에 다소의 차이가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> a=3.4999991880367722, b=1.4000002761665138 after iterations\n"
     ]
    }
   ],
   "source": [
    "print(\"---> a={0}, b={1} after iterations\".format(a,b)) #정밀도때문에 위에랑 약간 다른것(위에는 소수점 3자리정도..?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat=list()\n",
    "for i in range(len(x)):\n",
    "    yhat.append(a + b*x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.899999464203286, 6.2999997403698, 7.700000016536313, 9.100000292702827]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.plot(x,yhat)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
