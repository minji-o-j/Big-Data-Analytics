{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문제: zscore, cdf 계산\n",
    " 성적데이터는 n이 적지만, (편의상)정규분포를 이룬다고 가정하자.\n",
    "\n",
    "- 1) 성적데이터로 DataFrame을 생성.\n",
    "- 2) zscore 컬럼을 생성. zscore를 계산하려면, 평균과 표준편차를 알아야 한다. 계산식에 F함수를 직접 사용하면 오류가 발생한다. 따로 평균과 표준편차를 구해서 계산식에서 사용해야 한다.\n",
    "- 3) cdf 컬럼을 생성. scipy.stats.norm.cdf() 함수는 데이터타입을 float로 맞추어 주어야 한다. cdf는 평균=0, 표준편차=1을 기본 값으로 누적확률을 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "marks=[\n",
    "    \"김하나, English, 100\",\n",
    "    \"김하나, Math, 80\",\n",
    "    \"임하나, English, 70\",\n",
    "    \"임하나, Math, 100\",\n",
    "    \"김갑돌, English, 82.3\",\n",
    "    \"김갑돌, Math, 98.5\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame 생성\n",
    "- 배열내에 문자열을 가지고 있는 경우, 바로 DataFrame으로 만들면 따옴표로 묶인 문자열이 한 값이 된다.\n",
    "- RDD를 만들어 주고 map() 함수로 분리한 후 DataFrame을 만들어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "_marksRdd=spark.sparkContext.parallelize(marks).map(lambda x:x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "_marksDf=spark.createDataFrame(_marksRdd, schema=[\"name\", \"subject\", \"mark\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- mark: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_marksDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스키마를 자동유추하면, mark 마저도 string으로 읽어온다. 소수점으로 변환이 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 타입 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# _marksDf = _marksDf.withColumn('markF_', F.expr(\"CAST(mark AS FLOAT)\"))  # ok\n",
    "_marksDf = _marksDf.withColumn('markF', _marksDf['mark'].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- mark: string (nullable = true)\n",
      " |-- markF: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_marksDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+-----+\n",
      "|  name| subject| mark|markF|\n",
      "+------+--------+-----+-----+\n",
      "|김하나| English|  100|100.0|\n",
      "|김하나|    Math|   80| 80.0|\n",
      "|임하나| English|   70| 70.0|\n",
      "|임하나|    Math|  100|100.0|\n",
      "|김갑돌| English| 82.3| 82.3|\n",
      "|김갑돌|    Math| 98.5| 98.5|\n",
      "+------+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_marksDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F.mean, F.stddev를 사용하면 zscore계산 오류\n",
    "- `stats.zscore()` 함수는 배열을 입력받아서 zscore를 계산한다.\n",
    "- 점수는 Column 데이터이라서, 배열로 변환해서 넘겨주려면 번거롭다.\n",
    "\n",
    "zscore를 계산하려면, 평균과 표준편차를 알아야 한다.  \n",
    "전체에 대한 평균을 계산하고, 이를 각 mark에 적용하는 계산식을 사용해 보자.  \n",
    "이 때 `F.mean()`, `F.stddev()` 함수를 사용하면 오류가 발생한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "zscoreUdf = F.udf(lambda x: (x-F.mean(x))/F.stddev) # does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zscoreUdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mean, stdev를 별도 계산해서 zscore 계산\n",
    "- 평균과 표준편차를 먼저 구하고 계산식에 넣어주자.  \n",
    "- **컬럼명은 아래와 같이 따옴표로 혹은 `F.col('markF')`로 넣어주어도 된다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "_markStats = _marksDf.select(\n",
    "    F.mean('markF').alias('mean'), # 평균 # alias: 이름 변경\n",
    "    F.stddev('markF').alias('std') # 표준편차\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(mean=88.46666717529297, std=12.786190172956093)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_markStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanMark = _markStats[0]['mean']\n",
    "stdMark = _markStats[0]['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88.46666717529297"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meanMark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.786190172956093"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stdMark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또는 2차원 인덱스를 사용하여 평균을 읽어도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88.46666717529297"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_markStats[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.786190172956093"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_markStats[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zscore를 계산하려면, FloatType으로 형변환 해준다.\n",
    "- zscore: 표준편차기준 얼마나 떨어져있는가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "zscoreUdf = F.udf(lambda x: (x-meanMark)/stdMark, FloatType()) # return as FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이렇게 해도 나오긴 한다. 그리고 zscore가 자릿수가 길~게나옴\n",
    "# _marksDf=_marksDf.withColumn(\"zscore\", (_marksDf['markF']-meanMark)/stdMark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "_marksDf=_marksDf.withColumn(\"zscore\", zscoreUdf(_marksDf['markF']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+-----+-----------+\n",
      "|  name| subject| mark|markF|     zscore|\n",
      "+------+--------+-----+-----+-----------+\n",
      "|김하나| English|  100|100.0|  0.9020148|\n",
      "|김하나|    Math|   80| 80.0| -0.6621728|\n",
      "|임하나| English|   70| 70.0| -1.4442666|\n",
      "|임하나|    Math|  100|100.0|  0.9020148|\n",
      "|김갑돌| English| 82.3| 82.3|-0.48229098|\n",
      "|김갑돌|    Math| 98.5| 98.5| 0.78470075|\n",
      "+------+--------+-----+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_marksDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cdf 계산\n",
    "norm.cdf는 numpy.float64를 반환하는데, spark에서 사용하지 않는 데이터타입이다. float()로 형변환을 해주자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "type(norm.cdf(1)) #float64는 스파크가 이해할수없다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "#bad_norm_cdf = F.udf(lambda x: norm.cdf(x), FloatType()) # FloatType() does not work\n",
    "normCdf = F.udf(lambda x: float(norm.cdf(x))) # FloatType() does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normCdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 원점수를 사용하면 1.0¶\n",
    "cdf는 평균=0, 표준편차=1을 기본 값으로 누적확률을 계산한다. 점수가 그런 범위에 있지 않고 훨씬 넘어가므로 1.0이 계산된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+-----+-----------+---+\n",
      "|  name| subject| mark|markF|     zscore|cdf|\n",
      "+------+--------+-----+-----+-----------+---+\n",
      "|김하나| English|  100|100.0|  0.9020148|1.0|\n",
      "|김하나|    Math|   80| 80.0| -0.6621728|1.0|\n",
      "|임하나| English|   70| 70.0| -1.4442666|1.0|\n",
      "|임하나|    Math|  100|100.0|  0.9020148|1.0|\n",
      "|김갑돌| English| 82.3| 82.3|-0.48229098|1.0|\n",
      "|김갑돌|    Math| 98.5| 98.5| 0.78470075|1.0|\n",
      "+------+--------+-----+-----+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_marksDf.withColumn(\"cdf\", normCdf(_marksDf['markF'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### zscore를 사용하여 cdf 계산\n",
    "zscore는 평균 0 표준편차 1을 잘 가지고있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "_marksDf=_marksDf.withColumn(\"cdf\", normCdf(_marksDf['zscore']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+-----+-----------+-------------------+\n",
      "|  name| subject| mark|markF|     zscore|                cdf|\n",
      "+------+--------+-----+-----+-----------+-------------------+\n",
      "|김하나| English|  100|100.0|  0.9020148| 0.8164754981807292|\n",
      "|김하나|    Math|   80| 80.0| -0.6621728| 0.2539302463290559|\n",
      "|임하나| English|   70| 70.0| -1.4442666| 0.0743320011235712|\n",
      "|임하나|    Math|  100|100.0|  0.9020148| 0.8164754981807292|\n",
      "|김갑돌| English| 82.3| 82.3|-0.48229098|0.31479962882028223|\n",
      "|김갑돌|    Math| 98.5| 98.5| 0.78470075| 0.7836854740814176|\n",
      "+------+--------+-----+-----+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_marksDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window 함수를 사용하여 zscore 계산\n",
    "전체에 대한 평균점수를 컬럼으로 만드려면 Window 기능을 사용해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전체 Window\n",
    "점수평균이라고 하면, Spark는 어떤 평균인지 모른다.\n",
    "(사람별 점수평균인지, 과목별평균인지 알려주어야 한다.)  \n",
    "\n",
    "정작 우리가 필요한 것은 전체점수의 평균이다. `rowsBetween(-sys.maxsize, sys.maxsize)`는 최대, 최소 값으로 윈도우를 정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql.window import Window\n",
    "byAll = Window.rowsBetween(-sys.maxsize, sys.maxsize) #모든 행으로 window 만들겠다는 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전체의 평균, 표준편차 컬럼을 만들고 계산\n",
    "전체 Window에 대해 평균, 표준편차와 과목별 평균을 계산해보자.  \n",
    "이 때 평균, 표준편차를 컬럼으로 만든 후에 zscore를 계산해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "_marksDf = _marksDf.withColumn(\"mean\", F.avg(_marksDf['markF']).over(byAll)) #over 이용해서 전체 window 넘겨준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "_marksDf = _marksDf.withColumn(\"stddev\", F.stddev(_marksDf['markF']).over(byAll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "과목별 평균은 직접 zscore 계산에 필요하지 않지만 덤으로 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "bySubject = Window.partitionBy('subject')\n",
    "_marksDf = _marksDf.withColumn(\"meanBySubject\", F.avg(_marksDf['markF']).over(bySubject))# 과목평균컬럼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+-----+-----------+-------------------+-----------------+------------------+-----------------+\n",
      "|name  |subject |mark |markF|zscore     |cdf                |mean             |stddev            |meanBySubject    |\n",
      "+------+--------+-----+-----+-----------+-------------------+-----------------+------------------+-----------------+\n",
      "|김하나| English| 100 |100.0|0.9020148  |0.8164754981807292 |88.46666717529297|12.786190172956093|84.10000101725261|\n",
      "|임하나| English| 70  |70.0 |-1.4442666 |0.0743320011235712 |88.46666717529297|12.786190172956093|84.10000101725261|\n",
      "|김갑돌| English| 82.3|82.3 |-0.48229098|0.31479962882028223|88.46666717529297|12.786190172956093|84.10000101725261|\n",
      "|김하나| Math   | 80  |80.0 |-0.6621728 |0.2539302463290559 |88.46666717529297|12.786190172956093|92.83333333333333|\n",
      "|임하나| Math   | 100 |100.0|0.9020148  |0.8164754981807292 |88.46666717529297|12.786190172956093|92.83333333333333|\n",
      "|김갑돌| Math   | 98.5|98.5 |0.78470075 |0.7836854740814176 |88.46666717529297|12.786190172956093|92.83333333333333|\n",
      "+------+--------+-----+-----+-----------+-------------------+-----------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_marksDf.show(_marksDf.count(), truncate=False) #_marksDf.count() 모든 행 보여준다 # truncate=False 값이 길어도 자르지 않고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이름으로도 되나?\n",
    "byName = Window.partitionBy('name')\n",
    "name_marksDf = _marksDf.withColumn(\"meanByName\", F.avg(_marksDf['markF']).over(byName))# 이름별 평균내기도 가능!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+-----+-----------+-------------------+----------------+\n",
      "|  name| subject| mark|markF|     zscore|                cdf|      meanByName|\n",
      "+------+--------+-----+-----+-----------+-------------------+----------------+\n",
      "|김갑돌| English| 82.3| 82.3|-0.48229098|0.31479962882028223|90.4000015258789|\n",
      "|김갑돌|    Math| 98.5| 98.5| 0.78470075| 0.7836854740814176|90.4000015258789|\n",
      "|김하나| English|  100|100.0|  0.9020148| 0.8164754981807292|            90.0|\n",
      "|김하나|    Math|   80| 80.0| -0.6621728| 0.2539302463290559|            90.0|\n",
      "|임하나| English|   70| 70.0| -1.4442666| 0.0743320011235712|            85.0|\n",
      "|임하나|    Math|  100|100.0|  0.9020148| 0.8164754981807292|            85.0|\n",
      "+------+--------+-----+-----+-----------+-------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name_marksDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "_marksDf = _marksDf.withColumn(\"zscore1\", (F.col('markF')-F.col('mean'))/F.col('stddev'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|     zscore|            zscore1|\n",
      "+-----------+-------------------+\n",
      "|  0.9020148|  0.902014804151829|\n",
      "| -0.6621728| -0.662172786480269|\n",
      "| -1.4442666| -1.444266581796318|\n",
      "|  0.9020148|  0.902014804151829|\n",
      "|-0.48229098|-0.4822909748814927|\n",
      "| 0.78470075| 0.7847007348544217|\n",
      "+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_marksDf.select('zscore', 'zscore1').show(_marksDf.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전체의 평균, 표준편차 컬럼을 만들지 않고 계산\n",
    "또는 직접 Window 함수를 직접 사용하여 zscore를 계산할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "_marksDf = _marksDf.withColumn(\"zscore2\", (F.col('markF')-F.avg('markF').over(byAll))/F.stddev('markF').over(byAll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-------------------+\n",
      "|     zscore|            zscore1|            zscore2|\n",
      "+-----------+-------------------+-------------------+\n",
      "|  0.9020148|  0.902014804151829|  0.902014804151829|\n",
      "| -0.6621728| -0.662172786480269| -0.662172786480269|\n",
      "| -1.4442666| -1.444266581796318| -1.444266581796318|\n",
      "|  0.9020148|  0.902014804151829|  0.902014804151829|\n",
      "|-0.48229098|-0.4822909748814927|-0.4822909748814927|\n",
      "| 0.78470075| 0.7847007348544217| 0.7847007348544217|\n",
      "+-----------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_marksDf.select('zscore', 'zscore1', 'zscore2').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
