{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S.3 데이터 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터로부터 좀 더 의미있는 신호를 읽기 위해서는 예측, 분류, 군집화, 추천 등 기계학습 필요\n",
    "\n",
    "\n",
    "- **ETL** (Extract, Transform, Load)은 소스에서 필요한 데이터를 추출, 변환하고, 다른 타겟으로 로딩하는 것으로 말함\n",
    "  - **추출**은 원천에서 데이터를 가져오는 것, spark에서는 예를 들어 csv, NoSQL 등에서 데이터를 읽어서 RDD, DataFrame을 생성하는 작업을 말한다.\n",
    "  - **변환**은 분석 가능한 형식으로 변환하는 것으로, 결측 값이나 이상값을 제거하거나 map() 함수를 사용하거나 데이터타입 변환을 하는 등의 작업을 한다.\n",
    "  - **로딩**은 변환한 데이터를 저장하여 놓는 것으로 Spark에서는 RDD, DataFrame의 형식으로 만들어 놓는다. 지도학습 Supervised Learning을 하려면, **DataFrame은 label, features 컬럼을, RDD는 label과 features를 가지고 있는 Labeled Point**로 구성해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import iplantuml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%plantuml --jar\n",
    "@startuml\n",
    "database source\n",
    "rectangle transform\n",
    "source -right-> transform: extract\n",
    "database target\n",
    "transform -right-> target: load\n",
    "@enduml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ml 라이브러리가 새로운 기능이 추가되고 있음. mllib는 유지보수만.\n",
    "\n",
    "패키지|설명|데이터 타입 예\n",
    ":---:|:---:|:---:\n",
    "mllib|RDD API를 제공|`pyspark.mllib.linalg.Vector` 또는 `pyspark.mllib.linalg.Matrix`\n",
    "ml|DataFrame API를 제공|`pyspark.ml.linalg.Vector` 또는 `pyspark.ml.linalg.Matrix`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S.4 RDD 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구분|설명\n",
    ":---:|:---:\n",
    "Vector|numpy vector와 같은 기능을 한다. **dense**와 **sparse** vector로 구분한다.\n",
    "Labeled Point|분류를 의미하는 클래스 또는 **label**과 속성 **features** 이 묶인 구조로서, 지도학습 supervised learning을 할 경우 사용된다.\n",
    "Matrix|numpy matrix와 같은 특징을 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.4.1 vectors\n",
    "행렬 Vector는 dense와 sparse로 구분할 수 있다.\n",
    "\n",
    "- **dense vector**는 빈 값이 별로 없이 모든 행렬이 값을 가지고 있다.\n",
    "- **sparse vector**는 빈 값이 많아서, 값이 있는 경우 그 값이 있는 인덱스로 표현해 배열을 축약하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense Vectors 밀집벡터\n",
    "- 벡터는 일련의 수로 구성이 되고, 행벡터 또는 열벡터가 될 수 있다. \n",
    "\n",
    "- 채워지는 값이 대부분 0이면 희소벡터 Sparse Vectors로 만들어 질 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy array를 사용하면 만들어지는 것이 dense vector\n",
    "dv = np.array([1.0, 2.1, 3]) #dv: dense vector numpy로 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark에서는 Vectors 명령어로 벡터를 만들 수 있다.\n",
    "\n",
    "단 **RDD는 mllib 모듈**을 사용해서 vectors를 만들고, 반면에 **DataFrame은 ml**를 사용한다는 점에 유의하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pyspark.mllib 모듈을 이용하여 Vectors를 생성\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "dv1mllib=Vectors.dense([1.0, 2.1, 3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense vector: [1.0,2.1,3.0]\n",
      "Type: <class 'pyspark.mllib.linalg.DenseVector'>\n"
     ]
    }
   ],
   "source": [
    "print (\"Dense vector: {}\\nType: {}\".format(dv1mllib, type(dv1mllib)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.ml 모듈을 이용하여 Vectors를 생성\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "dv1ml=Vectors.dense([1.0, 2.1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml의 dense vector: [1.0,2.1,3.0]\n"
     ]
    }
   ],
   "source": [
    "print (\"ml의 dense vector: {}\".format(dv1ml))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 2.1 3.0 "
     ]
    }
   ],
   "source": [
    "# numpy array와 같은 특징 : 인덱스로 값 읽기, 반복문 사용 가능\n",
    "for e in dv1mllib:\n",
    "    print (e, end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.41"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  product, dot, norm과 같은 벡터 연산을 할 수도 있다\n",
    "dv1mllib.dot(dv1mllib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.41"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(dv,dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([1.0, 4.41, 9.0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 더하기, 빼기, 곱하기, 나누기 연산도 가능\n",
    "# 곱하기 연산을 해보자. dot와 달리 항목별로 실행됨.\n",
    "dv1mllib*dv1mllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse Vectors 희소행렬\n",
    "- 행렬에는 0 값이 많이 존재하기 때문에, 0값이 아닌 **NZ** Nonzero만 저장하면 훨씬 효율적\n",
    "- **sparse는 실제 값이 없는 요소**, **'0'을 제거**하여 만든 vector이다.\n",
    "  - Spark에서 type field (1 바이트 길이)를 통해 식별한다 (0: sparse, 1: dense)\n",
    "  \n",
    "\n",
    "예를 들어, 다음은 1차원 dense vector이다.\n",
    "\n",
    "> [160, 69, 0, 0, 24]\n",
    "\n",
    "sparse vectors는 값 중에 **0이 포함된 경우 이를 생략**하고, 값이 있는 요소 Nonzero만 남기게 된다.  \n",
    "3은 컬럼 갯수, 0, 1, 4는 값이 있는 컬럼, [160.0, 69.0, 24.0]는 실제 값을 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv1 = Vectors.sparse(5,[0,1,4],[160.0,69.0,24.0])\n",
    "\n",
    "# 요소가 5개인데, 0,1,4번째에 값이 있고, 그 값은 순서대로 ---이다.\n",
    "# 장소 효율적으로 사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.linalg.SparseVector"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mllib 또는 ml인지 확인\n",
    "type(sv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`toArray()` 함수를 사용하면 sparse에서 dense로 벡터를 변환할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([160.,  69.,   0.,   0.,  24.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv1.toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.4.2 Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseMatrix(3, 2, [1.0, 2.0, 3.0, 4.0, 5.0, 6.0], False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Matrices\n",
    "\n",
    "Matrices.dense(3, 2, [1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse Vectors\n",
    ">[1 0 2]  \n",
    "[0 0 3]  \n",
    "[4 5 6]\n",
    "\n",
    "위와 같은 2차원 dense vectors를 sparse vectors의 배열 방식으로 표현해보자. 우선 다음과 같이 행, 열, 값 vector를 만든다.\n",
    "\n",
    ">행 | 0 | 0 | 1 | 2 | 2 | 2   `# 0번째에 값 2번 등장 -> 0 2개....`  \n",
    "열 | 0 | 2 | 2 | 0 | 1 | 2   `몇번째 행에 값 나오는가를 세로로 생각`  \n",
    "값 | 1 | 2 | 3 | 4 | 5 | 6\n",
    "\n",
    "행을 보면 0번째에 '1','2' 1번째에 '3', 2번째에 '4','5','6'이므로 0,0,1,2,2,2  \n",
    "열을 보면 0번째에 '1', 2번째 '2','3', 0번째 '4', 1번째 '5', 2번째 '6'이므로 0,2,2,0,1,2\n",
    "\n",
    "행, 열, 데이터를 한 쌍으로 읽으면 된다. 즉 행 0, 열 0의 위치에 1, 행 0, 열 2의 위치에 2. 이런 식으로 6개의 데이터가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "row = np.array([0, 0, 1, 2, 2, 2])\n",
    "col = np.array([0, 2, 2, 0, 1, 2])\n",
    "data = np.array([1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy의 sparse vector로 표현해보기\n",
    "import scipy.sparse as sps\n",
    "\n",
    "mtx = sps.csc_matrix((data, (row, col)), shape=(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 2]\n",
      " [0 0 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "print (mtx.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse Vectors의 CSR (Compressed Sparse Row) 또는 Yale Format\n",
    "다음 5개의 값으로 표현된다.\n",
    "\n",
    "1. 첫째 행 개수 (int)\n",
    "2. 둘째 열 개수 (int)\n",
    "3. 세째 int[]은 열 포인터 (IA):\n",
    "  - IA[0]=0 시작 값은 0으로, IA[i]=IA[i-1] + (i-1)열의 NNZ (NZ 개수, Num of NonZeroes)\n",
    "4. 네째 int[]은 행 인덱스 (JA): 각 NZ의 행 인덱스\n",
    "5. 마지막은 소수 (double)로 실제 값 리스트\n",
    "\n",
    "\n",
    "2차원 dense vectors를 만들어 보자. 행의 갯수, 열의 갯수, 실제 값을 넣어주면 생성된다.\n",
    "\n",
    "- 6은 행 갯수\n",
    "- 4는 열 갯수\n",
    "- 다음 수는 행렬을 해체하여 연속적인 수로 나열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Matrices\n",
    "\n",
    "dm = Matrices.dense(6,4,[1, 2, 0, 0, 0, 0, 0, 3, 0, 4, 0, 0, 0, 0, 5, 6, 7, 0, 0, 0, 0, 0, 0, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [2., 3., 0., 0.],\n",
       "       [0., 0., 5., 0.],\n",
       "       [0., 4., 6., 0.],\n",
       "       [0., 0., 7., 0.],\n",
       "       [0., 0., 0., 8.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.toArray() #2차원 배열로 변환됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음을 희소행렬로 변환해보자.\n",
    "\n",
    "> [ 1.,  0.,  0.,  0.]  \n",
    "[ 2.,  3.,  0.,  0.]  \n",
    "[ 0.,  0.,  5.,  0.]  \n",
    "[ 0.,  4.,  6.,  0.]  \n",
    "[ 0.,  0.,  7.,  0.]  \n",
    "[ 0.,  0.,  0.,  8.]\n",
    "\n",
    "- 6은 행 갯수\n",
    "- 4는 열 갯수\n",
    "\n",
    "\n",
    "- 다음은 열포인터 [0, 2, 4, 7, 8]\n",
    "  - 열로 세어서 요소의 개수 2, 2, 3, 1을 가지고 구성을 한다. 즉 열로 요소가 2, 2, 3, 1개 이다.\n",
    "  - 0:IA[0]=0으로 시작, **2**:IA[1]=IA[0]+2, **4**:IA[2]=IA[1]+2 **7**:IA[3]=IA[2]+3, **8**:IA[4]=IA[3]+1\n",
    "  \n",
    "  \n",
    "- 다음은 행인덱스 JA [0, 1, 1, 3, 2, 3, 4, 5]\n",
    "  - 행으로 세어서 (단 컬럼순을 지켜서) 0 1 1 3 2 3 4 5 -- 열 순서대로 센다(0 1 1 2 3 3 4 5가 아니라)\n",
    "  - 1:0행, 2:1행, 3:1행, 4:3행, 5:2행, 6:3행, 7:4행, 8:5행\n",
    "  \n",
    "  \n",
    "- 마지막은 소수로 실제 값 리스트 [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseMatrix(6, 4, [0, 2, 4, 7, 8], [0, 1, 1, 3, 2, 3, 4, 5], [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0], False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.toSparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = Matrices.sparse(3, 2, [0, 1, 3], [0, 2, 1], [9, 6, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=sm.toDense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[9., 0.],\n",
      "             [0., 8.],\n",
      "             [0., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.4.3 분산 Matrix (Distributed Matrix)\n",
    "- matrix가 나누어져서 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Row Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [[1.0,2.0,3.0],[1.1,2.1,3.1],[1.2,2.2,3.3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "my=spark.sparkContext.parallelize(p) #RDD 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 2.0, 3.0], [1.1, 2.1, 3.1], [1.2, 2.2, 3.3]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my.collect() #RDD 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "rm=RowMatrix(my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.mllib.linalg.distributed.RowMatrix'>\n"
     ]
    }
   ],
   "source": [
    "print (type(rm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([1.0, 2.0, 3.0]),\n",
       " DenseVector([1.1, 2.1, 3.1]),\n",
       " DenseVector([1.2, 2.2, 3.3])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm.rows.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm.numRows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm.numCols()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexed Row Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import IndexedRow\n",
    "\n",
    "# RDD 생성\n",
    "irRdd = spark.sparkContext.parallelize([\n",
    "    IndexedRow(1, [3, 1, 2]), #2개의 요소로 되어있는 형식, 앞의 번호: 순서가 있는 data(시계열 data)일 때\n",
    "    IndexedRow(2, [1, 3, 2]),\n",
    "    IndexedRow(3, [5, 4, 3]),\n",
    "    IndexedRow(4, [6, 7, 4]),\n",
    "    IndexedRow(5, [8, 9, 2]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import IndexedRowMatrix\n",
    "\n",
    "irm = IndexedRowMatrix(irRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(irm.numRows())\n",
    "print(irm.numCols())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.4.4 Labeled Point\n",
    "- 로컬벡터 - 레이블을 가지고있는 밀집or 희소행렬\n",
    "- 레이블은 **double 형식**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### label, features로 구성\n",
    "분류 및 회귀분석에 사용되는 데이터 타잎이다. 'label'과 'features'로 구성된다.\n",
    "\n",
    "구분|지도학습을 하기 위한 label과 features의 구성\n",
    ":---:|:---:\n",
    "label|supervised learning에서 '구분 값'으로 사용한다. 데이터타입은 'DoubleType'으로 설정되어야 한다.\n",
    "features|sparse, dense 모두 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [1.0,2.0,3.0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "LabeledPoint(1.0, [1.0, 2.0, 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1992.0, (10,[0,1,2],[3.0,5.5,10.0]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sparse vectors(희소행렬)로 features 구성해보기\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "LabeledPoint(1992, Vectors.sparse(10, {0: 3.0, 1:5.5, 2: 10.0})) #개수, labled point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "서로 다른 패키지의 데이터타잎 `mllib LabeledPoint`와 `ml Vectors`를 혼용하면, 형변환 오류가 발생한다. 이러한 오류는 패키지를 혼용하지 않으면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [1.0,2.1,3.0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "LabeledPoint(1.0, dv1mllib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [1.0,2.1,3.0])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "LabeledPoint(1.0, Vectors.fromML(dv1ml))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.4.5 svm의 입력파일 형식\n",
    "limsvm은 기계학습 모델인 svm을 위한 입력데이터 형식이다. 0은 label, 나머지는 index:value 쌍으로 구성한다.\n",
    ">[label] [index1]:[value1] [index2]:[value2] ...  \n",
    "[label] [index1]:[value1] [index2]:[value2] ...\n",
    "\n",
    "- ex  \n",
    "> 0 128:51 129:159 130:253 131:159 132:50 155:48 156:238 157:252 158:252 159:252 160:237 182:54 183:227 184:253 185:252 186:239 187:233 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "fsvm=os.path.join(os.environ[\"SPARK_HOME\"],'data','mllib','sample_libsvm_data.txt')\n",
    "dfsvm = spark.read.format(\"libsvm\").load(fsvm) #별도의 형식이라 지정이 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dfsvm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfsvm.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0.0, features=SparseVector(692, {127: 51.0, 128: 159.0, 129: 253.0, 130: 159.0, 131: 50.0, 154: 48.0, 155: 238.0, 156: 252.0, 157: 252.0, 158: 252.0, 159: 237.0, 181: 54.0, 182: 227.0, 183: 253.0, 184: 252.0, 185: 239.0, 186: 233.0, 187: 252.0, 188: 57.0, 189: 6.0, 207: 10.0, 208: 60.0, 209: 224.0, 210: 252.0, 211: 253.0, 212: 252.0, 213: 202.0, 214: 84.0, 215: 252.0, 216: 253.0, 217: 122.0, 235: 163.0, 236: 252.0, 237: 252.0, 238: 252.0, 239: 253.0, 240: 252.0, 241: 252.0, 242: 96.0, 243: 189.0, 244: 253.0, 245: 167.0, 262: 51.0, 263: 238.0, 264: 253.0, 265: 253.0, 266: 190.0, 267: 114.0, 268: 253.0, 269: 228.0, 270: 47.0, 271: 79.0, 272: 255.0, 273: 168.0, 289: 48.0, 290: 238.0, 291: 252.0, 292: 252.0, 293: 179.0, 294: 12.0, 295: 75.0, 296: 121.0, 297: 21.0, 300: 253.0, 301: 243.0, 302: 50.0, 316: 38.0, 317: 165.0, 318: 253.0, 319: 233.0, 320: 208.0, 321: 84.0, 328: 253.0, 329: 252.0, 330: 165.0, 343: 7.0, 344: 178.0, 345: 252.0, 346: 240.0, 347: 71.0, 348: 19.0, 349: 28.0, 356: 253.0, 357: 252.0, 358: 195.0, 371: 57.0, 372: 252.0, 373: 252.0, 374: 63.0, 384: 253.0, 385: 252.0, 386: 195.0, 399: 198.0, 400: 253.0, 401: 190.0, 412: 255.0, 413: 253.0, 414: 196.0, 426: 76.0, 427: 246.0, 428: 252.0, 429: 112.0, 440: 253.0, 441: 252.0, 442: 148.0, 454: 85.0, 455: 252.0, 456: 230.0, 457: 25.0, 466: 7.0, 467: 135.0, 468: 253.0, 469: 186.0, 470: 12.0, 482: 85.0, 483: 252.0, 484: 223.0, 493: 7.0, 494: 131.0, 495: 252.0, 496: 225.0, 497: 71.0, 510: 85.0, 511: 252.0, 512: 145.0, 520: 48.0, 521: 165.0, 522: 252.0, 523: 173.0, 538: 86.0, 539: 253.0, 540: 225.0, 547: 114.0, 548: 238.0, 549: 253.0, 550: 162.0, 566: 85.0, 567: 252.0, 568: 249.0, 569: 146.0, 570: 48.0, 571: 29.0, 572: 85.0, 573: 178.0, 574: 225.0, 575: 253.0, 576: 223.0, 577: 167.0, 578: 56.0, 594: 85.0, 595: 252.0, 596: 252.0, 597: 252.0, 598: 229.0, 599: 215.0, 600: 252.0, 601: 252.0, 602: 252.0, 603: 196.0, 604: 130.0, 622: 28.0, 623: 199.0, 624: 252.0, 625: 252.0, 626: 253.0, 627: 252.0, 628: 252.0, 629: 233.0, 630: 145.0, 651: 25.0, 652: 128.0, 653: 252.0, 654: 253.0, 655: 252.0, 656: 141.0, 657: 37.0}))]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsvm.take(1) #label에 features로 별도로 구성되어있음을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLUtils를 사용하여 RDD 읽기\n",
    "또는 MLUtils.loadLibSVMFile로 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "data = MLUtils.loadLibSVMFile(spark.sparkContext, fsvm) #sparkContext라는 설정 해주어야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = data.map(lambda x: x.label)\n",
    "features = data.map(lambda x: x.features) #map으로 label과 features 나누어 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(692, {127: 51.0, 128: 159.0, 129: 253.0, 130: 159.0, 131: 50.0, 154: 48.0, 155: 238.0, 156: 252.0, 157: 252.0, 158: 252.0, 159: 237.0, 181: 54.0, 182: 227.0, 183: 253.0, 184: 252.0, 185: 239.0, 186: 233.0, 187: 252.0, 188: 57.0, 189: 6.0, 207: 10.0, 208: 60.0, 209: 224.0, 210: 252.0, 211: 253.0, 212: 252.0, 213: 202.0, 214: 84.0, 215: 252.0, 216: 253.0, 217: 122.0, 235: 163.0, 236: 252.0, 237: 252.0, 238: 252.0, 239: 253.0, 240: 252.0, 241: 252.0, 242: 96.0, 243: 189.0, 244: 253.0, 245: 167.0, 262: 51.0, 263: 238.0, 264: 253.0, 265: 253.0, 266: 190.0, 267: 114.0, 268: 253.0, 269: 228.0, 270: 47.0, 271: 79.0, 272: 255.0, 273: 168.0, 289: 48.0, 290: 238.0, 291: 252.0, 292: 252.0, 293: 179.0, 294: 12.0, 295: 75.0, 296: 121.0, 297: 21.0, 300: 253.0, 301: 243.0, 302: 50.0, 316: 38.0, 317: 165.0, 318: 253.0, 319: 233.0, 320: 208.0, 321: 84.0, 328: 253.0, 329: 252.0, 330: 165.0, 343: 7.0, 344: 178.0, 345: 252.0, 346: 240.0, 347: 71.0, 348: 19.0, 349: 28.0, 356: 253.0, 357: 252.0, 358: 195.0, 371: 57.0, 372: 252.0, 373: 252.0, 374: 63.0, 384: 253.0, 385: 252.0, 386: 195.0, 399: 198.0, 400: 253.0, 401: 190.0, 412: 255.0, 413: 253.0, 414: 196.0, 426: 76.0, 427: 246.0, 428: 252.0, 429: 112.0, 440: 253.0, 441: 252.0, 442: 148.0, 454: 85.0, 455: 252.0, 456: 230.0, 457: 25.0, 466: 7.0, 467: 135.0, 468: 253.0, 469: 186.0, 470: 12.0, 482: 85.0, 483: 252.0, 484: 223.0, 493: 7.0, 494: 131.0, 495: 252.0, 496: 225.0, 497: 71.0, 510: 85.0, 511: 252.0, 512: 145.0, 520: 48.0, 521: 165.0, 522: 252.0, 523: 173.0, 538: 86.0, 539: 253.0, 540: 225.0, 547: 114.0, 548: 238.0, 549: 253.0, 550: 162.0, 566: 85.0, 567: 252.0, 568: 249.0, 569: 146.0, 570: 48.0, 571: 29.0, 572: 85.0, 573: 178.0, 574: 225.0, 575: 253.0, 576: 223.0, 577: 167.0, 578: 56.0, 594: 85.0, 595: 252.0, 596: 252.0, 597: 252.0, 598: 229.0, 599: 215.0, 600: 252.0, 601: 252.0, 602: 252.0, 603: 196.0, 604: 130.0, 622: 28.0, 623: 199.0, 624: 252.0, 625: 252.0, 626: 253.0, 627: 252.0, 628: 252.0, 629: 233.0, 630: 145.0, 651: 25.0, 652: 128.0, 653: 252.0, 654: 253.0, 655: 252.0, 656: 141.0, 657: 37.0})]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.4.6 TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiRdd3 = spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))\\\n",
    "    .map(lambda line: line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(1048576, {1026674: 1.0}),\n",
       " SparseVector(1048576, {148618: 1.0, 183975: 1.0, 216207: 1.0, 261052: 1.0, 617454: 1.0, 696349: 1.0, 721336: 1.0, 816618: 1.0, 897662: 1.0}),\n",
       " SparseVector(1048576, {60386: 1.0, 177421: 1.0, 568609: 1.0, 569458: 1.0, 847171: 1.0, 850510: 1.0, 1040679: 1.0}),\n",
       " SparseVector(1048576, {261052: 4.0, 816618: 4.0}),\n",
       " SparseVector(1048576, {60386: 4.0, 594754: 4.0}),\n",
       " SparseVector(1048576, {21980: 1.0, 70882: 1.0, 274690: 1.0, 357784: 1.0, 549790: 1.0, 597434: 1.0, 804583: 1.0, 829803: 1.0, 935701: 1.0}),\n",
       " SparseVector(1048576, {154253: 1.0, 261052: 1.0, 438276: 1.0, 460085: 1.0, 585459: 1.0, 664288: 1.0, 816618: 1.0, 935701: 2.0, 948143: 1.0, 1017889: 1.0}),\n",
       " SparseVector(1048576, {270017: 1.0, 472985: 1.0, 511771: 1.0, 718483: 1.0, 820917: 1.0}),\n",
       " SparseVector(1048576, {34116: 1.0, 87407: 1.0, 276491: 1.0, 348943: 1.0, 482882: 1.0, 549350: 1.0, 721336: 1.0, 816618: 1.0, 1025622: 1.0}),\n",
       " SparseVector(1048576, {1769: 1.0, 151357: 1.0, 500659: 1.0, 547760: 1.0, 979482: 1.0})]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.feature import HashingTF\n",
    "\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(wikiRdd3) #rdd를 입력으로 해서 TF 생성\n",
    "tf.collect() #row 벡터, 1048576: 전체 개수? 1026674: 1.0 -> 단어;빈도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.4.7 TF-IDF\n",
    "IDF는 전체에서 몇 개의 문서에 씌였는지를 반대로 계산한 값이다. 뒤 DataFrame를 사용하여 TF-IDF를 계산하면서 자세히 설명하기로 한다.\n",
    "\n",
    "지도학습 훈련 data를 만들기 위해서 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(1048576, {1026674: 1.7047}),\n",
       " SparseVector(1048576, {148618: 1.7047, 183975: 1.7047, 216207: 1.7047, 261052: 1.0116, 617454: 1.7047, 696349: 1.7047, 721336: 1.2993, 816618: 0.7885, 897662: 1.7047}),\n",
       " SparseVector(1048576, {60386: 1.2993, 177421: 1.7047, 568609: 1.7047, 569458: 1.7047, 847171: 1.7047, 850510: 1.7047, 1040679: 1.7047}),\n",
       " SparseVector(1048576, {261052: 4.0464, 816618: 3.1538}),\n",
       " SparseVector(1048576, {60386: 5.1971, 594754: 6.819}),\n",
       " SparseVector(1048576, {21980: 1.7047, 70882: 1.7047, 274690: 1.7047, 357784: 1.7047, 549790: 1.7047, 597434: 1.7047, 804583: 1.7047, 829803: 1.7047, 935701: 1.2993}),\n",
       " SparseVector(1048576, {154253: 1.7047, 261052: 1.0116, 438276: 1.7047, 460085: 1.7047, 585459: 1.7047, 664288: 1.7047, 816618: 0.7885, 935701: 2.5986, 948143: 1.7047, 1017889: 1.7047}),\n",
       " SparseVector(1048576, {270017: 1.7047, 472985: 1.7047, 511771: 1.7047, 718483: 1.7047, 820917: 1.7047}),\n",
       " SparseVector(1048576, {34116: 1.7047, 87407: 1.7047, 276491: 1.7047, 348943: 1.7047, 482882: 1.7047, 549350: 1.7047, 721336: 1.2993, 816618: 0.7885, 1025622: 1.7047}),\n",
       " SparseVector(1048576, {1769: 1.7047, 151357: 1.7047, 500659: 1.7047, 547760: 1.7047, 979482: 1.7047})]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.4.8 StandardScaler\n",
    "데이터를 표준화하려면 1) 평균과 표준편차를 계산하고, 2) 측정값에서 평균을 빼고, 표준편차로 나누어 주면 된다. 즉 zscore를 계산하는 것과 같다.\n",
    "\n",
    "$$ z = \\frac {\\bar{x_n} - \\mu} {\\sigma / \\sqrt{n}} $$\n",
    "\n",
    "#### 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tRdd = spark.sparkContext\\\n",
    "    .textFile(os.path.join('data', 'ds_spark_heightweight.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정규화 할 값만 추출\n",
    "탭으로 분리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '65.78', '112.99']]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tRdd.map(lambda x: x.split('\\t')).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', 65.78, 112.99]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tRdd.map(lambda x: x.split('\\t')).map(lambda x: [str(x[0]), float(x[1]), float(x[2])]).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', 65.78, 112.99]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#몇개의 명령어 붙임: pipeline\n",
    "tRdd.map(lambda x: x.split('\\t'))\\\n",
    "    .map(lambda x: [str(x[0]), float(x[1]), float(x[2])])\\\n",
    "    .take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "_tRdd =tRdd\\\n",
    "    .map(lambda x: x.split('\\t'))\\\n",
    "    .map(lambda x: [str(x[0]), float(x[1]), float(x[2])])\\\n",
    "    .map(lambda x: Vectors.dense([x[1], x[2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "_tRdd =tRdd\\\n",
    "    .map(lambda x: x.split('\\t'))\\\n",
    "    .map(lambda x: [str(x[0]), float(x[1]), float(x[2])])\\\n",
    "    .map(lambda x: [x[1], x[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 표준화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import StandardScaler\n",
    "scaler1 = StandardScaler().fit(_tRdd)\n",
    "scaler2 = StandardScaler(withMean=True, withStd=True).fit(_tRdd) #평균, 표춘편차 적용하여 fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([-1.2458, -1.2299]),\n",
       " DenseVector([1.9011, 0.5934]),\n",
       " DenseVector([0.7388, 1.8767]),\n",
       " DenseVector([0.0919, 1.0473]),\n",
       " DenseVector([-0.1439, 1.1993])]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler2.transform(_tRdd).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
