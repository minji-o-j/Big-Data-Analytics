{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S.5 DataFrame 변환\n",
    "DataFrame으로 만들어진 데이터를 변환해보자. 이러한 작업이 필요한 이유는 **기계학습에 넘겨줄 입력데이터를 형식에 맞추어야 하기 때문**이다.\n",
    "\n",
    "데이터는 형식에 맞게 변환되고, 군집화, 회귀분석, 분류, 추천 모델 등에 입력으로 사용된다 물론 데이터는 '일련의 수' 또는 '텍스트'로 구성된다. 이런 데이터로부터 특징을 추출하여 feature vectors를 구성한다. 지도학습을 하는 경우에는 class 또는 label 값이 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.5.1 Labeled Point를 label, features 컬럼으로 분해\n",
    "RDD LabeledPoint는 label과 vectors로 구성되어 있다. 따라서 LabeledPoint를 DataFrame으로 읽어오면, 2개의 컬럼으로 생성된다. 이를 label, features 컬럼으로 맞추어 주도록 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 레이블이 있는 Python List에서 DataFrame 생성\n",
    "label과 features를 가지고 가지고 있는 데이터를 생성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [[1, [1.0, 2.0, 3.0]], [1, [1.1, 2.1, 3.1]], [0, [1.2, 2.2, 3.3]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 1\n",
      "features: [1.0, 2.0, 3.0]\n"
     ]
    }
   ],
   "source": [
    "print (\"label: {}\\nfeatures: {}\".format(p[0][0], p[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf=spark.createDataFrame(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=1, _2=[1.0, 2.0, 3.0]),\n",
       " Row(_1=1, _2=[1.1, 2.1, 3.1]),\n",
       " Row(_1=0, _2=[1.2, 2.2, 3.3])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDf.collect() #_1, _2라고 자동으로 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LabeledPoint에서 DataFrame 생성\n",
    "Python List를 LabeledPoint로 만들어 보자. LabeledPoint는 RDD에서 사용하는 구조로서 mllib 라이브러리를 사용해서 만들고 있다. DataFrame은 LabeledPoint를 컬럼으로 가지고 있지 않는다.\n",
    "\n",
    "\n",
    "LabeledPoint벡터로 df을 만들면 자동으로 label과 features가 구분된다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "p = [LabeledPoint(1, [1.0,2.0,3.0]),\n",
    "     LabeledPoint(1, [1.1,2.1,3.1]),\n",
    "     LabeledPoint(0, [1.2,2.2,3.3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf=spark.createDataFrame(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([1.0, 2.0, 3.0]), label=1.0),\n",
       " Row(features=DenseVector([1.1, 2.1, 3.1]), label=1.0),\n",
       " Row(features=DenseVector([1.2, 2.2, 3.3]), label=0.0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mllib.linalg.Vectors를 사용하여 DataFrame을 생성\n",
    "앞서 mllib와 ml 모듈을 섞어서 사용하지 않아야 한다고 했다. mllib vectors를 사용해도 DataFrame을 생성하는데 문제가 없다. 컬럼명을 [\"label\", \"features\"]으로 하지 않으면, 자동명명되니 주의하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabeledPoint가 아닌 경우\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "trainDf = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, 1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, 0.5]))], [\"label\", \"features\"]) #컬럼 명 주어야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=1.0, features=DenseVector([0.0, 1.1, 0.1])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 1.0, 1.0])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 1.3, 1.0])),\n",
       " Row(label=1.0, features=DenseVector([0.0, 1.2, 0.5]))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD에서 DataFrame 생성\n",
    "rdd에서 DataFrame을 생성하면 labe, features이 당연히 생성이 되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.mllib.linalg import SparseVector # mllib ok\n",
    "from pyspark.ml.linalg import SparseVector # ml ok\n",
    "\n",
    "_rdd = spark.sparkContext.parallelize([\n",
    "    (0.0, SparseVector(4, {1: 1.0, 3: 5.5})),\n",
    "    (1.0, SparseVector(4, {0: -1.0, 2: 0.5}))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df=_rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: double (nullable = true)\n",
      " |-- _2: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df.printSchema() #컬럼명 주지 않아서 자동으로 만들어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df=_df.withColumnRenamed('_1', 'label').withColumnRenamed('_2', 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0| (4,[1,3],[1.0,5.5])|\n",
      "|  1.0|(4,[0,2],[-1.0,0.5])|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.5.2 단어 빈도\n",
    "정량 데이터는 합계, 평균, 표준편차 등 의미있는 통계량을 계산하거나, 이런 통계량이 집단 간에 차이가 있는지 분석한다. 반면에 텍스트는 정량데이터와 같이 이러한 통계량의 계산이 가능하지 못하게 된다. 텍스트에 어떤 단어가 얼마나 쓰였는지, 또한 같이 쓰이게 된 단어는 무엇인지 등 단어의 빈도에 따라 정량화하여 과학적인 분석을 하게 된다.\n",
    "\n",
    "#### Bag of Words 모델\n",
    "자연어처리 NLP에서 사용하는 모델로, 텍스트를 단어의 집합, 'bag of words'으로 구성된다고 보며, 단어의 순서는 의미를 가지지 않는다. 이런 영화리뷰가 있다고 하자.\n",
    "\n",
    ">\"...그 영화는 매우 강렬했다. 그냥 좋았다. 영화관에서 보는 동안 긴장을 늦출 수 없었다. 갑돌이가 분장한 악당의 케릭터가 만들어지는 과정은 흥미롭지 않을 수가 없었다. 무비의 이야기 전개는 빠르고, 무엇이 진실이고 거짓인지 판단할 수 없었다. 누가 이런 영화를 좋아 하지 않을 수가 있겠는가 이모티콘...\"\n",
    "\n",
    "이를 단어로 분리하고, 정량화 해보자.\n",
    "\n",
    "#### 텍스트 변환 단계\n",
    "- 단계 1: 단어로 분할 Tokenization\n",
    "  - 그, 영화는, 매우, 강렬했다, 그냥, 좋았다, 영화관에서, 보는, 동안, 긴장을, 늦출, 수, 없었다, 갑돌이가, 분장한, 악당의, 케릭터가, 만들어지는, 과정은, 흥미롭지, 않을, 수가, 없었다, 무비의, 이야기, 전개는, 빠르고, 무엇이, 진실이고, 거짓인지, 판단할, 수, 없었다, 누가, 이런, 영화를, 좋아, 하지, 않을, 수가, 있겠는가, 이모티콘\n",
    "\n",
    "\n",
    "- 단계 2: 정리\n",
    "  - 불필요, 오타, 동음이의, 이음동의 등\n",
    "\n",
    "\n",
    "- 단계 3: 불용어 stopwords 제거\n",
    "  - 그, 수, 수가, 수, 이런, 하지, 수가 등\n",
    "  \n",
    "  \n",
    "- 단계 4: 어간 추출 stemming 영화는, 영화의는 다른 단어지만 조사를 제거하면 동일한 단어\n",
    "  - 좋았다, 좋아 단어들은 어근을 판별하면 동일한 단어이다.\n",
    "  - 영화, 무비의 단어는 이음동의\n",
    "  \n",
    "- 단계 5: 계량화\n",
    "\n",
    "  - word vector로 만든다.\n",
    "  - 있다-없다, 단어빈도, TF-IDF 사용할 수 있다.\n",
    "dense, sparse 모두 가능하다. [1,1,1,1,1,0,0],[0,1,0,1,1,1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python을 사용한 단어 빈도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let it be lyrics\n",
    "doc=[\n",
    "    \"When I find myself in times of trouble\",\n",
    "    \"Mother Mary comes to me\",\n",
    "    \"Speaking words of wisdom, let it be\",\n",
    "    \"And in my hour of darkness\",\n",
    "    \"She is standing right in front of me\",\n",
    "    \"Speaking words of wisdom, let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Whisper words of wisdom, let it be\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d={}\n",
    "for sentence in doc:\n",
    "    words=sentence.split()\n",
    "    for word in words:\n",
    "        if word in d:\n",
    "            d[word]+=1\n",
    "        else:\n",
    "            d[word]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When\t1\n",
      "I\t1\n",
      "find\t1\n",
      "myself\t1\n",
      "in\t3\n",
      "times\t1\n",
      "of\t6\n",
      "trouble\t1\n",
      "Mother\t1\n",
      "Mary\t1\n",
      "comes\t1\n",
      "to\t1\n",
      "me\t2\n",
      "Speaking\t2\n",
      "words\t3\n",
      "wisdom,\t3\n",
      "let\t3\n",
      "it\t7\n",
      "be\t7\n",
      "And\t1\n",
      "my\t1\n",
      "hour\t1\n",
      "darkness\t1\n",
      "She\t1\n",
      "is\t1\n",
      "standing\t1\n",
      "right\t1\n",
      "front\t1\n",
      "Let\t4\n",
      "Whisper\t1\n"
     ]
    }
   ],
   "source": [
    "# for k,v in d.iteritems():  # python2\n",
    "for k,v in d.items():\n",
    "    print (\"{}\\t{}\".format(k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.5.3 Spark의 transformer, estimator\n",
    "RDD를 만들고 나서도 데이터를 변환하기 위해 map-reduce와 같은 함수 또는 transform(), fit()을 사용하는 것과 같이, DataFrame도 역시 Transformer, Estimator를 사용할 수 있다. 이러한 Spark ml 라이브러리는 Python의 scikit-learn에서 영향을 받아 기계학습 API transformer, estimator, evaluator가 유사하다.\n",
    "\n",
    "- Estimator는 DataFrame에 적용되는 알고리즘을 말하는 것으로, Transformer를 생성해낸다. `Estimator.fit()`\n",
    "- Transformer는 DataFrame을 적용해서 다른 DataFrame으로 생성한다. `Transformer.transform()`\n",
    "- Evaluator는 pyspark.ml.evaluation의 'BinaryClassificationEvaluator', 'RegressionEvaluator', 'MulticlassClassificationEvaluator', 'MultilabelClassificationEvaluator', 'ClusteringEvaluator', 'RankingEvaluator' 등이 있다.\n",
    "\n",
    "\n",
    "먼저 텍스트를 2차원 배열로 만들자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2d=[\n",
    "    [\"When I find myself in times of trouble\"],\n",
    "    [\"Mother Mary comes to me\"],\n",
    "    [\"Speaking words of wisdom, let it be\"],\n",
    "    [\"And in my hour of darkness\"],\n",
    "    [\"She is standing right in front of me\"],\n",
    "    [\"Speaking words of wisdom, let it be\"],\n",
    "    [u\"우리 Let it be\"],\n",
    "    [u\"나 Let it be\"],\n",
    "    [u\"너 Let it be\"],\n",
    "    [\"Let it be\"],\n",
    "    [\"Whisper words of wisdom, let it be\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 DataFrame을 생성한다. schema는 만들어 주지 않고 컬럼명을 sent로 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(doc2d, ['sent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "truncate=True는 줄여서, False는 출력을 줄이지 않고 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                sent|\n",
      "+--------------------+\n",
      "|When I find mysel...|\n",
      "|Mother Mary comes...|\n",
      "|Speaking words of...|\n",
      "|And in my hour of...|\n",
      "|She is standing r...|\n",
      "|Speaking words of...|\n",
      "|      우리 Let it be|\n",
      "|        나 Let it be|\n",
      "|        너 Let it be|\n",
      "|           Let it be|\n",
      "|Whisper words of ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|sent                                  |\n",
      "+--------------------------------------+\n",
      "|When I find myself in times of trouble|\n",
      "|Mother Mary comes to me               |\n",
      "|Speaking words of wisdom, let it be   |\n",
      "|And in my hour of darkness            |\n",
      "|She is standing right in front of me  |\n",
      "|Speaking words of wisdom, let it be   |\n",
      "|우리 Let it be                        |\n",
      "|나 Let it be                          |\n",
      "|너 Let it be                          |\n",
      "|Let it be                             |\n",
      "|Whisper words of wisdom, let it be    |\n",
      "+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.5.4 Tokenizer\n",
    "\n",
    "토큰: 최소단위의 상징\n",
    "\n",
    "먼저 용어를 정리해 보자.\n",
    "\n",
    "- corpus는 어떤 주제에 대해 쓰여지거나, 어떤 사람이 작성한 전체 '말뭉치'를 말한다. 여러 문장으로 구성된 텍스트 집합을 말한다.\n",
    "\n",
    "\n",
    "- document는 문장으로 구성된 문서를 말하지만, 한 문장으로만 구성될 수도, 여러 문장으로 만들어질 수도 있다. 예를 들어, \"why she had to go\" 같은 한 문장도 document라고 하고, \"why she had to go.. I don't know\" 역시 마찬가지이다.\n",
    "\n",
    "\n",
    "- vocabularay는 중복없는 단어 집합을 말하며, 예를 들면, \"why\",\"she\",\"had\",\"to\",\"go\",\"where\",\"have\" 등은 단어이다.\n",
    "\n",
    "\n",
    "- Tokenizer는 document를 단어로 분리한다. 분리하는 기준은 whitespace로 공백, TAB, CR, New Line 등이 해당된다.\n",
    "\n",
    "  - 입력 컬럼은 \"sent\"로,\n",
    "  - 출력 컬럼은 \"words\"로 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer #df니까 ml이용\n",
    "tokenizer = Tokenizer(inputCol=\"sent\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transform()은 앞서 만든 tokenizer모델에 DataFrame을 변환하여 다른 DataFrame을 생성한다. 그 결과는 문자열 배열로 구성된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokDf = tokenizer.transform(myDf) #토큰화한 df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                sent|               words|\n",
      "+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|\n",
      "|Mother Mary comes...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokDf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for문으로 출력해보자. Row() 객체로 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(sent='When I find myself in times of trouble', words=['when', 'i', 'find', 'myself', 'in', 'times', 'of', 'trouble'])\n",
      "Row(sent='Mother Mary comes to me', words=['mother', 'mary', 'comes', 'to', 'me'])\n",
      "Row(sent='Speaking words of wisdom, let it be', words=['speaking', 'words', 'of', 'wisdom,', 'let', 'it', 'be'])\n"
     ]
    }
   ],
   "source": [
    "for r in tokDf.select(\"sent\", \"words\").take(3):\n",
    "    print (r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.5.5 RegTokenizer (*reg*)\n",
    "Tokenizer는 white space로 분리하지만, RegexTokenizer는 단어를 분리하기 위해 정규표현식을 적용할 수 있다. 정규표현식을 사용하여 분리하거나 특정 패턴을 추출할 수 있다. 공백으로 분리할 경우 간단히 정규표현식 \\s 패턴을 적용할 수 있다. 한글에는 \\w 패턴이 적용되지 않는다.\n",
    "\n",
    "- \\s는 공백문자\n",
    "- \\w는 숫자 및 대소문자 [A-Za-z0-9_]\n",
    "- 별표 *는 0 또는 그 이상, 더하기 +는 1 또는 그 이상을 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "re = RegexTokenizer(inputCol=\"sent\", outputCol=\"wordsReg\", pattern=\"\\\\s+\") #공백이 1또는 그이상일 때 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                sent|            wordsReg|\n",
      "+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|\n",
      "|Mother Mary comes...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "|And in my hour of...|[and, in, my, hou...|\n",
      "|She is standing r...|[she, is, standin...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "|      우리 Let it be| [우리, let, it, be]|\n",
      "|        나 Let it be|   [나, let, it, be]|\n",
      "|        너 Let it be|   [너, let, it, be]|\n",
      "|           Let it be|       [let, it, be]|\n",
      "|Whisper words of ...|[whisper, words, ...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reDf=re.transform(myDf)\n",
    "reDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.5.6 Stopwords\n",
    "텍스트를 분리하고 나면, 별 의미가 없거나 쓸모가 없는 단어들이 존재한다. 예를 들어 이, 그, 저와 같은 한 단어 또는 있다 등과 같은 일부 동사, 그래서, 그러나 등과 같은 접속사 등이 후보가 될 수 있다. 이런 불필요한 단어들을 불용어 Stopwords라고 하며, 입력데이터에서 제거하도록 한다. 영어의 경우 불용어가 식별되어 제공되고 있다 http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stop = StopWordsRemover(inputCol=\"wordsReg\", outputCol=\"nostops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=list()\n",
    "_stopwords=stop.getStopWords()\n",
    "for e in _stopwords:\n",
    "    stopwords.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopWordsRemover_bc2ec952f014"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_mystopwords=[u\"나\",u\"너\", u\"우리\"] # u: unicode\n",
    "for e in _mystopwords:\n",
    "    stopwords.append(e) #영어불용어+우리나라불용어\n",
    "stop.setStopWords(stopwords) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i/me/my/myself/we/our/ours/ourselves/you/your/yours/yourself/yourselves/he/him/his/himself/she/her/hers/herself/it/its/itself/they/them/their/theirs/themselves/what/which/who/whom/this/that/these/those/am/is/are/was/were/be/been/being/have/has/had/having/do/does/did/doing/a/an/the/and/but/if/or/because/as/until/while/of/at/by/for/with/about/against/between/into/through/during/before/after/above/below/to/from/up/down/in/out/on/off/over/under/again/further/then/once/here/there/when/where/why/how/all/any/both/each/few/more/most/other/some/such/no/nor/not/only/own/same/so/than/too/very/s/t/can/will/just/don/should/now/i'll/you'll/he'll/she'll/we'll/they'll/i'd/you'd/he'd/she'd/we'd/they'd/i'm/you're/he's/she's/it's/we're/they're/i've/we've/you've/they've/isn't/aren't/wasn't/weren't/haven't/hasn't/hadn't/don't/doesn't/didn't/won't/wouldn't/shan't/shouldn't/mustn't/can't/couldn't/cannot/could/here's/how's/let's/ought/that's/there's/what's/when's/where's/who's/why's/would/나/너/우리/"
     ]
    }
   ],
   "source": [
    "for e in stop.getStopWords():\n",
    "    print (e, end=\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|            wordsReg|             nostops|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|[find, times, tro...|\n",
      "|Mother Mary comes...|[mother, mary, co...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|\n",
      "|And in my hour of...|[and, in, my, hou...|    [hour, darkness]|\n",
      "|She is standing r...|[she, is, standin...|[standing, right,...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|\n",
      "|      우리 Let it be| [우리, let, it, be]|               [let]|\n",
      "|        나 Let it be|   [나, let, it, be]|               [let]|\n",
      "|        너 Let it be|   [너, let, it, be]|               [let]|\n",
      "|           Let it be|       [let, it, be]|               [let]|\n",
      "|Whisper words of ...|[whisper, words, ...|[whisper, words, ...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopDf=stop.transform(reDf) #불용어 제거 된 컬럼\n",
    "stopDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.5.7 CountVectorizer\n",
    "CountVectorizer는 단어의 빈도 수를 계산한다.\n",
    "\n",
    "3번째 문장 \"Speaking words of wisdom, let it be\"의 word vector를 구성해 본다. id 값은 모든 문장에서 단어를 추출하고 나서야 부여된다.\n",
    "\n",
    "단어 (3행 \"Speaking words of wisdom, let it be\")|id|빈도\n",
    ":---|---|---\n",
    "Speaking|7|1\n",
    "words|13|1\n",
    "of|stopword|0\n",
    "wisdom|12|1\n",
    "let|3|1\n",
    "it|stopword|0\n",
    "be|stopword|0\n",
    "\n",
    "위 word vector를 표로 나타내면 아래와 같다. 행은 문장, 열은 id이다. 3행은 doc2이다. 해당하는 단어 id의 빈도를 적었다. 다른 행과 열은 이해를 돕기 위해 비워 놓았다.\n",
    "\n",
    "doc \\ 단어 id|1|2|3|4|5|6|7|8|9|10|11|12|13|...\n",
    "---|---|---|---|---|---|---|---|---|---|---|---|---|---|---\n",
    "doc 0||||||||||||||...\n",
    "doc 1||||||||||||||...\n",
    "doc 2|||1||||1|||||1|1|...\n",
    "...||||||||||||||..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn CountVectorizer\n",
    "sklearn 라이브러리로 단어빈도를 세어보자. 입력으로 단어의 집합을 넣어야 하지만, 위 문서는 2차원 리스트이다. 먼저 2차원 리스트를 1차원으로 변경하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "doc = reduce(lambda x,y: x+y, doc2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english') #내장 stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 0)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 13)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 3)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 1)\t1\n",
      "  (4, 8)\t1\n",
      "  (4, 6)\t1\n",
      "  (5, 7)\t1\n",
      "  (5, 13)\t1\n",
      "  (5, 12)\t1\n",
      "  (5, 3)\t1\n",
      "  (6, 3)\t1\n",
      "  (6, 14)\t1\n",
      "  (7, 3)\t1\n",
      "  (8, 3)\t1\n",
      "  (9, 3)\t1\n",
      "  (10, 13)\t1\n",
      "  (10, 12)\t1\n",
      "  (10, 3)\t1\n",
      "  (10, 11)\t1\n"
     ]
    }
   ],
   "source": [
    "print (vectorizer.fit_transform(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'times': 9,\n",
       " 'trouble': 10,\n",
       " 'mother': 5,\n",
       " 'mary': 4,\n",
       " 'comes': 0,\n",
       " 'speaking': 7,\n",
       " 'words': 13,\n",
       " 'wisdom': 12,\n",
       " 'let': 3,\n",
       " 'hour': 2,\n",
       " 'darkness': 1,\n",
       " 'standing': 8,\n",
       " 'right': 6,\n",
       " '우리': 14,\n",
       " 'whisper': 11}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit_transform(doc).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark CountVectorizer\n",
    "- 입력: text, Tockenizer를 사용해서 단어로 분리하고 난 후 사용\n",
    "- 출력: 단어 빈도 word vector(sparse), 즉 단어별 단어 빈도 TF\n",
    "- minDF은 자주 사용된 단어가 아니어서 제거해야할 단어, 즉 문서에 사용된 빈도 document frequency를 제한하는 것. 소수점 사용해야함(비율을 의미하기 때문)\n",
    "  - ex) minDF=1.0 : 문서 1개 이하에서 나타난 단어는 무시\n",
    "- maxDf는 너무 많이 발생하는 경우 무시\n",
    "  - ex) 0.5는 전체 문서의 50%보다 많이 발생하는 경우 무시, 1.0은 100%보다 많이 발생하는 경우 무시 (즉, 어떤 단어도 무시하지 말라는 의미)\n",
    "  \n",
    "  정수는 사용된 문서의 수, 몇 개의 문서에 사용되어야하는지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"nostops\", outputCol=\"cv\", vocabSize=30, minDF=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizerModel은 fit()하고 나면 얻어진다. 다음에 사용하는 HashingTF는 fit()하지 않는다는 점에서 차이가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = cv.fit(stopDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.feature.CountVectorizer'> <class 'pyspark.ml.feature.CountVectorizerModel'>\n"
     ]
    }
   ],
   "source": [
    "print (type(cv),type(cvModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvDf = cvModel.transform(stopDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                sent|            wordsReg|             nostops|                  cv|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|[find, times, tro...|(16,[5,6,8],[1.0,...|\n",
      "|Mother Mary comes...|[mother, mary, co...|[mother, mary, co...|(16,[10,13,14],[1...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|(16,[0,1,2,3],[1....|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvDf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame 전체를 출력하면 보기 불편하므로, 이 가운데 일부 컬럼만을 선택하여 출력할 수 있다. (16,[5,6,8],[1.0,1.0,1.0]) 16은 전체 단어의 개수, 그리고 다음 5,6,8은 값이 있는 컬럼 번호, 1.0,1.0,1.0은 그 값을 말한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|             nostops|                  cv|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[find, times, tro...|(16,[5,6,8],[1.0,...|\n",
      "|Mother Mary comes...|[mother, mary, co...|(16,[10,13,14],[1...|\n",
      "|Speaking words of...|[speaking, words,...|(16,[0,1,2,3],[1....|\n",
      "|And in my hour of...|    [hour, darkness]|(16,[7,9],[1.0,1.0])|\n",
      "|She is standing r...|[standing, right,...|(16,[4,12,15],[1....|\n",
      "|Speaking words of...|[speaking, words,...|(16,[0,1,2,3],[1....|\n",
      "|      우리 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|        나 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|        너 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|           Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|Whisper words of ...|[whisper, words, ...|(16,[0,1,2,11],[1...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvDf.select('sent','nostops','cv').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer에서 사용된 단어 목록을 출력할 수 있다. 아래 단어의 수를 세어보면 위 sparse vector의 컬럼 개수와 동일하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['let',\n",
       " 'wisdom,',\n",
       " 'words',\n",
       " 'speaking',\n",
       " 'right',\n",
       " 'trouble',\n",
       " 'find',\n",
       " 'hour',\n",
       " 'times',\n",
       " 'darkness',\n",
       " 'mother',\n",
       " 'whisper',\n",
       " 'front',\n",
       " 'mary',\n",
       " 'comes',\n",
       " 'standing']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.5.8 TF-IDF\n",
    "TfidfTransformer는 **TF-IDF(Term Frequency-Inverse Document Frequency)** 를 계산한다. 이를 위해서는 우선 Tokenizer를 사용하여 문장을 단어로 분리해 놓아야 한다.\n",
    "\n",
    "HashingTF를 사용하여 'word vector'를 계산한다. HashingTF은 hash함수에 따라 단어의 고유번호를 생성하며, 단어 수가 많아지면서 **고유번호가 충돌할 수 있는 가능성이 있지만 이를 최소화**하게 된다. 그리고 IDF를 계산하고, TF-IDF를 계산한다.\n",
    "\n",
    "#### S.5.8.1 TF-IDF 계산\n",
    "'Let it be'가사 세 번째 줄 'wisdom' 단어의 TF-IDF를 계산해보자  \n",
    "TF는 단어빈도수, 즉 문서에 단어가 나타난 빈도수를 의미한다. 단어빈도는 경우에 따라서는 문제가 될 수 있다. 예를 들어, 'a', 'the', 'of'와 같은 단어는 빈도는 높지만 별로 유용하지 못하다.\n",
    "\n",
    "이 경우 **IDF**는 유용하다. IDF는 자주 나타나는 단어에 대한 가중치를 줄이고, 드물게 나타나는 단어에 가중치를 높이는 방식으로 계산된다.\n",
    "\n",
    "\n",
    "t는 단어, 문서는 d, D는 corpus,\n",
    "\n",
    "항목|설명|예제\n",
    ":---|:---|:---\n",
    "tf(d,f)|단어 t가 문서 d에서 나타나는 단어의 빈도 수, term frequency|$f_{t,d}$ / (number of words in d) = 1/4 = 0.25  (3번째 문서에 stopwords를 제외하면 4개의 단어, wisdom은 1회 나타난다.)\n",
    "df|document frequency 단어가 나타난 문서 수|3 (wisdom이 포함된 문서는 3)\n",
    "N|number of documents 전체 문서의 수|11 (전체의 문서는 11개)\n",
    "idf|inverse document frequency 단어가 나타난 문서의 비율을 거꾸로|ln(N+1 / df+1) + 1 = log(12/4) + 1 = 1.09861 + 1, 0으로 나뉘는 것을 방지하기 위해 smoothing, 즉 1을 더한다.\n",
    "\n",
    "\n",
    "프로그래밍에서는 메모리를 적게 사용하도록 설계되어 있다. 1/4와 같이 정수 타잎으로 연산하면, 정수를 사용하여 연산하고 그 결과도 정수를 출력하게 된다. 이를 변환하기 위해 1.의 경우에서와 같이 소수를 사용하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf: 2.09861228866811\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "tf=1./4\n",
    "df=3.\n",
    "N=11.\n",
    "idf=math.log((N+1)/(df+1))+1\n",
    "print (\"idf: {}\".format(idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S.5.8.2 sklearn을 사용한 TF-IDF\n",
    "우선 'sklearn'의 TF-IDF를 계산해보자. CountVectorizer는 텍스트를 단어의 빈도로 변환해주어, 문서 x 단어 표를 출력할 수 있다. CountVectorizer()의 인자로 analyzer (\"word\", \"character ngram\" 등 선택), tokenizer (단어의 tokenizer를 지정), stop_words (불용어 처리 기준), max_features (최대 속성 개수) 등을 지정할 수 있다. 그 다음으로, TF-IDF를 계산할 수 있다. 이 때 (문서id, 단어id) 별로 결과가 출력된다.\n",
    "\n",
    "TfidfVectorizer를 사용해서 계산하면 그 결과를 아래와 같이 볼 수 있다.\n",
    "\n",
    ">(2,12) 2.09861228867\n",
    "\n",
    "결과에서 '2'는 3번째 문서번호, '12'는 'wisdom' 단어번호 TF-IDF는 2.09861228867이다.\n",
    "\n",
    "\n",
    "\n",
    "max_df, min_df는 기본 값이 1.0으로 굳이 설정하지 않아도 되는데, **어떤 단어도 무시하지 말라는 의미**이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=1.0, stop_words='english',norm = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10)\t2.791759469228055\n",
      "  (0, 9)\t2.791759469228055\n",
      "  (1, 0)\t2.791759469228055\n",
      "  (1, 4)\t2.791759469228055\n",
      "  (1, 5)\t2.791759469228055\n",
      "  (2, 3)\t1.4054651081081644\n",
      "  (2, 12)\t2.09861228866811\n",
      "  (2, 13)\t2.09861228866811\n",
      "  (2, 7)\t2.386294361119891\n",
      "  (3, 1)\t2.791759469228055\n",
      "  (3, 2)\t2.791759469228055\n",
      "  (4, 6)\t2.791759469228055\n",
      "  (4, 8)\t2.791759469228055\n",
      "  (5, 3)\t1.4054651081081644\n",
      "  (5, 12)\t2.09861228866811\n",
      "  (5, 13)\t2.09861228866811\n",
      "  (5, 7)\t2.386294361119891\n",
      "  (6, 14)\t2.791759469228055\n",
      "  (6, 3)\t1.4054651081081644\n",
      "  (7, 3)\t1.4054651081081644\n",
      "  (8, 3)\t1.4054651081081644\n",
      "  (9, 3)\t1.4054651081081644\n",
      "  (10, 11)\t2.791759469228055\n",
      "  (10, 3)\t1.4054651081081644\n",
      "  (10, 12)\t2.09861228866811\n",
      "  (10, 13)\t2.09861228866811\n"
     ]
    }
   ],
   "source": [
    "print (vectorizer.fit_transform(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'times': 9,\n",
       " 'trouble': 10,\n",
       " 'mother': 5,\n",
       " 'mary': 4,\n",
       " 'comes': 0,\n",
       " 'speaking': 7,\n",
       " 'words': 13,\n",
       " 'wisdom': 12,\n",
       " 'let': 3,\n",
       " 'hour': 2,\n",
       " 'darkness': 1,\n",
       " 'standing': 8,\n",
       " 'right': 6,\n",
       " '우리': 14,\n",
       " 'whisper': 11}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.79175947, 2.79175947, 2.79175947, 1.40546511, 2.79175947,\n",
       "       2.79175947, 2.79175947, 2.38629436, 2.79175947, 2.79175947,\n",
       "       2.79175947, 2.79175947, 2.09861229, 2.09861229, 2.79175947])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S.5.8.3 Spark를 사용한 TF-IDF\n",
    "##### TF\n",
    "HashingTF는 단어집합을 워드벡터 word vector로 변환하는데, 해시함수를 사용해서 단어에 해당하는 일련번호를 결정한다. HashingTF에서의 numFeatures는 $2^n$으로 결정하게 된다. 단어 갯수가 900이면, $2^{10}=1024$이므로 1024로 설정하면 된다. 기본은 $2^{18}=262,144$이다. **너무 적게 설정되면 인덱스가 부족하거나 적절하게 매핑될 수 있으니 주의해야 한다.**\n",
    "\n",
    "\n",
    "\n",
    "예를 들어, 문서 [speaking, words, wisdom,, let]의 경우 (32,[4,24,27],[1.0,1.0,2.0])가 출력된다. 아래의 결과와 비교하면 단어 하나가 유실된 것을 알 수 있다. (4개가 나타나야함)  \n",
    "32를 생략하면 기본값 (2^18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "# hashTF = HashingTF(inputCol=\"nostops\", outputCol=\"hash\", numFeatures=32) #  mapping indices insufficient\n",
    "hashTF = HashingTF(inputCol=\"nostops\", outputCol=\"hash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashDf = hashTF.transform(stopDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+--------------------------------------------------------+\n",
      "|nostops                        |hash                                                    |\n",
      "+-------------------------------+--------------------------------------------------------+\n",
      "|[find, times, trouble]         |(262144,[64317,91878,152481],[1.0,1.0,1.0])             |\n",
      "|[mother, mary, comes]          |(262144,[24657,63767,245426],[1.0,1.0,1.0])             |\n",
      "|[speaking, words, wisdom,, let]|(262144,[27556,151864,173339,175131],[1.0,1.0,1.0,1.0]) |\n",
      "|[hour, darkness]               |(262144,[74517,98431],[1.0,1.0])                        |\n",
      "|[standing, right, front]       |(262144,[84798,218360,229166],[1.0,1.0,1.0])            |\n",
      "|[speaking, words, wisdom,, let]|(262144,[27556,151864,173339,175131],[1.0,1.0,1.0,1.0]) |\n",
      "|[let]                          |(262144,[173339],[1.0])                                 |\n",
      "|[let]                          |(262144,[173339],[1.0])                                 |\n",
      "|[let]                          |(262144,[173339],[1.0])                                 |\n",
      "|[let]                          |(262144,[173339],[1.0])                                 |\n",
      "|[whisper, words, wisdom,, let] |(262144,[151864,173339,175131,188139],[1.0,1.0,1.0,1.0])|\n",
      "+-------------------------------+--------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashDf.select(\"nostops\", \"hash\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"hash\", outputCol=\"idf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfModel = idf.fit(hashDf)\n",
    "idfDf = idfModel.transform(hashDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(nostops=['find', 'times', 'trouble'], hash=SparseVector(262144, {64317: 1.0, 91878: 1.0, 152481: 1.0}))\n",
      "Row(nostops=['mother', 'mary', 'comes'], hash=SparseVector(262144, {24657: 1.0, 63767: 1.0, 245426: 1.0}))\n",
      "Row(nostops=['speaking', 'words', 'wisdom,', 'let'], hash=SparseVector(262144, {27556: 1.0, 151864: 1.0, 173339: 1.0, 175131: 1.0}))\n",
      "Row(nostops=['hour', 'darkness'], hash=SparseVector(262144, {74517: 1.0, 98431: 1.0}))\n",
      "Row(nostops=['standing', 'right', 'front'], hash=SparseVector(262144, {84798: 1.0, 218360: 1.0, 229166: 1.0}))\n",
      "Row(nostops=['speaking', 'words', 'wisdom,', 'let'], hash=SparseVector(262144, {27556: 1.0, 151864: 1.0, 173339: 1.0, 175131: 1.0}))\n",
      "Row(nostops=['let'], hash=SparseVector(262144, {173339: 1.0}))\n",
      "Row(nostops=['let'], hash=SparseVector(262144, {173339: 1.0}))\n",
      "Row(nostops=['let'], hash=SparseVector(262144, {173339: 1.0}))\n",
      "Row(nostops=['let'], hash=SparseVector(262144, {173339: 1.0}))\n"
     ]
    }
   ],
   "source": [
    "for e in idfDf.select(\"nostops\",\"hash\").take(10):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.5.9 Word2Vec\n",
    "단어를 벡터로 변환하는 방법을 말한다.\n",
    "\n",
    "**Bag of Words** 모델은 **단어 순서와 문맥을 무시**한다.  \n",
    "**Word2Vec**는 이런 BoW 모델의 단점을 극복하기 위해서, **말뭉치로부터 단어들 서로의 맥락 또는 연관성 Word Embedding을 신경망으로 학습하여 Word2Vec을 계산**한다. 단어가 벡터로 변환되면, 벡터연산이 가능해지고 서로 간의 거리를 측정하여, 가까울수록 비슷한 단어라고 해석하게 된다.  \n",
    "\n",
    "Word2Vec을 계산하고 나면, 벡터('king') - 벡터('man') + 벡터('woman') = 벡터('queen) 이런 연산이 가능해진다. 즉 king 단어벡터에서 man 단어백터를 빼고, woman 단어백터를 더하면, queen 단어백터를 구할 수 있다는 의미이다.\n",
    "\n",
    "vectorSize는 단어벡터를 몇 개로 구성할 것인지, minCount는 최소 단어빈도를 설정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"words\", outputCol=\"w2v\")\n",
    "model = word2Vec.fit(tokDf) #토큰으로 분리시킨 다음에 적용시켜야한다.\n",
    "w2vDf = model.transform(tokDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(w2v=DenseVector([-0.0426, -0.0448, -0.0308]))\n",
      "Row(w2v=DenseVector([0.0002, -0.0528, 0.0787]))\n",
      "Row(w2v=DenseVector([-0.0691, -0.0187, 0.0205]))\n"
     ]
    }
   ],
   "source": [
    "for e in w2vDf.select(\"w2v\").take(3):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------------------------------------------------+\n",
      "|word    |vector                                                            |\n",
      "+--------+------------------------------------------------------------------+\n",
      "|trouble |[-0.10907474905252457,-0.12260671705007553,-0.008148939348757267] |\n",
      "|mother  |[-0.089715875685215,0.04702672362327576,0.026056621223688126]     |\n",
      "|find    |[-0.0019626470748335123,0.027025384828448296,-0.16112865507602692]|\n",
      "|standing|[-0.1180938333272934,0.014508146792650223,0.1571418046951294]     |\n",
      "|wisdom, |[-0.15978337824344635,0.07689274847507477,0.033270757645368576]   |\n",
      "|in      |[-0.07372022420167923,-0.08980047702789307,-0.035899002104997635] |\n",
      "|myself  |[-0.06915149837732315,-0.008410882204771042,-0.15042924880981445] |\n",
      "|is      |[-0.1075735092163086,0.11674908548593521,-0.026824142783880234]   |\n",
      "|darkness|[0.11327086389064789,-0.12288354337215424,0.02928961254656315]    |\n",
      "|우리    |[-0.058154866099357605,-0.02477012760937214,-0.09321960061788559] |\n",
      "|front   |[0.1416950672864914,0.03343282267451286,0.12569814920425415]      |\n",
      "|it      |[-0.10350707918405533,0.1689012199640274,0.058146342635154724]    |\n",
      "|너      |[-0.07187286764383316,-0.15807180106639862,0.013316527009010315]  |\n",
      "|she     |[0.03746093437075615,0.09408345073461533,0.00394409941509366]     |\n",
      "|comes   |[0.16487953066825867,-0.07311463356018066,-0.0116868382319808]    |\n",
      "|i       |[0.05946485698223114,-0.08422790467739105,0.14365510642528534]    |\n",
      "|hour    |[0.09500771015882492,0.13378649950027466,0.0206912774592638]      |\n",
      "|to      |[-0.08667294681072235,-0.11591747403144836,0.12777991592884064]   |\n",
      "|speaking|[0.11991837620735168,-0.08622502535581589,-9.784733410924673E-4]  |\n",
      "|mary    |[-0.04926742613315582,-0.10071603208780289,0.13184872269630432]   |\n",
      "+--------+------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.getVectors().show(truncate=False) #숫자: 신경망을 통해서 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|    word|        similarity|\n",
      "+--------+------------------+\n",
      "|      my|0.9588775038719177|\n",
      "|darkness|0.8516458868980408|\n",
      "|      너|0.7743990421295166|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.findSynonyms(\"times\", 3).show() #문장에서 비어있는 단어 채우는 등의 일을 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.5.10 NGram\n",
    "단어 n 개가 붙어 있을 때 더 의미가 있는 경우가 있다.\n",
    "텍스트를 대상으로 하면, n-gram은 연속된 n개의 토큰으로 구성된 순열을 말한다.  \n",
    "unigram은 한 단어로, bigram은 두 단어로 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramDf = ngram.transform(tokDf) #토큰으로 분리시킨 다음에 적용시켜야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------------+\n",
      "|                sent|               words|                ngrams|\n",
      "+--------------------+--------------------+----------------------+\n",
      "|When I find mysel...|[when, i, find, m...|  [when i, i find, ...|\n",
      "|Mother Mary comes...|[mother, mary, co...|  [mother mary, mar...|\n",
      "|Speaking words of...|[speaking, words,...|  [speaking words, ...|\n",
      "|And in my hour of...|[and, in, my, hou...|  [and in, in my, m...|\n",
      "|She is standing r...|[she, is, standin...|  [she is, is stand...|\n",
      "|Speaking words of...|[speaking, words,...|  [speaking words, ...|\n",
      "|      우리 Let it be| [우리, let, it, be]|[우리 let, let it, ...|\n",
      "|        나 Let it be|   [나, let, it, be]| [나 let, let it, i...|\n",
      "|        너 Let it be|   [너, let, it, be]| [너 let, let it, i...|\n",
      "|           Let it be|       [let, it, be]|       [let it, it be]|\n",
      "|Whisper words of ...|[whisper, words, ...|  [whisper words, w...|\n",
      "+--------------------+--------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngramDf.show() #i find 와 같이 붙을 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(words=['when', 'i', 'find', 'myself', 'in', 'times', 'of', 'trouble'], ngrams=['when i', 'i find', 'find myself', 'myself in', 'in times', 'times of', 'of trouble'])\n",
      "Row(words=['mother', 'mary', 'comes', 'to', 'me'], ngrams=['mother mary', 'mary comes', 'comes to', 'to me'])\n",
      "Row(words=['speaking', 'words', 'of', 'wisdom,', 'let', 'it', 'be'], ngrams=['speaking words', 'words of', 'of wisdom,', 'wisdom, let', 'let it', 'it be'])\n"
     ]
    }
   ],
   "source": [
    "for e in ngramDf.select(\"words\",\"ngrams\").take(3):\n",
    "    print (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.5.11 StringIndexer\n",
    "문자열 컬럼을 인덱스 컬럼으로 변환한다. 빈도가 제일 높은 순서로 0.0부터 인덱스 값이 주어진다. 인덱스는 double 형을 가지게 된다. 없는 레이블에 대해서는 예외가 발생할 수 있으므로 (default), setHandleInvalid(\"skip\") 함수로 'skip', 'keep', 'error' 등으로 설정할 수 있다.\n",
    "\n",
    "label, 명목변수: 단어 그 자체로 의미가 있다.\n",
    "\n",
    "\n",
    "구분|설명|예\n",
    ":---:|:---:|:---:\n",
    "nominal|명목 또는 구분 값 cateogry|사자, 호랑이, 사람\n",
    "ordinal|명목값과 다른 점은 순서가 있다.|키 low, med, high\n",
    "interval|일정한 간격이 있다.|150-165, 165-180, 180-195\n",
    "\n",
    "\n",
    "현재 텍스트에 대해서는 적당한 명목변수가 없으므로, 문장 전체에 대해 인덱스로 변환해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"sent\", outputCol=\"sentLabel\") #문장 전체를 넣었음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=labelIndexer.fit(myDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "siDf=model.transform(myDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|                sent|sentLabel|\n",
      "+--------------------+---------+\n",
      "|When I find mysel...|      5.0|\n",
      "|Mother Mary comes...|      3.0|\n",
      "|Speaking words of...|      0.0|\n",
      "|And in my hour of...|      1.0|\n",
      "|She is standing r...|      4.0|\n",
      "|Speaking words of...|      0.0|\n",
      "|      우리 Let it be|      9.0|\n",
      "|        나 Let it be|      7.0|\n",
      "|        너 Let it be|      8.0|\n",
      "|           Let it be|      2.0|\n",
      "|Whisper words of ...|      6.0|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "siDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.5.12 연속데이터의 변환\n",
    "몸무게(inches), 키(pounds) 데이터를 분석해보자. 이 데이터는 정량, 연속 데이터이다. 출처는 https://people.sc.fsu.edu/~jburkardt/data/csv/hw_200.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "rdd=spark.sparkContext\\\n",
    "    .textFile(os.path.join('data','ds_spark_heightweight.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd=rdd.map(lambda line:[float(x) for x in line.split('\\t')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(myRdd,[\"id\",\"weight\",\"height\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Binarizer #이분화\n",
    "\n",
    "binarizer = Binarizer(threshold=68.0, inputCol=\"weight\", outputCol=\"weight2\") #68기준 이분화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "binDf = binarizer.transform(myDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+\n",
      "|  id|weight|height|weight2|\n",
      "+----+------+------+-------+\n",
      "| 1.0| 65.78|112.99|    0.0|\n",
      "| 2.0| 71.52|136.49|    1.0|\n",
      "| 3.0|  69.4|153.03|    1.0|\n",
      "| 4.0| 68.22|142.34|    1.0|\n",
      "| 5.0| 67.79| 144.3|    0.0|\n",
      "| 6.0|  68.7| 123.3|    1.0|\n",
      "| 7.0|  69.8|141.49|    1.0|\n",
      "| 8.0| 70.01|136.46|    1.0|\n",
      "| 9.0|  67.9|112.37|    0.0|\n",
      "|10.0| 66.78|120.67|    0.0|\n",
      "+----+------+------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binDf.show(10) #68보다작으면 0, 크면 1이 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer#삼분화\n",
    "\n",
    "discretizer = QuantileDiscretizer(numBuckets=3, inputCol=\"height\", outputCol=\"height3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdDf = discretizer.fit(binDf).transform(binDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+-------+\n",
      "|  id|weight|height|weight2|height3|\n",
      "+----+------+------+-------+-------+\n",
      "| 1.0| 65.78|112.99|    0.0|    0.0|\n",
      "| 2.0| 71.52|136.49|    1.0|    1.0|\n",
      "| 3.0|  69.4|153.03|    1.0|    2.0|\n",
      "| 4.0| 68.22|142.34|    1.0|    2.0|\n",
      "| 5.0| 67.79| 144.3|    0.0|    2.0|\n",
      "| 6.0|  68.7| 123.3|    1.0|    0.0|\n",
      "| 7.0|  69.8|141.49|    1.0|    2.0|\n",
      "| 8.0| 70.01|136.46|    1.0|    1.0|\n",
      "| 9.0|  67.9|112.37|    0.0|    0.0|\n",
      "|10.0| 66.78|120.67|    0.0|    0.0|\n",
      "+----+------+------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qdDf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.5.13 VectorAssembler\n",
    "열을 묶어서 Vector열로 만든다. features 컬럼을 생성할 경우에 사용한다. 단 문자열 string은 묶을 수 없다.\n",
    "기계학습- lable, features 컬럼 있음, 이 때 속성을 여러 컬럼에서 가지고 있을 수 있는데 이 경우 여러 컬럼을 합쳐서 하나의 vector 컬럼으로 만들어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "va = VectorAssembler(inputCols=[\"weight2\",\"height3\"],outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaDf = va.transform(qdDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- weight2: double (nullable = true)\n",
      " |-- height3: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vaDf.printSchema()  #벡터 형태 features가 만들어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-------+-------+---------+\n",
      "| id|weight|height|weight2|height3| features|\n",
      "+---+------+------+-------+-------+---------+\n",
      "|1.0| 65.78|112.99|    0.0|    0.0|(2,[],[])|\n",
      "|2.0| 71.52|136.49|    1.0|    1.0|[1.0,1.0]|\n",
      "|3.0|  69.4|153.03|    1.0|    2.0|[1.0,2.0]|\n",
      "|4.0| 68.22|142.34|    1.0|    2.0|[1.0,2.0]|\n",
      "|5.0| 67.79| 144.3|    0.0|    2.0|[0.0,2.0]|\n",
      "+---+------+------+-------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vaDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.5.14 Pipeline¶\n",
    "Pipeline은 여러 Estimator를 묶은 Estimator를 반환한다. Pipeline은 여러 작업을 묶어, 순서대로 단계적으로 Estimator를 적용하기 위해 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "        (0, \"a b c d e spark\", 1.0),\n",
    "        (1, \"b d\", 0.0),\n",
    "        (2, \"spark f g h\", 1.0),\n",
    "        (3, \"hadoop mapreduce\", 0.0),\n",
    "        (4, \"my dog has flea problems. help please.\",0.0)\n",
    "    ], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  1.0|(262144,[74920,89...|\n",
      "|  0.0|(262144,[89530,14...|\n",
      "|  1.0|(262144,[36803,17...|\n",
      "|  0.0|(262144,[132966,1...|\n",
      "|  0.0|(262144,[1074,389...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.select('label', 'features').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
