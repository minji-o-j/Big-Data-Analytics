{"cells":[{"cell_type":"markdown","source":["# 큰 글씨\n## 마크다운 입력시 맨 위에 `%md` 라고 쓴다.\n- 여기는 또 하나의 컴퓨터!\n\n---\n## 2"],"metadata":{}},{"cell_type":"code","source":["%fs \nls /"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/</td><td>databricks-datasets/</td><td>0</td></tr><tr><td>dbfs:/databricks-results/</td><td>databricks-results/</td><td>0</td></tr><tr><td>dbfs:/tmp/</td><td>tmp/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":2},{"cell_type":"code","source":["%fs\nls /databricks-datasets/"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/</td><td>databricks-datasets/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/COVID/</td><td>COVID/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/README.md</td><td>README.md</td><td>976</td></tr><tr><td>dbfs:/databricks-datasets/Rdatasets/</td><td>Rdatasets/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/SPARK_README.md</td><td>SPARK_README.md</td><td>3359</td></tr><tr><td>dbfs:/databricks-datasets/adult/</td><td>adult/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/airlines/</td><td>airlines/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/amazon/</td><td>amazon/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/asa/</td><td>asa/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/atlas_higgs/</td><td>atlas_higgs/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/bikeSharing/</td><td>bikeSharing/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cctvVideos/</td><td>cctvVideos/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/credit-card-fraud/</td><td>credit-card-fraud/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs100/</td><td>cs100/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs110x/</td><td>cs110x/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs190/</td><td>cs190/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/data.gov/</td><td>data.gov/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/definitive-guide/</td><td>definitive-guide/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flights/</td><td>flights/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flower_photos/</td><td>flower_photos/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flowers/</td><td>flowers/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/genomics/</td><td>genomics/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/hail/</td><td>hail/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/iot/</td><td>iot/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/iot-stream/</td><td>iot-stream/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark/</td><td>learning-spark/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark-v2/</td><td>learning-spark-v2/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/lending-club-loan-stats/</td><td>lending-club-loan-stats/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/med-images/</td><td>med-images/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/mnist-digits/</td><td>mnist-digits/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/news20.binary/</td><td>news20.binary/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/nyctaxi/</td><td>nyctaxi/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/online_retail/</td><td>online_retail/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/overlap-join/</td><td>overlap-join/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/power-plant/</td><td>power-plant/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/rwe/</td><td>rwe/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sai-summit-2019-sf/</td><td>sai-summit-2019-sf/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sample_logs/</td><td>sample_logs/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/samples/</td><td>samples/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sfo_customer_survey/</td><td>sfo_customer_survey/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sms_spam_collection/</td><td>sms_spam_collection/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/songs/</td><td>songs/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/</td><td>structured-streaming/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/timeseries/</td><td>timeseries/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/tpch/</td><td>tpch/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/weather/</td><td>weather/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wiki/</td><td>wiki/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wikipedia-datasets/</td><td>wikipedia-datasets/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wine-quality/</td><td>wine-quality/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":3},{"cell_type":"code","source":["dbutils.fs.ls('dbfs:/databricks-datasets/')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[6]: [FileInfo(path=&#39;dbfs:/databricks-datasets/&#39;, name=&#39;databricks-datasets/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/COVID/&#39;, name=&#39;COVID/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/README.md&#39;, name=&#39;README.md&#39;, size=976),\n FileInfo(path=&#39;dbfs:/databricks-datasets/Rdatasets/&#39;, name=&#39;Rdatasets/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/SPARK_README.md&#39;, name=&#39;SPARK_README.md&#39;, size=3359),\n FileInfo(path=&#39;dbfs:/databricks-datasets/adult/&#39;, name=&#39;adult/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/airlines/&#39;, name=&#39;airlines/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/amazon/&#39;, name=&#39;amazon/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/asa/&#39;, name=&#39;asa/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/atlas_higgs/&#39;, name=&#39;atlas_higgs/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/bikeSharing/&#39;, name=&#39;bikeSharing/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/cctvVideos/&#39;, name=&#39;cctvVideos/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/credit-card-fraud/&#39;, name=&#39;credit-card-fraud/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/cs100/&#39;, name=&#39;cs100/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/cs110x/&#39;, name=&#39;cs110x/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/cs190/&#39;, name=&#39;cs190/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/data.gov/&#39;, name=&#39;data.gov/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/definitive-guide/&#39;, name=&#39;definitive-guide/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/flights/&#39;, name=&#39;flights/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/flower_photos/&#39;, name=&#39;flower_photos/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/flowers/&#39;, name=&#39;flowers/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/genomics/&#39;, name=&#39;genomics/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/hail/&#39;, name=&#39;hail/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/iot/&#39;, name=&#39;iot/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/iot-stream/&#39;, name=&#39;iot-stream/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/learning-spark/&#39;, name=&#39;learning-spark/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/learning-spark-v2/&#39;, name=&#39;learning-spark-v2/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/lending-club-loan-stats/&#39;, name=&#39;lending-club-loan-stats/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/med-images/&#39;, name=&#39;med-images/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/mnist-digits/&#39;, name=&#39;mnist-digits/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/news20.binary/&#39;, name=&#39;news20.binary/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/nyctaxi/&#39;, name=&#39;nyctaxi/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/online_retail/&#39;, name=&#39;online_retail/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/overlap-join/&#39;, name=&#39;overlap-join/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/power-plant/&#39;, name=&#39;power-plant/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/rwe/&#39;, name=&#39;rwe/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/sai-summit-2019-sf/&#39;, name=&#39;sai-summit-2019-sf/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/sample_logs/&#39;, name=&#39;sample_logs/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/samples/&#39;, name=&#39;samples/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/sfo_customer_survey/&#39;, name=&#39;sfo_customer_survey/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/sms_spam_collection/&#39;, name=&#39;sms_spam_collection/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/songs/&#39;, name=&#39;songs/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/structured-streaming/&#39;, name=&#39;structured-streaming/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/timeseries/&#39;, name=&#39;timeseries/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/tpch/&#39;, name=&#39;tpch/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/weather/&#39;, name=&#39;weather/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/wiki/&#39;, name=&#39;wiki/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/wikipedia-datasets/&#39;, name=&#39;wikipedia-datasets/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/wine-quality/&#39;, name=&#39;wine-quality/&#39;, size=0)]</div>"]}}],"execution_count":4},{"cell_type":"code","source":["dbutils.fs.ls(\"/databricks-datasets/\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[7]: [FileInfo(path=&#39;dbfs:/databricks-datasets/&#39;, name=&#39;databricks-datasets/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/COVID/&#39;, name=&#39;COVID/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/README.md&#39;, name=&#39;README.md&#39;, size=976),\n FileInfo(path=&#39;dbfs:/databricks-datasets/Rdatasets/&#39;, name=&#39;Rdatasets/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/SPARK_README.md&#39;, name=&#39;SPARK_README.md&#39;, size=3359),\n FileInfo(path=&#39;dbfs:/databricks-datasets/adult/&#39;, name=&#39;adult/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/airlines/&#39;, name=&#39;airlines/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/amazon/&#39;, name=&#39;amazon/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/asa/&#39;, name=&#39;asa/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/atlas_higgs/&#39;, name=&#39;atlas_higgs/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/bikeSharing/&#39;, name=&#39;bikeSharing/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/cctvVideos/&#39;, name=&#39;cctvVideos/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/credit-card-fraud/&#39;, name=&#39;credit-card-fraud/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/cs100/&#39;, name=&#39;cs100/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/cs110x/&#39;, name=&#39;cs110x/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/cs190/&#39;, name=&#39;cs190/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/data.gov/&#39;, name=&#39;data.gov/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/definitive-guide/&#39;, name=&#39;definitive-guide/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/flights/&#39;, name=&#39;flights/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/flower_photos/&#39;, name=&#39;flower_photos/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/flowers/&#39;, name=&#39;flowers/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/genomics/&#39;, name=&#39;genomics/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/hail/&#39;, name=&#39;hail/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/iot/&#39;, name=&#39;iot/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/iot-stream/&#39;, name=&#39;iot-stream/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/learning-spark/&#39;, name=&#39;learning-spark/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/learning-spark-v2/&#39;, name=&#39;learning-spark-v2/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/lending-club-loan-stats/&#39;, name=&#39;lending-club-loan-stats/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/med-images/&#39;, name=&#39;med-images/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/mnist-digits/&#39;, name=&#39;mnist-digits/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/news20.binary/&#39;, name=&#39;news20.binary/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/nyctaxi/&#39;, name=&#39;nyctaxi/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/online_retail/&#39;, name=&#39;online_retail/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/overlap-join/&#39;, name=&#39;overlap-join/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/power-plant/&#39;, name=&#39;power-plant/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/rwe/&#39;, name=&#39;rwe/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/sai-summit-2019-sf/&#39;, name=&#39;sai-summit-2019-sf/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/sample_logs/&#39;, name=&#39;sample_logs/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/samples/&#39;, name=&#39;samples/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/sfo_customer_survey/&#39;, name=&#39;sfo_customer_survey/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/sms_spam_collection/&#39;, name=&#39;sms_spam_collection/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/songs/&#39;, name=&#39;songs/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/structured-streaming/&#39;, name=&#39;structured-streaming/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/timeseries/&#39;, name=&#39;timeseries/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/tpch/&#39;, name=&#39;tpch/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/weather/&#39;, name=&#39;weather/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/wiki/&#39;, name=&#39;wiki/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/wikipedia-datasets/&#39;, name=&#39;wikipedia-datasets/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-datasets/wine-quality/&#39;, name=&#39;wine-quality/&#39;, size=0)]</div>"]}}],"execution_count":5},{"cell_type":"code","source":["dbutils.fs.ls(\"/\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[8]: [FileInfo(path=&#39;dbfs:/databricks-datasets/&#39;, name=&#39;databricks-datasets/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/databricks-results/&#39;, name=&#39;databricks-results/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/tmp/&#39;, name=&#39;tmp/&#39;, size=0)]</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["리눅스 기반으로 작동하기 때문에 맨 앞에 `/`를 써준다!  \ndbutils.fs.ls(\"`/`databricks-datasets/\")  \n그리고 home이 있다"],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.ls(\"file:/home\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[9]: [FileInfo(path=&#39;file:/home/ubuntu/&#39;, name=&#39;ubuntu/&#39;, size=4096)]</div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["---"],"metadata":{}},{"cell_type":"code","source":["%python\nimport pkg_resources\npkg_resources.get_distribution('scipy').version"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[10]: &#39;1.4.1&#39;</div>"]}}],"execution_count":10},{"cell_type":"code","source":["dbutils.library.list()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[11]: []</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["기본 버전은 1.4.1이지만 따로 설치된 버전은 없다는 뜻임  \n1.5.0을 설치해보자!"],"metadata":{}},{"cell_type":"code","source":["dbutils.library.installPyPI('scipy','1.5.0')\ndbutils.library.restartPython()\ndbutils.library.list()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[12]: [&#39;scipy==1.5.0&#39;]</div>"]}}],"execution_count":13},{"cell_type":"code","source":["%python\n# 파이썬에서 import해서 확인해보자!\nimport scipy\nimport pkg_resources\npkg_resources.get_distribution('scipy').version"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[1]: &#39;1.5.0&#39;</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["---\n## 3.2\n파일을 쓰고 저장해보자"],"metadata":{}},{"cell_type":"code","source":["jstxt=\"\"\"Wikipedia\nApache Spark is an open source cluster computing framework.\n아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\nApache Spark Apache Spark Apache Spark Apache Spark\n아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\nOriginally developed at the University of California, Berkeley's AMPLab,\nthe Spark codebase was later donated to the Apache Software Foundation,\nwhich has maintained it since.\nSpark provides an interface for programming entire clusters with\nimplicit data parallelism and fault-tolerance.\"\"\"\ndbutils.fs.put(\"/data/ds_spark_wiki.txt\", jstxt)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Wrote 572 bytes.\nOut[12]: True</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["파일이 써졌을것이다!  \ndata-add data- DBFS의 data 폴더에서 확인가능! `db_spark_wiki.txt`  \n또는 아래와 같이 명령어 방식으로 확인이 가능하다"],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.ls(\"/data/\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[13]: [FileInfo(path=&#39;dbfs:/data/ds_spark_wiki.txt&#39;, name=&#39;ds_spark_wiki.txt&#39;, size=572)]</div>"]}}],"execution_count":18},{"cell_type":"code","source":["#파일을 지우려면\n# dbutils.fs.rm(\"/data/ds_spark_wiki.txt\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[11]: True</div>"]}}],"execution_count":19},{"cell_type":"code","source":["dbutils.fs.ls(\"/FileStore/tables/\")\n# 아직 안올려서그렇다! 마우스로 올리면 FileStore/tables아래 저장됨"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[14]: []</div>"]}}],"execution_count":20},{"cell_type":"code","source":["dbutils.fs.ls(\"/FileStore/tables/\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[15]: [FileInfo(path=&#39;dbfs:/FileStore/tables/test.txt&#39;, name=&#39;test.txt&#39;, size=23)]</div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["스파크 테스트~~ㄱ"],"metadata":{}},{"cell_type":"code","source":["df=spark.read.csv('/FileStore/tables/test.txt')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"code","source":["df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+\n       _c0|\n+----------+\n      test|\nmy name is|\n     minji|\n+----------+\n\n</div>"]}}],"execution_count":24},{"cell_type":"markdown","source":["---\n# ds3 spark session"],"metadata":{}},{"cell_type":"code","source":["spark"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://10.172.246.180:45822\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]}}],"execution_count":26},{"cell_type":"markdown","source":["local[8]은 cpu를 8개쓴다는 말이라고 한다!"],"metadata":{}},{"cell_type":"code","source":["# 노트북에서 파이스파크를 설치해서 사용하는 경우에는~\n\nimport pyspark\nmyConf=pyspark.SparkConf() #스파크 설정 관련 정보\nspark = pyspark.sql.SparkSession\\\n    .builder\\\n    .master(\"local\")\\\n    .appName(\"myApp\")\\ #이 프로그램 이름\n    .config(conf=myConf)\\ #그 설정 관련 정보를 여기에 넣는다\n    .getOrCreate()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-cyan-fg\">  File </span><span class=\"ansi-green-fg\">&#34;&lt;command-2165011122111359&gt;&#34;</span><span class=\"ansi-cyan-fg\">, line </span><span class=\"ansi-green-fg\">8</span>\n<span class=\"ansi-red-fg\">    .appName(&#34;myApp&#34;)\\ #이 프로그램 이름</span>\n                                 ^\n<span class=\"ansi-red-fg\">SyntaxError</span><span class=\"ansi-red-fg\">:</span> unexpected character after line continuation character\n</div>"]}}],"execution_count":28},{"cell_type":"code","source":["# \\t는 tab키를 말한다.\n# {}는 뒤의 변수를 {}에 넣어서 출력하겠다는 것을 나타낸다\n\nprint (\"Spark version \\t: {}\".format(spark.version))\nprint (\"Spark App \\t: {}\".format(spark.conf.get('spark.app.name')))\nprint (\"Spark Master \\t: {}\".format(spark.conf.get('spark.master')))\nprint (\"Spark Host \\t: {}\".format(spark.conf.get('spark.driver.host')))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Spark version \t: 3.0.0\nSpark App \t: myApp\nSpark Master \t: local\nSpark Host \t: 10.172.246.180\n</div>"]}}],"execution_count":29},{"cell_type":"code","source":["sc"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://10.172.246.180:45822\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        "]}}],"execution_count":30},{"cell_type":"code","source":["sqlContext"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[27]: &lt;pyspark.sql.context.SQLContext at 0x7fec7b8f8650&gt;</div>"]}}],"execution_count":31},{"cell_type":"markdown","source":["`spark`, `sc`, `sqlContext`는 pyspark를 노트북에서 사용하면 만들어주어야한다  \n여기선 자동으로 만들어서 제공!(편리)"],"metadata":{}},{"cell_type":"code","source":["print (\"Spark version \\t: {}\".format(spark.version))\nprint (\"Spark App \\t: {}\".format(spark.conf.get('spark.app.name')))\nprint (\"Spark Master \\t: {}\".format(spark.conf.get('spark.master')))\nprint (\"Spark Host \\t: {}\".format(spark.conf.get('spark.driver.host')))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Spark version \t: 3.0.0\nSpark App \t: myApp\nSpark Master \t: local\nSpark Host \t: 10.172.246.180\n</div>"]}}],"execution_count":33},{"cell_type":"markdown","source":["강의 영상이랑 좀 다르다ㅠ  \n\n강의자료: myspark를 .builder이용해서 만듦 근데 여기서는 spark에 그냥 덧씌웠?음  \n그래서 cmd33(바로 위의 셀)을 실행해도 그 위에서 만든 spakr가 나와서 myApp이 나오는 것 같다."],"metadata":{}},{"cell_type":"code","source":["spark.conf.get('spark.sql.warehouse.dir')\n#보통은 노트북에서 spark를 실행하는 디렉토리에 warehouse 생성됨, 자동생성"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[37]: &#39;/user/hive/warehouse&#39;</div>"]}}],"execution_count":35},{"cell_type":"code","source":["# 모든 생성정보 출력\nspark.sparkContext.getConf().getAll()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[38]: [(&#39;spark.files.useFetchCache&#39;, &#39;false&#39;),\n (&#39;spark.databricks.preemption.enabled&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.abfs.impl&#39;,\n  &#39;shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem&#39;),\n (&#39;spark.driver.tempDirectory&#39;, &#39;/local_disk0/tmp&#39;),\n (&#39;spark.hadoop.fs.adl.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.parquet.block.size.row.check.max&#39;, &#39;10&#39;),\n (&#39;spark.hadoop.fs.s3a.connection.maximum&#39;, &#39;200&#39;),\n (&#39;spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2&#39;, &#39;0&#39;),\n (&#39;spark.shuffle.reduceLocality.enabled&#39;, &#39;false&#39;),\n (&#39;spark.sql.streaming.checkpointFileManagerClass&#39;,\n  &#39;com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager&#39;),\n (&#39;spark.databricks.service.dbutils.repl.backend&#39;,\n  &#39;com.databricks.dbconnect.ReplDBUtils&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverNodeType&#39;, &#39;dev-tier-node&#39;),\n (&#39;spark.hadoop.spark.sql.sources.outputCommitterClass&#39;,\n  &#39;com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter&#39;),\n (&#39;spark.databricks.clusterUsageTags.instanceBootstrapType&#39;, &#39;ssh&#39;),\n (&#39;spark.streaming.driver.writeAheadLog.allowBatching&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterSource&#39;, &#39;UI&#39;),\n (&#39;spark.hadoop.hive.server2.transport.mode&#39;, &#39;http&#39;),\n (&#39;spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled&#39;, &#39;false&#39;),\n (&#39;spark.executor.memory&#39;, &#39;8278m&#39;),\n (&#39;spark.databricks.driverNodeTypeId&#39;, &#39;dev-tier-node&#39;),\n (&#39;spark.sql.parquet.compression.codec&#39;, &#39;snappy&#39;),\n (&#39;spark.databricks.clusterUsageTags.hailEnabled&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.containerType&#39;, &#39;LXC&#39;),\n (&#39;spark.eventLog.enabled&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.wasb.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem&#39;),\n (&#39;spark.executor.tempDirectory&#39;, &#39;/local_disk0/tmp&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterName&#39;, &#39;MJ&#39;),\n (&#39;spark.databricks.workerNodeTypeId&#39;, &#39;dev-tier-node&#39;),\n (&#39;spark.hadoop.mapred.output.committer.class&#39;,\n  &#39;com.databricks.backend.daemon.data.client.DirectOutputCommitter&#39;),\n (&#39;spark.hadoop.hive.server2.thrift.http.port&#39;, &#39;10000&#39;),\n (&#39;spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version&#39;, &#39;2&#39;),\n (&#39;spark.sql.allowMultipleContexts&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterEbsVolumeSize&#39;, &#39;0&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterAllTags&#39;,\n  &#39;[{&#34;key&#34;:&#34;Name&#34;,&#34;value&#34;:&#34;ce3-worker&#34;}]&#39;),\n (&#39;spark.home&#39;, &#39;/databricks/spark&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterTargetWorkers&#39;, &#39;0&#39;),\n (&#39;spark.sql.warehouse.dir&#39;, &#39;/user/hive/warehouse&#39;),\n (&#39;spark.hadoop.hive.server2.idle.operation.timeout&#39;, &#39;7200000&#39;),\n (&#39;spark.task.reaper.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.passthrough.s3a.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider&#39;),\n (&#39;spark.storage.memoryFraction&#39;, &#39;0.5&#39;),\n (&#39;spark.databricks.session.share&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterResourceClass&#39;, &#39;default&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterFirstOnDemand&#39;, &#39;0&#39;),\n (&#39;spark.driver.maxResultSize&#39;, &#39;4g&#39;),\n (&#39;spark.databricks.delta.multiClusterWrites.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterSku&#39;, &#39;STANDARD_SKU&#39;),\n (&#39;spark.worker.cleanup.enabled&#39;, &#39;false&#39;),\n (&#39;spark.databricks.workspace.matplotlibInline.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableCredentialPassthrough&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType&#39;,\n  &#39;ebs_volume_type: GENERAL_PURPOSE_SSD\\n&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableJdbcAutoStart&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.wasb.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterEbsVolumeType&#39;,\n  &#39;GENERAL_PURPOSE_SSD&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterLogDestination&#39;, &#39;&#39;),\n (&#39;spark.cleaner.referenceTracking.blocking&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.parquet.page.size.check.estimate&#39;, &#39;false&#39;),\n (&#39;spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class&#39;,\n  &#39;com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory&#39;),\n (&#39;spark.databricks.delta.preview.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.isSingleUserCluster&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterState&#39;, &#39;Pending&#39;),\n (&#39;spark.databricks.tahoe.logStore.azure.class&#39;,\n  &#39;com.databricks.tahoe.store.AzureLogStore&#39;),\n (&#39;spark.databricks.clusterUsageTags.sparkVersion&#39;, &#39;7.0.x-scala2.12&#39;),\n (&#39;spark.executor.extraClassPath&#39;,\n  &#39;/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/api-base--api-base_java-spark_3.0_2.12_deploy.jar:/databricks/jars/api-base--api-base-spark_3.0_2.12_deploy.jar:/databricks/jars/api--rpc--rpc_parser-spark_3.0_2.12_deploy.jar:/databricks/jars/chauffeur-api--api--endpoints--endpoints-spark_3.0_2.12_deploy.jar:/databricks/jars/chauffeur-api--chauffeur-api-spark_3.0_2.12_deploy.jar:/databricks/jars/common--client--client-spark_3.0_2.12_deploy.jar:/databricks/jars/common--common-spark_3.0_2.12_deploy.jar:/databricks/jars/common--credentials--credentials-spark_3.0_2.12_deploy.jar:/databricks/jars/common--hadoop--hadoop-spark_3.0_2.12_deploy.jar:/databricks/jars/common--jetty--client--client-spark_3.0_2.12_deploy.jar:/databricks/jars/common--lazy--lazy-spark_3.0_2.12_deploy.jar:/databricks/jars/common--libcommon_resources.jar:/databricks/jars/common--path--path-spark_3.0_2.12_deploy.jar:/databricks/jars/common--rate-limiter--rate-limiter-spark_3.0_2.12_deploy.jar:/databricks/jars/common--storage--storage-spark_3.0_2.12_deploy.jar:/databricks/jars/common--tracing--tracing-spark_3.0_2.12_deploy.jar:/databricks/jars/daemon--data--client--client-spark_3.0_2.12_deploy.jar:/databricks/jars/daemon--data--client--conf--conf-spark_3.0_2.12_deploy.jar:/databricks/jars/daemon--data--client--utils-spark_3.0_2.12_deploy.jar:/databricks/jars/daemon--data--data-common--data-common-spark_3.0_2.12_deploy.jar:/databricks/jars/dbfs--utils--dbfs-utils-spark_3.0_2.12_deploy.jar:/databricks/jars/extern--acl--auth--auth-spark_3.0_2.12_deploy.jar:/databricks/jars/extern--extern-spark_3.0_2.12_deploy.jar:/databricks/jars/extern--libaws-regions.jar:/databricks/jars/----jackson_annotations_shaded--libjackson-annotations.jar:/databricks/jars/----jackson_core_shaded--libjackson-core.jar:/databricks/jars/----jackson_databind_shaded--libjackson-databind.jar:/databricks/jars/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:/databricks/jars/jsonutil--jsonutil-spark_3.0_2.12_deploy.jar:/databricks/jars/libraries--api--managedLibraries--managedLibraries-spark_3.0_2.12_deploy.jar:/databricks/jars/libraries--api--typemappers--typemappers-spark_3.0_2.12_deploy.jar:/databricks/jars/libraries--libraries-spark_3.0_2.12_deploy.jar:/databricks/jars/logging--log4j-mod--log4j-mod-spark_3.0_2.12_deploy.jar:/databricks/jars/logging--utils--logging-utils-spark_3.0_2.12_deploy.jar:/databricks/jars/s3commit--client--client-spark_3.0_2.12_deploy.jar:/databricks/jars/s3commit--common--common-spark_3.0_2.12_deploy.jar:/databricks/jars/s3--s3-spark_3.0_2.12_deploy.jar:/databricks/jars/----scalapb_090--com.fasterxml.jackson.core__jackson-core__2.9.9_shaded.jar:/databricks/jars/----scalapb_090--com.google.android__annotations__4.1.1.4_shaded.jar:/databricks/jars/----scalapb_090--com.google.api.grpc__proto-google-common-protos__1.12.0_shaded.jar:/databricks/jars/----scalapb_090--com.google.code.gson__gson__2.7_shaded.jar:/databricks/jars/----scalapb_090--com.google.errorprone__error_prone_annotations__2.3.2_shaded.jar:/databricks/jars/----scalapb_090--com.google.guava__guava__20.0_shaded.jar:/databricks/jars/----scalapb_090--com.google.protobuf__protobuf-java__3.7.1_shaded.jar:/databricks/jars/----scalapb_090--com.google.protobuf__protobuf-java-util__3.7.1_shaded.jar:/databricks/jars/----scalapb_090--com.lihaoyi__fastparse_2.12__2.1.3_shaded.jar:/databricks/jars/----scalapb_090--com.lihaoyi__sourcecode_2.12__0.1.7_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-api__1.22.1_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-context__1.22.1_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-core__1.22.1_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-netty__1.22.1_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-protobuf__1.21.0_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-protobuf-lite__1.21.0_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-stub__1.21.0_shaded.jar:/databricks/jars/----scalapb_090--io.opencensus__opencensus-api__0.21.0_shaded.jar:/databricks/jars/----scalapb_090--io.opencensus__opencensus-contrib-grpc-metrics__0.21.0_shaded.jar:/databricks/jars/----scalapb_090--io.perfmark__perfmark-api__0.16.0_shaded.jar:/databricks/jars/----scalapb_090--org.codehaus.mojo__animal-sniffer-annotations__1.17_shaded.jar:/databricks/jars/----scalapb_090--runtime-unshaded-jetty9-hadoop1_2.12_deploy_shaded.jar:/databricks/jars/secret-manager--api--api-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--common--spark-common-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--common-utils--utils-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--conf-reader--conf-reader_lib-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--dbutils--dbutils-api-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--antlr--parser-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--common--driver-common-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--display--display-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--driver-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--events-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--ml--ml-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--secret-redaction-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--spark--resources-resources.jar:/databricks/jars/spark--sql-extension--sql-extension-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--versions--3.0--shim_2.12_deploy.jar:/databricks/jars/spark--versions--3.0--spark_2.12_deploy.jar:/databricks/jars/sqlgateway--common--endpoint_id-spark_3.0_2.12_deploy.jar:/databricks/jars/sqlgateway--history--api--api-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--azure--com.fasterxml.jackson.core__jackson-core__2.7.2_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.azure__azure-client-runtime__1.6.4_container_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.azure__azure-keyvault-core__1.0.0_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.azure__azure-storage__8.6.4_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.rest__client-runtime__1.6.4_container_shaded.jar:/databricks/jars/third_party--azure--org.apache.commons__commons-lang3__3.4_shaded.jar:/databricks/jars/third_party--datalake--datalake-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--dropwizard-metrics-log4j-v3.2.6--metrics-log4j-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--aopalliance__aopalliance__1.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-annotations__2.7.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-databind__2.7.2_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.7.2_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.code.findbugs__jsr305__1.3.9_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__11.0.2_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__16.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.inject__guice__3.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-annotations__1.2.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__7.0.0_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__8.6.4_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.rest__client-runtime__1.1.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__logging-interceptor__3.3.1_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp__3.3.1_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp-urlconnection__3.3.1_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okio__okio__1.8.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__adapter-rxjava__2.1.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__converter-jackson__2.1.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__retrofit__2.1.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.sun.xml.bind__jaxb-impl__2.2.3-1_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180625_3682417-spark_3.0_2.12_deploy_shaded.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180920_b33d810-spark_3.0_2.12_deploy_shaded.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--io.netty__netty-all__4.0.52.Final_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--io.reactivex__rxjava__1.2.4_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.activation__activation__1.1_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.inject__javax.inject__1_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.xml.bind__jaxb-api__2.2.2_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.xml.stream__stax-api__1.0-2_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--joda-time__joda-time__2.4_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.htrace__htrace-core__3.1.0-incubating_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop--hadoop-tools--hadoop-aws--lib-spark_3.0_2.12_deploy_shaded.jar:/databricks/jars/third_party--jackson--guava_only_shaded.jar:/databricks/jars/third_party--jackson--jackson-module-scala-shaded_2.12_deploy.jar:/databricks/jars/third_party--jackson--jsr305_only_shaded.jar:/databricks/jars/third_party--jackson--paranamer_only_shaded.jar:/databricks/jars/third_party--jetty8-shaded-client--databricks-patched-jetty-client-jar_shaded.jar:/databricks/jars/third_party--jetty8-shaded-client--databricks-patched-jetty-http-jar_shaded.jar:/databricks/jars/third_party--jetty8-shaded-client--jetty-io_shaded.jar:/databricks/jars/third_party--jetty8-shaded-client--jetty-jmx_shaded.jar:/databricks/jars/third_party--jetty8-shaded-client--jetty-util_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-client_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-http_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-io_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-util_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.code.findbugs__jsr305__3.0.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.code.gson__gson__2.8.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.errorprone__error_prone_annotations__2.1.3_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.guava__guava__26.0-android_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.j2objc__j2objc-annotations__1.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.lmax__disruptor__3.4.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--commons-codec__commons-codec__1.9_shaded.jar:/databricks/jars/third_party--opencensus-shaded--commons-logging__commons-logging__1.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.squareup.okhttp3__okhttp__3.9.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.squareup.okio__okio__1.13.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.grpc__grpc-context__1.19.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-client__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-core__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-thrift__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-tracerresolver__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-api__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-jaeger__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-util__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-impl__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-impl-core__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing.contrib__opentracing-tracerresolver__0.1.5_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-api__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-noop__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-util__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.httpcomponents__httpclient__4.4.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.httpcomponents__httpcore__4.4.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.thrift__libthrift__0.11.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.checkerframework__checker-compat-qual__2.5.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.codehaus.mojo__animal-sniffer-annotations__1.14_shaded.jar:/databricks/jars/third_party--prometheus-client--jmx_collector-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--prometheus-client--simpleclient_common-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--prometheus-client--simpleclient_dropwizard-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--prometheus-client--simpleclient_servlet-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--prometheus-client--simpleclient-spark_3.0_2.12_deploy.jar:/databricks/jars/utils--process_utils-spark_3.0_2.12_deploy.jar:/databricks/jars/workflow--workflow-spark_3.0_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--common--kvstore--kvstore-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--common--network-common--network-common-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--common--network-shuffle--network-shuffle-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--common--sketch--sketch-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--common--tags--tags-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--common--unsafe--unsafe-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--core--core-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--core--libcore_generated_resources.jar:/databricks/jars/----workspace_spark_3_0--core--libcore_resources.jar:/databricks/jars/----workspace_spark_3_0--graphx--graphx-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--launcher--launcher-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--antlr--antlr--antlr__antlr__2.7.7.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--amazon-kinesis-client--com.amazonaws__amazon-kinesis-client__1.12.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-autoscaling--com.amazonaws__aws-java-sdk-autoscaling__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudformation--com.amazonaws__aws-java-sdk-cloudformation__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudfront--com.amazonaws__aws-java-sdk-cloudfront__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudhsm--com.amazonaws__aws-java-sdk-cloudhsm__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudsearch--com.amazonaws__aws-java-sdk-cloudsearch__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudtrail--com.amazonaws__aws-java-sdk-cloudtrail__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudwatch--com.amazonaws__aws-java-sdk-cloudwatch__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudwatchmetrics--com.amazonaws__aws-java-sdk-cloudwatchmetrics__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-codedeploy--com.amazonaws__aws-java-sdk-codedeploy__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cognitoidentity--com.amazonaws__aws-java-sdk-cognitoidentity__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cognitosync--com.amazonaws__aws-java-sdk-cognitosync__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-config--com.amazonaws__aws-java-sdk-config__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-datapipeline--com.amazonaws__aws-java-sdk-datapipeline__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-directconnect--com.amazonaws__aws-java-sdk-directconnect__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-directory--com.amazonaws__aws-java-sdk-directory__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-dynamodb--com.amazonaws__aws-java-sdk-dynamodb__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ec2--com.amazonaws__aws-java-sdk-ec2__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ecs--com.amazonaws__aws-java-sdk-ecs__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-efs--com.amazonaws__aws-java-sdk-efs__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticache--com.amazonaws__aws-java-sdk-elasticache__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticbeanstalk--com.amazonaws__aws-java-sdk-elasticbeanstalk__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticloadbalancing--com.amazonaws__aws-java-sdk-elasticloadbalancing__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elastictranscoder--com.amazonaws__aws-java-sdk-elastictranscoder__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-emr--com.amazonaws__aws-java-sdk-emr__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-glacier--com.amazonaws__aws-java-sdk-glacier__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-iam--com.amazonaws__aws-java-sdk-iam__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-importexport--com.amazonaws__aws-java-sdk-importexport__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-kinesis--com.amazonaws__aws-java-sdk-kinesis__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-lambda--com.amazonaws__aws-java-sdk-lambda__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-logs--com.amazonaws__aws-java-sdk-logs__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-machinelearning--com.amazonaws__aws-java-sdk-machinelearning__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-opsworks--com.amazonaws__aws-java-sdk-opsworks__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-rds--com.amazonaws__aws-java-sdk-rds__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-redshift--com.amazonaws__aws-java-sdk-redshift__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-route53--com.amazonaws__aws-java-sdk-route53__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ses--com.amazonaws__aws-java-sdk-ses__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-simpledb--com.amazonaws__aws-java-sdk-simpledb__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-simpleworkflow--com.amazonaws__aws-java-sdk-simpleworkflow__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sns--com.amazonaws__aws-java-sdk-sns__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sqs--com.amazonaws__aws-java-sdk-sqs__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ssm--com.amazonaws__aws-java-sdk-ssm__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-storagegateway--com.amazonaws__aws-java-sdk-storagegateway__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-support--com.amazonaws__aws-java-sdk-support__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-swf-libraries--com.amazonaws__aws-java-sdk-swf-libraries__1.11.22.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-workspaces--com.amazonaws__aws-java-sdk-workspaces__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.chuusai--shapeless_2.12--com.chuusai__shapeless_2.12__2.3.3.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.clearspring.analytics--stream--com.clearspring.analytics__stream__2.9.6.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.databricks--Rserve--com.databricks__Rserve__1.8-3.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.databricks.scalapb--compilerplugin_2.12--com.databricks.scalapb__compilerplugin_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.databricks.scalapb--scalapb-runtime_2.12--com.databricks.scalapb__scalapb-runtime_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.esotericsoftware--kryo-shaded--com.esotericsoftware__kryo-shaded__4.0.2.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.esotericsoftware--minlog--com.esotericsoftware__minlog__1.3.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml--classmate--com.fasterxml__classmate__1.3.4.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.10.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.10.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.10.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.10.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.datatype--jackson-datatype-joda--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.10.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.module--jackson-module-paranamer--com.fasterxml.jackson.module__jackson-module-paranamer__2.10.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.module--jackson-module-scala_2.12--com.fasterxml.jackson.module__jackson-module-scala_2.12__2.10.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.github.ben-manes.caffeine--caffeine--com.github.ben-manes.caffeine__caffeine__2.3.4.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil--jniloader--com.github.fommil__jniloader__1.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--core--com.github.fommil.netlib__core__1.1.2.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_ref-java--com.github.fommil.netlib__native_ref-java__1.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_ref-java-natives--com.github.fommil.netlib__native_ref-java-natives__1.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_system-java--com.github.fommil.netlib__native_system-java__1.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_system-java-natives--com.github.fommil.netlib__native_system-java-natives__1.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--netlib-native_ref-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_ref-linux-x86_64-natives__1.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--netlib-native_system-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_system-linux-x86_64-natives__1.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.github.joshelser--dropwizard-metrics-hadoop-metrics2-reporter--com.github.joshelser__dropwizard-metrics-hadoop-metrics2-reporter__0.1.2.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.github.luben--zstd-jni--com.github.luben__zstd-jni__1.4.4-3.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.github.wendykierp--JTransforms--com.github.wendykierp__JTransforms__3.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__3.0.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.google.code.gson--gson--com.google.code.gson__gson__2.2.4.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.google.flatbuffers--flatbuffers-java--com.google.flatbuffers__flatbuffers-java__1.9.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.google.guava--guava--com.google.guava__guava__15.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.h2database--h2--com.h2database__h2__1.4.195.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.helger--profiler--com.helger__profiler__1.1.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.jcraft--jsch--com.jcraft__jsch__0.1.50.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.2.8.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.microsoft.sqlserver--mssql-jdbc--com.microsoft.sqlserver__mssql-jdbc__8.2.1.jre8.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-beanutils--commons-beanutils--commons-beanutils__commons-beanutils__1.9.4.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-codec--commons-codec--commons-codec__commons-codec__1.10.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-configuration--commons-configuration--commons-configuration__commons-configuration__1.6.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-dbcp--commons-dbcp--commons-dbcp__commons-dbcp__1.4.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-digester--commons-digester--commons-digester__commons-digester__1.8.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-fileupload--commons-fileupload--commons-fileupload__commons-fileupload__1.3.3.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-io--commons-io--commons-io__commons-io__2.4.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-net--commons-net--commons-net__commons-net__3.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-pool--commons-pool--commons-pool__commons-pool__1.5.4.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.ning--compress-lzf--com.ning__compress-lzf__1.0.3.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.sun.mail--javax.mail--com.sun.mail__javax.mail__1.5.2.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.tdunning--json--com.tdunning__json__1.8.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.trueaccord.lenses--lenses_2.12--com.trueaccord.lenses__lenses_2.12__0.4.12.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.twitter--chill_2.12--com.twitter__chill_2.12__0.9.5.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.twitter--chill-java--com.twitter__chill-java__0.9.5.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-app_2.12--com.twitter__util-app_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-core_2.12--com.twitter__util-core_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-function_2.12--com.twitter__util-function_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-jvm_2.12--com.twitter__util-jvm_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-lint_2.12--com.twitter__util-lint_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-registry_2.12--com.twitter__util-registry_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-stats_2.12--com.twitter__util-stats_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.typesafe--config--com.typesafe__config__1.2.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.typesafe.scala-logging--scala-logging_2.12--com.typesafe.scala-logging__scala-logging_2.12__3.7.2.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.univocity--univocity-parsers--com.univocity__univocity-parsers__2.8.3.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.zaxxer--HikariCP--com.zaxxer__HikariCP__3.1.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.10.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--io.airlift--aircompressor--io.airlift__aircompressor__0.10.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__4.1.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-graphite--io.dropwizard.metrics__metrics-graphite__4.1.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__4.1.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__4.1.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jmx--io.dropwizard.metrics__metrics-jmx__4.1.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__4.1.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__4.1.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__4.1.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--io.netty--netty-all--io.netty__netty-all__4.1.47.Final.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--jakarta.annotation--jakarta.annotation-api--jakarta.annotation__jakarta.annotation-api__1.3.5.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--jakarta.validation--jakarta.validation-api--jakarta.validation__jakarta.validation-api__2.0.2.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--jakarta.ws.rs--jakarta.ws.rs-api--jakarta.ws.rs__jakarta.ws.rs-api__2.1.6.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--javax.activation--activation--javax.activation__activation__1.1.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--javax.servlet--javax.servlet-api--javax.servlet__javax.servlet-api__3.1.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--javax.servlet.jsp--jsp-api--javax.servlet.jsp__jsp-api__2.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--javax.transaction--jta--javax.transaction__jta__1.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--javax.transaction--transaction-api--javax.transaction__transaction-api__1.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--javax.xml.bind--jaxb-api--javax.xml.bind__jaxb-api__2.2.2.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--javax.xml.stream--stax-api--javax.xml.stream__stax-api__1.0-2.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--javolution--javolution--javolution__javolution__5.5.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--jline--jline--jline__jline__2.14.6.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--joda-time--joda-time--joda-time__joda-time__2.10.5.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--log4j--log4j--log4j__log4j__1.2.17.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--net.razorvine--pyrolite--net.razorvine__pyrolite__4.30.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--net.sf.opencsv--opencsv--net.sf.opencsv__opencsv__2.3.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--net.sf.supercsv--super-csv--net.sf.supercsv__super-csv__2.2.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--snowflake-ingest-sdk--net.snowflake__snowflake-ingest-sdk__0.9.6.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--snowflake-jdbc--net.snowflake__snowflake-jdbc__3.12.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--spark-snowflake_2.12--net.snowflake__spark-snowflake_2.12__2.5.9-spark_2.4.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--net.sourceforge.f2j--arpack_combined_all--net.sourceforge.f2j__arpack_combined_all__0.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.acplt.remotetea--remotetea-oncrpc--org.acplt.remotetea__remotetea-oncrpc__1.1.2.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.antlr--antlr4-runtime--org.antlr__antlr4-runtime__4.7.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.5.2.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.apache.ant--ant-jsch--org.apache.ant__ant-jsch__1.9.2.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.2.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-format--org.apache.arrow__arrow-format__0.15.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-memory--org.apache.arrow__arrow-memory__0.15.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-vector--org.apache.arrow__arrow-vector__0.15.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro-ipc--org.apache.avro__avro-ipc__1.8.2.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro-mapred-hadoop2--org.apache.avro__avro-mapred-hadoop2__1.8.2.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.8.1.jar:/databric\n*** WARNING: skipped 27790 bytes of output ***\n\n (&#39;spark.hadoop.fs.azure.skip.metrics&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.spark.thriftserver.sqlGatewayCloseSessionHeaderName&#39;,\n  &#39;X-Databricks-SqlGateway-CloseSession&#39;),\n (&#39;spark.master&#39;, &#39;local[8]&#39;),\n (&#39;spark.scheduler.mode&#39;, &#39;FAIR&#39;),\n (&#39;spark.databricks.delta.logStore.crossCloud.fatal&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterWorkers&#39;, &#39;0&#39;),\n (&#39;spark.files.fetchFailure.unRegisterOutputOnHost&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.s3n.impl&#39;, &#39;com.databricks.s3a.S3AFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverPublicDns&#39;,\n  &#39;ec2-52-36-218-44.us-west-2.compute.amazonaws.com&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableSqlAclsOnly&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterEbsVolumeCount&#39;, &#39;0&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterNumSshKeys&#39;, &#39;0&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterOwnerOrgId&#39;, &#39;1048935133275055&#39;),\n (&#39;spark.databricks.tahoe.logStore.aws.class&#39;,\n  &#39;com.databricks.tahoe.store.S3LockBasedLogStore&#39;),\n (&#39;spark.speculation.quantile&#39;, &#39;0.9&#39;),\n (&#39;spark.shuffle.manager&#39;, &#39;SORT&#39;),\n (&#39;spark.files.overwrite&#39;, &#39;true&#39;),\n (&#39;spark.sql.hive.metastore.sharedPrefixes&#39;,\n  &#39;org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks&#39;),\n (&#39;spark.databricks.io.directoryCommit.enableLogicalDelete&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.spark.thriftserver.sqlGatewaySessionIdHeaderName&#39;,\n  &#39;X-Databricks-SqlGateway-SessionId&#39;),\n (&#39;spark.task.reaper.killTimeout&#39;, &#39;60s&#39;),\n (&#39;spark.r.numRBackendThreads&#39;, &#39;1&#39;),\n (&#39;spark.hadoop.fs.wasbs.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.parquet.block.size.row.check.min&#39;, &#39;10&#39;),\n (&#39;spark.hadoop.hive.server2.use.SSL&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.abfss.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.sql.hive.metastore.version&#39;, &#39;0.13.0&#39;),\n (&#39;spark.shuffle.service.port&#39;, &#39;4048&#39;),\n (&#39;spark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType&#39;, &#39;default&#39;),\n (&#39;spark.repl.class.outputDir&#39;,\n  &#39;/local_disk0/tmp/repl/spark-7199251671556674722-73ea3f66-73d8-4982-a156-2cf2e8cf2052&#39;),\n (&#39;spark.databricks.acl.client&#39;,\n  &#39;com.databricks.spark.sql.acl.client.SparkSqlAclClient&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterAvailability&#39;, &#39;ON_DEMAND&#39;),\n (&#39;spark.hadoop.hive.warehouse.subdir.inherit.perms&#39;, &#39;false&#39;),\n (&#39;spark.streaming.driver.writeAheadLog.closeFileAfterWrite&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb&#39;, &#39;0&#39;),\n (&#39;spark.hadoop.hive.server2.keystore.path&#39;,\n  &#39;/databricks/keys/jetty-ssl-driver-keystore.jks&#39;),\n (&#39;spark.databricks.credential.redactor&#39;,\n  &#39;com.databricks.logging.secrets.CredentialRedactorProxyImpl&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterPinned&#39;, &#39;false&#39;),\n (&#39;spark.sql.ui.retainedExecutions&#39;, &#39;100&#39;),\n (&#39;spark.databricks.acl.provider&#39;,\n  &#39;com.databricks.sql.acl.ReflectionBackedAclProvider&#39;),\n (&#39;spark.extraListeners&#39;,\n  &#39;com.databricks.backend.daemon.driver.DBCEventLoggingListener&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableElasticDisk&#39;, &#39;false&#39;),\n (&#39;spark.sql.parquet.cacheMetadata&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2&#39;, &#39;0&#39;),\n (&#39;spark.hadoop.fs.adl.impl&#39;, &#39;com.databricks.adl.AdlFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterNodeType&#39;, &#39;dev-tier-node&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableLocalDiskEncryption&#39;, &#39;false&#39;),\n (&#39;spark.databricks.tahoe.logStore.class&#39;,\n  &#39;com.databricks.tahoe.store.DelegatingLogStore&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverInstanceId&#39;, &#39;i-09c734d3e9fc2ee18&#39;),\n (&#39;spark.databricks.cloudProvider&#39;, &#39;AWS&#39;),\n (&#39;spark.sql.hive.convertMetastoreParquet&#39;, &#39;true&#39;),\n (&#39;spark.executor.id&#39;, &#39;driver&#39;),\n (&#39;spark.databricks.passthrough.adls.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider&#39;),\n (&#39;spark.app.name&#39;, &#39;Databricks Shell&#39;),\n (&#39;spark.driver.allowMultipleContexts&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.workerEnvironmentId&#39;,\n  &#39;default-worker-env&#39;),\n (&#39;spark.databricks.service.dbutils.server.backend&#39;,\n  &#39;com.databricks.dbconnect.SparkServerDBUtils&#39;),\n (&#39;spark.rdd.compress&#39;, &#39;true&#39;),\n (&#39;spark.databricks.eventLog.dir&#39;, &#39;eventlogs&#39;),\n (&#39;spark.sql.catalogImplementation&#39;, &#39;hive&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterCreator&#39;, &#39;Webapp&#39;),\n (&#39;spark.speculation&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.s3a.multipart.size&#39;, &#39;10485760&#39;),\n (&#39;spark.databricks.clusterUsageTags.cloudProvider&#39;, &#39;AWS&#39;),\n (&#39;spark.hadoop.databricks.dbfs.client.version&#39;, &#39;v1&#39;),\n (&#39;spark.hadoop.hive.server2.session.check.interval&#39;, &#39;60000&#39;),\n (&#39;spark.sql.hive.convertCTAS&#39;, &#39;true&#39;),\n (&#39;spark.metrics.conf&#39;, &#39;/databricks/spark/conf/metrics.properties&#39;),\n (&#39;spark.hadoop.spark.sql.parquet.output.committer.class&#39;,\n  &#39;org.apache.spark.sql.parquet.DirectParquetOutputCommitter&#39;),\n (&#39;spark.hadoop.fs.s3a.fast.upload.default&#39;, &#39;true&#39;),\n (&#39;spark.akka.frameSize&#39;, &#39;256&#39;),\n (&#39;spark.ui.port&#39;, &#39;45822&#39;),\n (&#39;spark.hadoop.fs.s3a.fast.upload&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterGeneration&#39;, &#39;0&#39;),\n (&#39;spark.hadoop.fs.s3.impl&#39;, &#39;com.databricks.s3a.S3AFileSystem&#39;),\n (&#39;spark.repl.class.uri&#39;, &#39;spark://10.172.246.180:39082/classes&#39;),\n (&#39;spark.hadoop.fs.abfs.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.wasbs.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem&#39;),\n (&#39;spark.sql.streaming.stopTimeout&#39;, &#39;15s&#39;),\n (&#39;spark.hadoop.hive.server2.keystore.password&#39;, &#39;[REDACTED]&#39;),\n (&#39;spark.speculation.multiplier&#39;, &#39;3&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverContainerId&#39;,\n  &#39;0d9f4b81d8124d718e1f2c5bf94dc1ad&#39;),\n (&#39;spark.storage.blockManagerTimeoutIntervalMs&#39;, &#39;300000&#39;),\n (&#39;spark.r.sql.derby.temp.dir&#39;, &#39;/tmp/RtmpkgzdNB&#39;),\n (&#39;spark.databricks.overrideDefaultCommitProtocol&#39;,\n  &#39;org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterNoDriverDaemon&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.instanceWorkerEnvId&#39;,\n  &#39;default-worker-env&#39;),\n (&#39;spark.hadoop.parquet.memory.pool.ratio&#39;, &#39;0.5&#39;),\n (&#39;spark.executor.extraJavaOptions&#39;,\n  &#39;-Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=256m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xss4m -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Ddatabricks.serviceName=spark-executor-1&#39;),\n (&#39;spark.sparkr.use.daemon&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverInstancePrivateIp&#39;,\n  &#39;10.172.252.77&#39;),\n (&#39;spark.scheduler.listenerbus.eventqueue.capacity&#39;, &#39;20000&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterScalingType&#39;, &#39;fixed_size&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterStateMessage&#39;, &#39;Starting Spark&#39;),\n (&#39;spark.sql.hive.metastore.jars&#39;, &#39;/databricks/hive/*&#39;),\n (&#39;spark.databricks.passthrough.adls.gen2.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider&#39;),\n (&#39;spark.hadoop.parquet.page.write-checksum.enabled&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.databricks.s3commit.client.sslTrustAll&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.s3a.threads.max&#39;, &#39;136&#39;),\n (&#39;spark.r.backendConnectionTimeout&#39;, &#39;604800&#39;),\n (&#39;spark.serializer.objectStreamReset&#39;, &#39;100&#39;),\n (&#39;spark.sql.sources.commitProtocolClass&#39;,\n  &#39;com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol&#39;),\n (&#39;spark.hadoop.spark.driverproxy.customHeadersToProperties&#39;,\n  &#39;X-Databricks-SqlGateway-SessionId:X-Databricks-SqlGateway-SessionId,X-Databricks-User-Token:spark.databricks.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds&#39;),\n (&#39;spark.hadoop.hive.server2.idle.session.timeout&#39;, &#39;900000&#39;),\n (&#39;spark.databricks.redactor&#39;,\n  &#39;com.databricks.spark.util.DatabricksSparkLogRedactorProxy&#39;),\n (&#39;spark.databricks.clusterUsageTags.autoTerminationMinutes&#39;, &#39;120&#39;),\n (&#39;spark.app.id&#39;, &#39;local-1599492094619&#39;),\n (&#39;spark.driver.host&#39;, &#39;10.172.246.180&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterPythonVersion&#39;, &#39;3&#39;),\n (&#39;spark.driver.port&#39;, &#39;39082&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableDfAcls&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterOwnerUserId&#39;, &#39;5027385567721087&#39;),\n (&#39;spark.hadoop.fs.abfss.impl&#39;,\n  &#39;shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount&#39;, &#39;0&#39;),\n (&#39;spark.shuffle.service.enabled&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.parquet.page.verify-checksum.enabled&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.s3a.multipart.threshold&#39;, &#39;104857600&#39;),\n (&#39;spark.hadoop.fs.s3a.impl&#39;, &#39;com.databricks.s3a.S3AFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.dataPlaneRegion&#39;, &#39;us-west-2&#39;),\n (&#39;spark.rpc.message.maxSize&#39;, &#39;256&#39;),\n (&#39;spark.logConf&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableJobsAutostart&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterMetastoreAccessType&#39;,\n  &#39;RDS_DIRECT&#39;),\n (&#39;spark.hadoop.hive.server2.enable.doAs&#39;, &#39;false&#39;),\n (&#39;eventLog.rolloverIntervalSeconds&#39;, &#39;3600&#39;),\n (&#39;spark.shuffle.memoryFraction&#39;, &#39;0.2&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterId&#39;, &#39;0907-151913-garbs292&#39;),\n (&#39;spark.databricks.sparkContextId&#39;, &#39;7199251671556674722&#39;),\n (&#39;spark.databricks.clusterUsageTags.containerZoneId&#39;, &#39;us-west-2c&#39;),\n (&#39;spark.databricks.clusterUsageTags.region&#39;, &#39;us-west-2&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterSpotBidPricePercent&#39;, &#39;100&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverContainerPrivateIp&#39;,\n  &#39;10.172.246.180&#39;)]</div>"]}}],"execution_count":36},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":37}],"metadata":{"name":"w2_cloud","notebookId":2165011122111329},"nbformat":4,"nbformat_minor":0}
