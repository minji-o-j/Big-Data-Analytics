{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S.6 RDD API\n",
    "### 2. RDD 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[1] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "nRdd = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "squared = nRdd.map(lambda x: x * x)\n",
    "print (squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16]\n"
     ]
    }
   ],
   "source": [
    "#변환의 실제 결과를 보려면 collect()를 사용해서 출력해야 한다.\n",
    "print (squared.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data/ds_spark_2cols.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/ds_spark_2cols.csv\n",
    "35, 2\n",
    "40, 27\n",
    "12, 38\n",
    "15, 31\n",
    "21, 1\n",
    "14, 19\n",
    "46, 1\n",
    "10, 34\n",
    "28, 3\n",
    "48, 1\n",
    "16, 2\n",
    "30, 3\n",
    "32, 2\n",
    "48, 1\n",
    "31, 2\n",
    "22, 1\n",
    "12, 3\n",
    "39, 29\n",
    "19, 37\n",
    "25, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['35, 2', '40, 27', '12, 38', '15, 31', '21, 1']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "myRdd4 = spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_2cols.csv\"))\n",
    "\n",
    "myRdd4.take(5)\n",
    "\n",
    "#여기서는 ''\"\" 딱히 구별 안함, 문자로 읽어왔다는 이야기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['35', ' 2'], ['40', ' 27'], ['12', ' 38'], ['15', ' 31'], ['21', ' 1']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2차원 list로\n",
    "myRdd5 = myRdd4.map(lambda line: line.split(','))\n",
    "myRdd5.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'352'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'35'+'2' #아직은 문자다 '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문자열을 정수로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#for 문 이용시\n",
    "x=['35', '2']\n",
    "for i in x:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35, 2]\n"
     ]
    }
   ],
   "source": [
    "#for 문 이용시, list로\n",
    "x=['35', '2']\n",
    "y=list()\n",
    "for i in x:\n",
    "    y.append(int(i))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[35, 2]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 줄여서 처리\n",
    "x=['35', '2']\n",
    "[int(i) for i in x] #pythonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[35, 2], [40, 27], [12, 38], [15, 31], [21, 1]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#map 함수에 넣어서 정수로 만들기\n",
    "myRdd6 = myRdd5.map(lambda x: [int(i) for i in x])\n",
    "myRdd6.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### txt파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_spark_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_wiki.txt\n",
    "Wikipedia\n",
    "Apache Spark is an open source cluster computing framework.\n",
    "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
    "Apache Spark Apache Spark Apache Spark Apache Spark\n",
    "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
    "Originally developed at the University of California, Berkeley's AMPLab,\n",
    "the Spark codebase was later donated to the Apache Software Foundation,\n",
    "which has maintained it since.\n",
    "Spark provides an interface for programming entire clusters with\n",
    "implicit data parallelism and fault-tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어로 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd2=spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서를 문장으로 쪼개기, 사용자함수 사용 x\n",
    "\n",
    "sentences=myRdd2.map(lambda x:x.split()) #split()안에 인자가 없으면 whitespace로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 count\n",
    "sentences.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사용자 함수를 이용한 단어 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#사용자 함수 이용\n",
    "\n",
    "def mySplit(x):\n",
    "    return x.split()\n",
    "\n",
    "sentences2=myRdd2.map(mySplit)\n",
    "sentences2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Wikipedia'],\n",
       " ['Apache',\n",
       "  'Spark',\n",
       "  'is',\n",
       "  'an',\n",
       "  'open',\n",
       "  'source',\n",
       "  'cluster',\n",
       "  'computing',\n",
       "  'framework.'],\n",
       " ['아파치', '스파크는', '오픈', '소스', '클러스터', '컴퓨팅', '프레임워크이다.']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.take(3)  # 3: 개수, 2차원 배열로 되어있는 것 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia \n",
      "-----\n",
      "Apache Spark is an open source cluster computing framework. \n",
      "-----\n",
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다. \n",
      "-----\n",
      "Apache Spark Apache Spark Apache Spark Apache Spark \n",
      "-----\n",
      "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크 \n",
      "-----\n",
      "Originally developed at the University of California, Berkeley's AMPLab, \n",
      "-----\n",
      "the Spark codebase was later donated to the Apache Software Foundation, \n",
      "-----\n",
      "which has maintained it since. \n",
      "-----\n",
      "Spark provides an interface for programming entire clusters with \n",
      "-----\n",
      "implicit data parallelism and fault-tolerance. \n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "#document를 sentence로 쪼갬\n",
    "for line in sentences.collect():  #sentences.collect() : 전체 data\n",
    "    \n",
    "    # sentence를 word로 쪼갬\n",
    "    for word in line:\n",
    "        print (word, end=\" \")\n",
    "    print (\"\\n-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문자 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len() : 문자 개수를 센다\n",
    "len(\"Apache Spark is an open source cluster computing framework.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 59, 32, 51, 31, 72, 71, 30, 64, 46]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#문자 개수를 센 후 배열로 만들어보기\n",
    "#문장별로 센다\n",
    "myRdd2.map(lambda s:len(s)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 교체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=[\"this is\",\"a line\"]\n",
    "_rdd=spark.sparkContext.parallelize(myList) #list에서 rdd 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is'], ['a', 'line']]\n"
     ]
    }
   ],
   "source": [
    "wordsRdd=_rdd.map(lambda x:x.split())\n",
    "print (wordsRdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is', 'AA line']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repRdd=_rdd.map(lambda x:x.replace(\"a\",\"AA\"))\n",
    "repRdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 대소문자 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'s'.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THIS', 'A']\n"
     ]
    }
   ],
   "source": [
    "upperRDD =wordsRdd.map(lambda x: x[0].upper())\n",
    "print (upperRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['THIS', 'IS'], ['A', 'LINE']]\n"
     ]
    }
   ],
   "source": [
    "upper2RDD =wordsRdd.map(lambda x: [i.upper() for i in x])\n",
    "print (upper2RDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd100 = spark.sparkContext.parallelize(range(1,101)) #1~100숫자 생성\n",
    "myRdd100.reduce(lambda subtotal, x: subtotal + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단순 통계 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum:  5050\n",
      "min:  1\n",
      "max:  100\n",
      "standard deviation: 28.86607004772212\n",
      "variance:  833.25\n"
     ]
    }
   ],
   "source": [
    "print (\"sum: \", myRdd100.sum())\n",
    "print (\"min: \", myRdd100.min())\n",
    "print (\"max: \", myRdd100.max())\n",
    "print (\"standard deviation:\", myRdd100.stdev()) ##표준편차\n",
    "print (\"variance: \", myRdd100.variance())       ##분산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many lines having 'Spark':  4\n"
     ]
    }
   ],
   "source": [
    "#line을 받아서 line안에 spark가 있는지 없는지 보기\n",
    "myRdd_spark=myRdd2.filter(lambda line: \"Spark\" in line) \n",
    "print (\"How many lines having 'Spark': \",myRdd_spark.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n"
     ]
    }
   ],
   "source": [
    "myRdd_unicode = myRdd2.filter(lambda line: u\"스파크\" in line)\n",
    "print (myRdd_unicode.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter()를 이용하여 stopwords 제거하기\n",
    "- stopwords: 불용어, '이', '그', '저' 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 기준, 단어가 stopwords에 있지 않으면 true가 되어서 아래서 출력\n",
    "stopwords = ['is','am','are','the','for','a', 'an', 'at']\n",
    "\n",
    "myRdd_stop = myRdd2.flatMap(lambda x:x.split())\\\n",
    "                    .filter(lambda x: x not in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia Apache Spark open source cluster computing framework. 아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다. Apache Spark Apache Spark Apache Spark Apache Spark 아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크 Originally developed University of California, Berkeley's AMPLab, Spark codebase was later donated to Apache Software Foundation, which has maintained it since. Spark provides interface programming entire clusters with implicit data parallelism and fault-tolerance. "
     ]
    }
   ],
   "source": [
    "for words in myRdd_stop.collect(): # collect(): 전체 단어\n",
    "    print (words, end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### foreach()\n",
    "- 반환 값이 없기 때문에 아무것도 안나온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).foreach(lambda x: x + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#map은 있다 (foreach()와 비교용)\n",
    "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).map(lambda x: x + 1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x): print(x)\n",
    "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).foreach(f) #foreach: 각각의 요소에 대해서"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pipeline\n",
    "- 파이프라인은 transformation(예: map()), action(예: collect()) 함수를 연이어 적용하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[['THIS', 'IS'], ['A', 'LINE']]\n"
     ]
    }
   ],
   "source": [
    "upper2list=wordsRdd.map(lambda x: [i.upper() for i in x]).collect()\n",
    "print (type(upper2list))\n",
    "print(upper2list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2]\n"
     ]
    }
   ],
   "source": [
    "wordsLength = wordsRdd\\\n",
    "    .map(len)\\\n",
    "    .collect()\n",
    "print (wordsLength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파일에 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#폴더 생성\n",
    "spark.sparkContext.parallelize(upper2list).saveAsTextFile(\"data/ds_spark_wiki_out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C 드라이브의 볼륨에는 이름이 없습니다.\n",
      " 볼륨 일련 번호: C826-817A\n",
      "\n",
      " C:\\Users\\user\\Bigdata\\data\\ds_spark_wiki_out 디렉터리\n",
      "\n",
      "2020-09-19  오후 06:41    <DIR>          .\n",
      "2020-09-19  오후 06:41    <DIR>          ..\n",
      "2020-09-19  오후 06:41                12 .part-00000.crc\n",
      "2020-09-19  오후 06:41                 8 ._SUCCESS.crc\n",
      "2020-09-19  오후 06:41                29 part-00000\n",
      "2020-09-19  오후 06:41                 0 _SUCCESS\n",
      "               4개 파일                  49 바이트\n",
      "               2개 디렉터리  373,071,044,608 바이트 남음\n"
     ]
    }
   ],
   "source": [
    "!dir data\\ds_spark_wiki_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['THIS', 'IS']\", \"['A', 'LINE']\"]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd=spark.sparkContext.textFile(\"data/ds_spark_wiki_out\")\n",
    "_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일로 쓰기 : coalesce()함수\n",
    "_rdd.map(lambda x: \"\".join(x)).coalesce(1).saveAsTextFile(\"data/ds_spark_wiki_txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THIS', 'IS']\n",
      "['A', 'LINE']\n"
     ]
    }
   ],
   "source": [
    "!type data\\ds_spark_wiki_txt\\part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### groupby\n",
    "- 문자 데이터 처리 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wikipedia',\n",
       " 'Apache Spark is an open source cluster computing framework.',\n",
       " '아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.',\n",
       " 'Apache Spark Apache Spark Apache Spark Apache Spark',\n",
       " '아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크',\n",
       " \"Originally developed at the University of California, Berkeley's AMPLab,\",\n",
       " 'the Spark codebase was later donated to the Apache Software Foundation,',\n",
       " 'which has maintained it since.',\n",
       " 'Spark provides an interface for programming entire clusters with',\n",
       " 'implicit data parallelism and fault-tolerance.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wi: <pyspark.resultiterable.ResultIterable object at 0x0000027D30AEB088>\n",
      "Ap: <pyspark.resultiterable.ResultIterable object at 0x0000027D30AEB3C8>\n",
      "아파: <pyspark.resultiterable.ResultIterable object at 0x0000027D30AEB848>\n",
      "Or: <pyspark.resultiterable.ResultIterable object at 0x0000027D30AEB608>\n",
      "th: <pyspark.resultiterable.ResultIterable object at 0x0000027D30AEBB88>\n",
      "wh: <pyspark.resultiterable.ResultIterable object at 0x0000027D30AEBF48>\n",
      "Sp: <pyspark.resultiterable.ResultIterable object at 0x0000027D30AEBBC8>\n",
      "im: <pyspark.resultiterable.ResultIterable object at 0x0000027D30AEB408>\n"
     ]
    }
   ],
   "source": [
    "#myRdd_group=myRdd2.flatMap(lambda x:x.split()).groupBy(lambda x:w[0:2])\n",
    "myRdd_group=myRdd2.groupBy(lambda x:x[0:2])  # 앞글자 2개만 떼어서 groupby\n",
    "# Wikipidia에서 Wi만 뜯어지게 된다\n",
    "\n",
    "#각 문장의 맨 앞 2글자만 떼어서\n",
    "for (k,v) in myRdd_group.collect(): # k,v: key, value // key, value 쌍으로 되어있다.\n",
    "    print (\"{}: {}\".format(k, v))\n",
    "    #ResultIterable object: value가 여러개, 반복문이 하나 더 필요하다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wi: Wikipedia\n",
      "-----\n",
      "Ap: Apache Spark is an open source cluster computing framework.\n",
      "Ap: Apache Spark Apache Spark Apache Spark Apache Spark\n",
      "-----\n",
      "아파: 아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
      "아파: 아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
      "-----\n",
      "Or: Originally developed at the University of California, Berkeley's AMPLab,\n",
      "-----\n",
      "th: the Spark codebase was later donated to the Apache Software Foundation,\n",
      "-----\n",
      "wh: which has maintained it since.\n",
      "-----\n",
      "Sp: Spark provides an interface for programming entire clusters with\n",
      "-----\n",
      "im: implicit data parallelism and fault-tolerance.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "#myRdd_group=myRdd2.flatMap(lambda x:x.split()).groupBy(lambda x:w[0:2])\n",
    "myRdd_group=myRdd2.groupBy(lambda x:x[0:2])\n",
    "\n",
    "for (k,v) in myRdd_group.collect():\n",
    "    for eachValue in v:\n",
    "        print (\"{}: {}\".format(k, eachValue))\n",
    "    print (\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "_testList=[(\"Seoul\",1),(\"Seoul\",1),(\"Seoul\",1),(\"Busan\",1),(\"Busan\",1),\n",
    "           (\"Seoul\",1),(\"Busan\",1),\n",
    "           (\"Seoul\",1),(\"Seoul\",1),(\"Busan\",1),(\"Busan\",1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rdd 생성\n",
    "_testRdd=spark.sparkContext.parallelize(_testList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Seoul', <pyspark.resultiterable.ResultIterable at 0x27d30b05508>),\n",
       " ('Busan', <pyspark.resultiterable.ResultIterable at 0x27d30b054c8>)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.groupBy(lambda x:x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Seoul',\n",
       "  [('Seoul', 1),\n",
       "   ('Seoul', 1),\n",
       "   ('Seoul', 1),\n",
       "   ('Seoul', 1),\n",
       "   ('Seoul', 1),\n",
       "   ('Seoul', 1)]),\n",
       " ('Busan',\n",
       "  [('Busan', 1), ('Busan', 1), ('Busan', 1), ('Busan', 1), ('Busan', 1)])]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mapValues 이용해서 출력가능!\n",
    "_testRdd.groupBy(lambda x:x[0]).mapValues(lambda x: list(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Seoul',\n",
       "  [('Seoul', 1),\n",
       "   ('Seoul', 1),\n",
       "   ('Seoul', 1),\n",
       "   ('Seoul', 1),\n",
       "   ('Seoul', 1),\n",
       "   ('Seoul', 1)]),\n",
       " ('Busan',\n",
       "  [('Busan', 1), ('Busan', 1), ('Busan', 1), ('Busan', 1), ('Busan', 1)])]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lambda말고 list로도가능!\n",
    "_testRdd.groupBy(lambda x:x[0]).mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
